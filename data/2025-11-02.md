<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 11]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets](https://arxiv.org/abs/2510.25779)
*Gagan Bansal,Wenyue Hua,Zezhou Huang,Adam Fourney,Amanda Swearngin,Will Epperson,Tyler Payne,Jake M. Hofman,Brendan Lucier,Chinmay Singh,Markus Mobius,Akshay Nambi,Archana Yadav,Kevin Gao,David M. Rothschild,Aleksandrs Slivkins,Daniel G. Goldstein,Hussein Mozannar,Nicole Immorlica,Maya Murad,Matthew Vogel,Subbarao Kambhampati,Eric Horvitz,Saleema Amershi*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As LLM agents advance, they are increasingly mediating economic decisions,
ranging from product discovery to transactions, on behalf of users. Such
applications promise benefits but also raise many questions about agent
accountability and value for users. Addressing these questions requires
understanding how agents behave in realistic market conditions. However,
previous research has largely evaluated agents in constrained settings, such as
single-task marketplaces (e.g., negotiation) or structured two-agent
interactions. Real-world markets are fundamentally different: they require
agents to handle diverse economic activities and coordinate within large,
dynamic ecosystems where multiple agents with opaque behaviors may engage in
open-ended dialogues. To bridge this gap, we investigate two-sided agentic
marketplaces where Assistant agents represent consumers and Service agents
represent competing businesses. To study these interactions safely, we develop
Magentic-Marketplace -- a simulated environment where Assistants and Services
can operate. This environment enables us to study key market dynamics: the
utility agents achieve, behavioral biases, vulnerability to manipulation, and
how search mechanisms shape market outcomes. Our experiments show that frontier
models can approach optimal welfare -- but only under ideal search conditions.
Performance degrades sharply with scale, and all models exhibit severe
first-proposal bias, creating 10-30x advantages for response speed over
quality. These findings reveal how behaviors emerge across market conditions,
informing the design of fair and efficient agentic marketplaces.

</details>


### [2] [Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion](https://arxiv.org/abs/2510.25929)
*Ziyi Wang,Carmine Ventre,Maria Polukarov*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Algorithmic collusion has emerged as a central question in AI: Will the
interaction between different AI agents deployed in markets lead to collusion?
More generally, understanding how emergent behavior, be it a cartel or market
dominance from more advanced bots, affects the market overall is an important
research question.
  We propose a hierarchical multi-agent reinforcement learning framework to
study algorithmic collusion in market making. The framework includes a
self-interested market maker (Agent~A), which is trained in an uncertain
environment shaped by an adversary, and three bottom-layer competitors: the
self-interested Agent~B1 (whose objective is to maximize its own PnL), the
competitive Agent~B2 (whose objective is to minimize the PnL of its opponent),
and the hybrid Agent~B$^\star$, which can modulate between the behavior of the
other two. To analyze how these agents shape the behavior of each other and
affect market outcomes, we propose interaction-level metrics that quantify
behavioral asymmetry and system-level dynamics, while providing signals
potentially indicative of emergent interaction patterns.
  Experimental results show that Agent~B2 secures dominant performance in a
zero-sum setting against B1, aggressively capturing order flow while tightening
average spreads, thus improving market execution efficiency. In contrast,
Agent~B$^\star$ exhibits a self-interested inclination when co-existing with
other profit-seeking agents, securing dominant market share through adaptive
quoting, yet exerting a milder adverse impact on the rewards of Agents~A and B1
compared to B2. These findings suggest that adaptive incentive control supports
more sustainable strategic co-existence in heterogeneous agent environments and
offers a structured lens for evaluating behavioral design in algorithmic
trading systems.

</details>


### [3] [Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems](https://arxiv.org/abs/2510.26585)
*Fulin Lin,Shaowen Chen,Ruishan Fang,Hongwei Wang,Tao Lin*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While Multi-Agent Systems (MAS) excel at complex tasks, their growing
autonomy with operational complexity often leads to critical inefficiencies,
such as excessive token consumption and failures arising from misinformation.
Existing methods primarily focus on post-hoc failure attribution, lacking
proactive, real-time interventions to enhance robustness and efficiency. To
this end, we introduce SupervisorAgent, a lightweight and modular framework for
runtime, adaptive supervision that operates without altering the base agent's
architecture. Triggered by an LLM-free adaptive filter, SupervisorAgent
intervenes at critical junctures to proactively correct errors, guide
inefficient behaviors, and purify observations. On the challenging GAIA
benchmark, SupervisorAgent reduces the token consumption of the Smolagent
framework by an average of 29.45% without compromising its success rate.
Extensive experiments across five additional benchmarks (math reasoning, code
generation, and question answering) and various SoTA foundation models validate
the broad applicability and robustness of our approach. The code is available
at https://github.com/LINs-lab/SupervisorAgent.

</details>


### [4] [A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation](https://arxiv.org/abs/2510.26740)
*Ashwin Kumar,William Yeoh*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce the General Incentives-based Framework for Fairness (GIFF), a
novel approach for fair multi-agent resource allocation that infers fair
decision-making from standard value functions. In resource-constrained
settings, agents optimizing for efficiency often create inequitable outcomes.
Our approach leverages the action-value (Q-)function to balance efficiency and
fairness without requiring additional training. Specifically, our method
computes a local fairness gain for each action and introduces a counterfactual
advantage correction term to discourage over-allocation to already well-off
agents. This approach is formalized within a centralized control setting, where
an arbitrator uses the GIFF-modified Q-values to solve an allocation problem.
  Empirical evaluations across diverse domains, including dynamic ridesharing,
homelessness prevention, and a complex job allocation task-demonstrate that our
framework consistently outperforms strong baselines and can discover
far-sighted, equitable policies. The framework's effectiveness is supported by
a theoretical foundation; we prove its fairness surrogate is a principled lower
bound on the true fairness improvement and that its trade-off parameter offers
monotonic tuning. Our findings establish GIFF as a robust and principled
framework for leveraging standard reinforcement learning components to achieve
more equitable outcomes in complex multi-agent systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [5] [Contribution-Guided Asymmetric Learning for Robust Multimodal Fusion under Imbalance and Noise](https://arxiv.org/abs/2510.26289)
*Zijing Xu,Yunfeng Kou,Kunming Wu,Hong Liu*

Main category: cs.MM

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal learning faces two major challenges: modality imbalance and data
noise, which significantly affect the robustness and generalization ability of
models. Existing methods achieve modality balance by suppressing dominant
modalities, but they neglect the inherent differences in the information value
between modalities, potentially leading to convergence to suboptimal solutions.
This paper proposes an innovative modality compression paradigm,
Contribution-Guided Asymmetric Learning (CAL), which aims to enhance the
contribution of high-contribution modalities while compressing weak modalities
to increase their contribution, allowing both to improve the performance of
multimodal information fusion. CAL is based on a modality contribution metric
W^m combining the information quantity I(m) and confidence D(m), and it designs
an asymmetric gradient acceleration mechanism and a contribution-aware
Asymmetric Information Bottleneck (AIB) compression mechanism. The former
accelerates the gradient update of modalities, while the latter dynamically
compresses the noise of low-contribution modalities.
  On five benchmark datasets, including emotion recognition, scene recognition,
and event localization tasks, CAL has shown outstanding performance in
imbalanced fusion tasks and noise robustness tests. On CREMA-D, KS, and AVE,
CAL achieves 79.30%, 74.82%, and 74.21% accuracy, significantly outperforming
the existing state-of-the-art model ARL. In high-noise robustness tests, CAL
also achieved leading performance under various attack strategies on the
MVSA-Single and NYUD2 datasets. These results validate the significant
advantages of CAL in modality imbalance and noise interference. CAL, as a
flexible and efficient framework, is easy to transfer to other tasks and has
broad adaptability and potential application prospects.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [6] [The Impact of Navigation Aids on Search Performance and Object Recall in Wide-Area Augmented Reality](https://arxiv.org/abs/2510.25957)
*Radha Kumaran,You-Jin Kim,Anne E Milner,Tom Bullock,Barry Giesbrecht,Tobias Höllerer*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Head-worn augmented reality (AR) is a hotly pursued and increasingly feasible
contender paradigm for replacing or complementing smartphones and watches for
continual information consumption. Here, we compare three different AR
navigation aids (on-screen compass, on-screen radar and in-world vertical
arrows) in a wide-area outdoor user study (n=24) where participants search for
hidden virtual target items amongst physical and virtual objects. We analyzed
participants' search task performance, movements, eye-gaze, survey responses
and object recall. There were two key findings. First, all navigational aids
enhanced search performance relative to a control condition, with some benefit
and strongest user preference for in-world arrows. Second, users recalled fewer
physical objects than virtual objects in the environment, suggesting reduced
awareness of the physical environment. Together, these findings suggest that
while navigational aids presented in AR can enhance search task performance,
users may pay less attention to the physical environment, which could have
undesirable side-effects.

</details>


### [7] [Risks and Opportunities in Human-Machine Teaming in Operationalizing Machine Learning Target Variables](https://arxiv.org/abs/2510.25974)
*Mengtian Guo,David Gotz,Yue Wang*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Predictive modeling has the potential to enhance human decision-making.
However, many predictive models fail in practice due to problematic problem
formulation in cases where the prediction target is an abstract concept or
construct and practitioners need to define an appropriate target variable as a
proxy to operationalize the construct of interest. The choice of an appropriate
proxy target variable is rarely self-evident in practice, requiring both domain
knowledge and iterative data modeling. This process is inherently
collaborative, involving both domain experts and data scientists. In this work,
we explore how human-machine teaming can support this process by accelerating
iterations while preserving human judgment. We study the impact of two
human-machine teaming strategies on proxy construction: 1) relevance-first:
humans leading the process by selecting relevant proxies, and 2)
performance-first: machines leading the process by recommending proxies based
on predictive performance. Based on a controlled user study of a proxy
construction task (N = 20), we show that the performance-first strategy
facilitated faster iterations and decision-making, but also biased users
towards well-performing proxies that are misaligned with the application goal.
Our study highlights the opportunities and risks of human-machine teaming in
operationalizing machine learning target variables, yielding insights for
future research to explore the opportunities and mitigate the risks.

</details>


### [8] [On the Go with AR: Attention to Virtual and Physical Targets while Varying Augmentation Density](https://arxiv.org/abs/2510.25978)
*You-Jin Kim,Radha Kumaran,Jingjing Luo,Tom Bullock,Barry Giesbrecht,Tobias Höllerer*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Augmented reality is projected to be a primary mode of information
consumption on the go, seamlessly integrating virtual content into the physical
world. However, the potential perceptual demands of viewing virtual annotations
while navigating a physical environment could impact user efficacy and safety,
and the implications of these demands are not well understood. Here, we
investigate the impact of virtual path guidance and augmentation density
(visual clutter) on search performance and memory. Participants walked along a
predefined path, searching for physical or virtual items. They experienced two
levels of augmentation density, and either walked freely or with enforced speed
and path guidance. Augmentation density impacted behavior and reduced awareness
of uncommon objects in the environment. Analysis of search task performance and
post-experiment item recall revealed differing attention to physical and
virtual objects. On the basis of these findings we outline considerations for
AR apps designed for use on the go.

</details>


### [9] [Designing for Dignity while Driving: Interaction Needs of Blind and Low-Vision Passengers in Fully Automated Vehicles](https://arxiv.org/abs/2510.26015)
*Zhengtao Ma,Rafael Gomez,Togtokhtur Batbold,Zishuo Zhu,Yueteng Yu,Ronald Schroeter*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fully automated vehicles (FAVs) hold promise for enhancing the mobility of
blind and low-vision (BLV) individuals. To understand the situated interaction
needs of BLV passengers, we conducted six on-road, and in-lab focus groups with
16 participants, immersing them in real-world driving conditions. Our thematic
analysis reveals that BLV participants express a high initial 'faith' in FAVs,
but require layered, value-sensitive information during the ride to cultivate
trust. The participants' modality preference for voice suggests re-evaluating
the role of haptics for BLV users in FAVs. Our findings show the importance of
a respectful interaction design in FAVs that both address BLV users' mobility
challenges and uphold their dignity. While others have advocated for a dignity
lens, our contribution lies in grounding this framework in empirical findings
and unpacking what it means to design for dignity in the context of FAVs.

</details>


### [10] [FractalBrain: A Neuro-interactive Virtual Reality Experience using Electroencephalogram (EEG) for Mindfulness](https://arxiv.org/abs/2510.26041)
*Jamie Ngoc Dinh,You-Jin Kim,Myungin Lee*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mindfulness has been studied and practiced in enhancing psychological
well-being while reducing neuroticism and psychopathological indicators.
However, practicing mindfulness with continuous attention is challenging,
especially for beginners. In the proposed system, FractalBrain, we utilize an
interactive audiovisual fractal with a geometric repetitive pattern that has
been demonstrated to induce meditative effects. FractalBrain presents an
experience combining a surreal virtual reality (VR) program with an
electroencephalogram (EEG) interface. While viewing an ever-changing
fractal-inspired artwork in an immersive environment, the user's EEG stream is
analyzed and mapped into VR. These EEG data adaptively manipulates the
audiovisual parameters in real-time, generating a distinct experience for each
user. The pilot feedback suggests the potential of the FractalBrain to
facilitate mindfulness and enhance attention.

</details>


### [11] [Look at That Distractor: Dynamic Translation Gain under Low Perceptual Load in Virtual Reality](https://arxiv.org/abs/2510.26265)
*Ling-Long Zou,Qiang Tong,Er-Xia Luo,Sen-Zhe Xu,Song-Hai Zhang,Fang-Lue Zhang*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Redirected walking utilizes gain adjustments within perceptual thresholds to
allow natural navigation in large scale virtual environments within confined
physical environments. Previous research has found that when users are
distracted by some scene elements, they are less sensitive to gain values.
However, the effects on detection thresholds have not been quantitatively
measured. In this paper, we present a novel method that dynamically adjusts
translation gain by leveraging visual distractors. We place distractors within
the user's field of view and apply a larger translation gain when their
attention is drawn to them. Because the magnitude of gain adjustment depends on
the user's level of engagement with the distractors, the redirection process
remains smooth and unobtrusive. To evaluate our method, we developed a task
oriented virtual environment for a user study. Results show that introducing
distractors in the virtual environment significantly raises users' translation
gain thresholds. Furthermore, assessments using the Simulator Sickness
Questionnaire and Igroup Presence Questionnaire indicate that the method
maintains user comfort and acceptance, supporting its effectiveness for RDW
systems.

</details>


### [12] [Interaction-Augmented Instruction: Modeling the Synergy of Prompts and Interactions in Human-GenAI Collaboration](https://arxiv.org/abs/2510.26069)
*Leixian Shen,Yifang Wang,Huamin Qu,Xing Xie,Haotian Li*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Text prompt is the most common way for human-generative AI (GenAI)
communication. Though convenient, it is challenging to convey fine-grained and
referential intent. One promising solution is to combine text prompts with
precise GUI interactions, like brushing and clicking. However, there lacks a
formal model to model synergistic designs between prompts and interactions,
hindering their comparison and innovation. To fill this gap, via an iterative
and deductive process, we develop the Interaction-Augmented Instruction (IAI)
model, a compact entity-relation graph formalizing how the combination of
interactions and text prompts enhances human-generative AI communication. With
the model, we distill twelve recurring and composable atomic interaction
paradigms from prior tools, verifying our model's capability to facilitate
systematic design characterization and comparison. Case studies further
demonstrate the model's utility in applying, refining, and extending these
paradigms. These results illustrate our IAI model's descriptive,
discriminative, and generative power for shaping future GenAI systems.

</details>


### [13] [Linking Heterogeneous Data with Coordinated Agent Flows for Social Media Analysis](https://arxiv.org/abs/2510.26172)
*Shifu Chen,Dazhen Deng,Zhihong Xu,Sijia Xu,Tai-Quan Peng,Yingcai Wu*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Social media platforms generate massive volumes of heterogeneous data,
capturing user behaviors, textual content, temporal dynamics, and network
structures. Analyzing such data is crucial for understanding phenomena such as
opinion dynamics, community formation, and information diffusion. However,
discovering insights from this complex landscape is exploratory, conceptually
challenging, and requires expertise in social media mining and visualization.
Existing automated approaches, though increasingly leveraging large language
models (LLMs), remain largely confined to structured tabular data and cannot
adequately address the heterogeneity of social media analysis. We present SIA
(Social Insight Agents), an LLM agent system that links heterogeneous
multi-modal data -- including raw inputs (e.g., text, network, and behavioral
data), intermediate outputs, mined analytical results, and visualization
artifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy
that connects insight types with suitable mining and visualization techniques,
SIA enables agents to plan and execute coherent analysis strategies. To ensure
multi-modal integration, it incorporates a data coordinator that unifies
tabular, textual, and network data into a consistent flow. Its interactive
interface provides a transparent workflow where users can trace, validate, and
refine the agent's reasoning, supporting both adaptability and trustworthiness.
Through expert-centered case studies and quantitative evaluation, we show that
SIA effectively discovers diverse and meaningful insights from social media
while supporting human-agent collaboration in complex analytical tasks.

</details>


### [14] [Avatar Appearance Beyond Pixels -- User Ratings and Avatar Preferences within Health Applications](https://arxiv.org/abs/2510.26251)
*Navid Ashrafi,Philipp Graf,Manuela Marquardt,Francesco Vona,Julia Schorlemmer,Jan-Niklas Voigt-Antons*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The appearance of a virtual avatar significantly influences its perceived
appropriateness and the user's experience, particularly in healthcare
applications. This study analyzed interactions with six avatars of varying
characteristics in a patient-reported outcome measures (PROMs) application to
investigate correlations between avatar ratings and user preferences.
Forty-seven participants completed a healthcare survey involving 30 PROMIS
items (Global Health and Physical Function) and then rated the avatars on
warmth, competence, attractiveness, and human-likeness, as well as their
willingness to share personal data. The results showed that competence was the
most critical factor in avatar selection, while human-likeness had minimal
impact on health data disclosure. Gender did not significantly affect the
ratings, but clothing style played a key role, with male avatars in
professional attire rated higher in competence due to gender-stereotypical
expectations. In contrast, professional female avatars were rated lower in
warmth and attractiveness. These findings underline the importance of
thoughtful avatar design in healthcare applications to enhance user experience
and engagement.

</details>


### [15] [Scaffolding Creativity: How Divergent and Convergent LLM Personas Shape Human Machine Creative Problem-Solving](https://arxiv.org/abs/2510.26490)
*Alon Rosenbaum,Yigal David,Eran Kaufman,Gilad Ravid,Amit Ronen,Assaf Krebs*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) are increasingly shaping creative work and
problem-solving; however, prior research suggests that they may diminish
unassisted creativity. To address this tension, a coach-like LLM environment
was developed that embodies divergent and convergent thinking personas as two
complementary processes. Effectiveness and user behavior were assessed through
a controlled experiment in which participants interacted with either persona,
while a control group engaged with a standard LLM providing direct answers.
  Notably, users' perceptions of which persona best supported their creativity
often diverged from objective performance measures. Trait-based analyses
revealed that individual differences predict when people utilize divergent
versus convergent personas, suggesting opportunities for adaptive sequencing.
Furthermore, interaction patterns reflected the design thinking model,
demonstrating how persona-guided support shapes creative problem-solving.
  Our findings provide design principles for creativity support systems that
strike a balance between exploration and convergence through persona-based
guidance and personalization. These insights advance human-AI collaboration
tools that scaffold rather than overshadow human creativity.

</details>


### [16] [Metacognition and Confidence Dynamics in Advice Taking from Generative AI](https://arxiv.org/abs/2510.26508)
*Clara Colombatto,Sean Rintel,Lev Tankelevitch*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative Artificial Intelligence (GenAI) can aid humans in a wide range of
tasks, but its effectiveness critically depends on users being able to evaluate
the accuracy of GenAI outputs and their own expertise. Here we asked how
confidence in self and GenAI contributes to decisions to seek and rely on
advice from GenAI ('prospective confidence'), and how advice-taking in turn
shapes this confidence ('retrospective confidence'). In a novel paradigm
involving text generation, participants formulated plans for events, and could
request advice from a GenAI (Study 1; N=200) or were randomly assigned to
receive advice (Study 2; N=300), which they could rely on or ignore. Advice
requests in Study 1 were related to higher prospective confidence in GenAI and
lower confidence in self. Advice-seekers showed increased retrospective
confidence in GenAI, while those who declined advice showed increased
confidence in self. Random assignment in Study 2 revealed that advice exposure
increases confidence in GenAI and in self, suggesting that GenAI advice-taking
causally boosts retrospective confidence. These results were mirrored in advice
reliance, operationalised as the textual similarity between GenAI advice and
participants' responses, with reliance associated with increased retrospective
confidence in both GenAI and self. Critically, participants who chose to
obtain/rely on advice provided more detailed responses (likely due to the
output's verbosity), but failed to check the output thoroughly, missing key
information. These findings underscore a key role for confidence in
interactions with GenAI, shaped by both prior beliefs about oneself and the
reliability of AI, and context-dependent exposure to advice.

</details>
