{"id": "2512.21551", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21551", "abs": "https://arxiv.org/abs/2512.21551", "authors": ["Hua Shen", "Tiffany Knearem", "Divy Thakkar", "Pat Pataranutaporn", "Anoop Sinha", "Yike", "Shi", "Jenny T. Liang", "Lama Ahmad", "Tanu Mitra", "Brad A. Myers", "Yang Li"], "title": "Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures", "comment": "CHI 2026 BiAlign Workshop", "summary": "The rapid integration of generative AI into everyday life underscores the need to move beyond unidirectional alignment models that only adapt AI to human values. This workshop focuses on bidirectional human-AI alignment, a dynamic, reciprocal process where humans and AI co-adapt through interaction, evaluation, and value-centered design. Building on our past CHI 2025 BiAlign SIG and ICLR 2025 Workshop, this workshop will bring together interdisciplinary researchers from HCI, AI, social sciences and more domains to advance value-centered AI and reciprocal human-AI collaboration. We focus on embedding human and societal values into alignment research, emphasizing not only steering AI toward human values but also enabling humans to critically engage with and evolve alongside AI systems. Through talks, interdisciplinary discussions, and collaborative activities, participants will explore methods for interactive alignment, frameworks for societal impact evaluation, and strategies for alignment in dynamic contexts. This workshop aims to bridge the disciplines' gaps and establish a shared agenda for responsible, reciprocal human-AI futures."}
{"id": "2512.21589", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.21589", "abs": "https://arxiv.org/abs/2512.21589", "authors": ["Masaaki Yamauchi", "Yiyuan Liang", "Hiroko Hara", "Hideyuki Shimonishi", "Masayuki Murata"], "title": "Emotion-Aware Smart Home Automation Based on the eBICA Model", "comment": "Accepted at IEEE ICCE 2026", "summary": "Smart home automation that adapts to a user's emotional state can enhance psychological safety in daily living environments. This study proposes an emotion-aware automation framework guided by the emotional Biologically Inspired Cognitive Architecture (eBICA), which integrates appraisal, somatic responses, and behavior selection. We conducted a proof-of-concept experiment in a pseudo-smart-home environment, where participants were exposed to an anxiety-inducing event followed by a comfort-inducing automation. State anxiety (STAI-S) was measured throughout the task sequence. The results showed a significant reduction in STAI-S immediately after introducing the avoidance automation, demonstrating that emotion-based control can effectively promote psychological safety. Furthermore, an analysis of individual characteristics suggested that personality and anxiety-related traits modulate the degree of relief, indicating the potential for personalized emotion-adaptive automation. Overall, this study provides empirical evidence that eBICA-based emotional control can function effectively in smart home environments and offers a foundation for next-generation affective home automation systems."}
{"id": "2512.21649", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.21649", "abs": "https://arxiv.org/abs/2512.21649", "authors": ["ATM Mizanur Rahman", "Sharifa Sultana"], "title": "Ghostcrafting AI: Under the Rug of Platform Labor", "comment": null, "summary": "Platform laborers play an indispensable yet hidden role in building and sustaining AI systems. Drawing on an eight-month ethnography of Bangladesh's platform labor industry and inspired by Gray and Suri, we conceptualize Ghostcrafting AI to describe how workers materially enable AI while remaining invisible or erased from recognition. Workers pursue platform labor as a path to prestige and mobility but sustain themselves through resourceful, situated learning - renting cyber-cafe computers, copying gig templates, following tutorials in unfamiliar languages, and relying on peer networks. At the same time, they face exploitative wages, unreliable payments, biased algorithms, and governance structures that make their labor precarious and invisible. To cope, they develop tactical repertoires such as identity masking, bypassing platform fees, and pirated tools. These practices reveal both AI's dependency on ghostcrafted labor and the urgent need for design, policy, and governance interventions that ensure fairness, recognition, and sustainability in platform futures."}
{"id": "2512.21481", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.21481", "abs": "https://arxiv.org/abs/2512.21481", "authors": ["Sunith Vallabhaneni", "Thomas Berkane", "Maimuna Majumder"], "title": "The AI Committee: A Multi-Agent Framework for Automated Validation and Remediation of Web-Sourced Data", "comment": null, "summary": "Many research areas rely on data from the web to gain insights and test their methods. However, collecting comprehensive research datasets often demands manually reviewing many web pages to identify and record relevant data points, which is labor-intensive and susceptible to error. While the emergence of large language models (LLM)-powered web agents has begun to automate parts of this process, they often struggle to ensure the validity of the data they collect. Indeed, these agents exhibit several recurring failure modes - including hallucinating or omitting values, misinterpreting page semantics, and failing to detect invalid information - which are subtle and difficult to detect and correct manually. To address this, we introduce the AI Committee, a novel model-agnostic multi-agent system that automates the process of validating and remediating web-sourced datasets. Each agent is specialized in a distinct task in the data quality assurance pipeline, from source scrutiny and fact-checking to data remediation and integrity validation. The AI Committee leverages various LLM capabilities - including in-context learning for dataset adaptation, chain-of-thought reasoning for complex semantic validation, and a self-correction loop for data remediation - all without task-specific training. We demonstrate the effectiveness of our system by applying it to three real-world datasets, showing that it generalizes across LLMs and significantly outperforms baseline approaches, achieving data completeness up to 78.7% and precision up to 100%. We additionally conduct an ablation study demonstrating the contribution of each agent to the Committee's performance. This work is released as an open-source tool for the research community."}
{"id": "2512.21747", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21747", "abs": "https://arxiv.org/abs/2512.21747", "authors": ["Gourav Siddhad", "Anurag Singh", "Rajkumar Saini", "Partha Pratim Roy"], "title": "Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG", "comment": "8 Pages, 3 Figures, 1 Table", "summary": "Driver drowsiness remains a primary cause of traffic accidents, necessitating the development of real-time, reliable detection systems to ensure road safety. This study presents a Modified TSception architecture designed for the robust assessment of driver fatigue using Electroencephalography (EEG). The model introduces a novel hierarchical architecture that surpasses the original TSception by implementing a five-layer temporal refinement strategy to capture multi-scale brain dynamics. A key innovation is the use of Adaptive Average Pooling, which provides the structural flexibility to handle varying EEG input dimensions, and a two - stage fusion mechanism that optimizes the integration of spatiotemporal features for improved stability. When evaluated on the SEED-VIG dataset and compared against established methods - including SVM, Transformer, EEGNet, ConvNeXt, LMDA-Net, and the original TSception - the Modified TSception achieves a comparable accuracy of 83.46% (vs. 83.15% for the original). Critically, the proposed model exhibits a substantially reduced confidence interval (0.24 vs. 0.36), signifying a marked improvement in performance stability. Furthermore, the architecture's generalizability is validated on the STEW mental workload dataset, where it achieves state-of-the-art results with 95.93% and 95.35% accuracy for 2-class and 3-class classification, respectively. These improvements in consistency and cross-task generalizability underscore the effectiveness of the proposed modifications for reliable EEG-based monitoring of drowsiness and mental workload."}
{"id": "2512.21727", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.21727", "abs": "https://arxiv.org/abs/2512.21727", "authors": ["Daniil Sherki", "Daniil Merkulov", "Alexandra Savina", "Ekaterina Muravleva"], "title": "PERELMAN: Pipeline for scientific literature meta-analysis. Technical report", "comment": null, "summary": "We present PERELMAN (PipEline foR sciEntific Literature Meta-ANalysis), an agentic framework designed to extract specific information from a large corpus of scientific articles to support large-scale literature reviews and meta-analyses. Our central goal is to reliably transform heterogeneous article content into a unified, machine-readable representation. PERELMAN first elicits domain knowledge-including target variables, inclusion criteria, units, and normalization rules-through a structured dialogue with a subject-matter expert. This domain knowledge is then reused across multiple stages of the pipeline and guides coordinated agents in extracting evidence from narrative text, tables, and figures, enabling consistent aggregation across studies. In order to assess reproducibility and validate our implementation, we evaluate the system on the task of reproducing the meta-analysis of layered Li-ion cathode properties (NMC811 material). We describe our solution, which has the potential to reduce the time required to prepare meta-analyses from months to minutes."}
{"id": "2512.21901", "categories": ["cs.GR", "cs.CG"], "pdf": "https://arxiv.org/pdf/2512.21901", "abs": "https://arxiv.org/abs/2512.21901", "authors": ["Yosuke Onoue"], "title": "Graph Drawing Stress Model with Resistance Distances", "comment": "Accepted by PacificVis 2026 (TVCG Journal Track)", "summary": "This paper challenges the convention of using graph-theoretic shortest distance in stress-based graph drawing. We propose a new paradigm based on resistance distance, derived from the graph Laplacian's spectrum, which better captures global graph structure. This approach overcomes theoretical and computational limitations of traditional methods, as resistance distance admits a natural isometric embedding in Euclidean space. Our experiments demonstrate improved neighborhood preservation and cluster faithfulness. We introduce Omega, a linear-time graph drawing algorithm that integrates a fast resistance distance embedding with random node-pair sampling for Stochastic Gradient Descent (SGD). This comprehensive random sampling strategy, enabled by efficient pre-computation of resistance distance embeddings, is more effective and robust than pivot-based sampling used in prior algorithms, consistently achieving lower and more stable stress values. The algorithm maintains $O(|E|)$ complexity for both weighted and unweighted graphs. Our work establishes a connection between spectral graph theory and stress-based layouts, providing a practical and scalable solution for network visualization."}
{"id": "2512.21796", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.21796", "abs": "https://arxiv.org/abs/2512.21796", "authors": ["Hye-Young Jo", "Ada Zhao", "Xiaoan Liu", "Ryo Suzuki"], "title": "Generative Lecture: Making Lecture Videos Interactive with LLMs and AI Clone Instructors", "comment": "14 pages, 16 figures", "summary": "We introduce Generative Lecture, a concept that makes existing lecture videos interactive through generative AI and AI clone instructors. By leveraging interactive avatars powered by HeyGen, ElevenLabs, and GPT-5, we embed an AI instructor into the video and augment the video content in response to students' questions. This allows students to personalize the lecture material, directly ask questions in the video, and receive tailored explanations generated and delivered by the AI-cloned instructor. From a design elicitation study (N=8), we identified four goals that guided the development of eight system features: 1) on-demand clarification, 2) enhanced visuals, 3) interactive example, 4) personalized explanation, 5) adaptive quiz, 6) study summary, 7) automatic highlight, and 8) adaptive break. We then conducted a user study (N=12) to evaluate the usability and effectiveness of the system and collected expert feedback (N=5). The results suggest that our system enables effective two-way communication and supports personalized learning."}
{"id": "2512.21878", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21878", "abs": "https://arxiv.org/abs/2512.21878", "authors": ["Marc S. Montalvo", "Hamed Yaghoobian"], "title": "MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting", "comment": "Accepted to the NeurIPS 2025 Workshop on Generative AI in Finance", "summary": "Recent advances in large language models (LLMs) are transforming data-intensive domains, with finance representing a high-stakes environment where transparent and reproducible analysis of heterogeneous signals is essential. Traditional quantitative methods remain vulnerable to survivorship bias, while many AI-driven approaches struggle with signal integration, reproducibility, and computational efficiency. We introduce MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news, while embedding explicit bias-mitigation protocols. The system leverages GPT-4.1-nano for reproducability and cost-efficient inference and generates weekly portfolios of 15-30 equities with allocation weights optimized for short-term performance. In an eight-week evaluation, MASFIN delivered a 7.33% cumulative return, outperforming the S&P 500, NASDAQ-100, and Dow Jones benchmarks in six of eight weeks, albeit with higher volatility. These findings demonstrate the promise of bias-aware, generative AI frameworks for financial forecasting and highlight opportunities for modular multi-agent design to advance practical, transparent, and reproducible approaches in quantitative finance."}
{"id": "2512.21968", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.21968", "abs": "https://arxiv.org/abs/2512.21968", "authors": ["Kureha Hamagashira", "Miyuki Azuma", "Sotaro Shimada"], "title": "Positive Narrativity Enhances Sense of Agency toward a VR Avatar", "comment": "11 pages,4 figures", "summary": "The full-body illusion (FBI) refers to the experience of perceiving a virtual avatar as one's own body. In virtual reality (VR) environments, inducing the FBI has been shown to modulate users' bodily experiences and behavior. Previous studies have demonstrated that embodying avatars with specific characteristics can influence users' actions, largely through the activation of implicit stereotypes. However, few studies have explicitly manipulated users' impressions of an avatar by introducing narrative context. The present study investigated how avatar narrativity, induced through contextual narratives, affects the FBI. Healthy participants embodied a powerful artificial lifeform avatar in VR after listening to either a positive narrative, in which the avatar used its abilities to protect others, or a negative narrative, in which it misused its power. Participants' impressions of the avatar and indices of bodily self-consciousness were subsequently assessed. The results showed that positive narratives significantly enhanced the sense of agency (SoA), and that SoA was positively correlated with participants' perceived personal familiarity with the avatar. These findings suggest that the avatar narrativity can modulate embodiment in VR."}
{"id": "2512.22016", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22016", "abs": "https://arxiv.org/abs/2512.22016", "authors": ["Xiangwen Zhang", "Xiaowei Dai", "Runnan Chen", "Xiaoming Chen", "Zeke Zexi Hu"], "title": "SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching", "comment": null, "summary": "Creating physically realistic content in VR often requires complex modeling tools or predefined 3D models, textures, and animations, which present significant barriers for non-expert users. In this paper, we propose SketchPlay, a novel VR interaction framework that transforms humans' air-drawn sketches and gestures into dynamic, physically realistic scenes, making content creation intuitive and playful like drawing. Specifically, sketches capture the structure and spatial arrangement of objects and scenes, while gestures convey physical cues such as velocity, direction, and force that define movement and behavior. By combining these complementary forms of input, SketchPlay captures both the structure and dynamics of user-created content, enabling the generation of a wide range of complex physical phenomena, such as rigid body motion, elastic deformation, and cloth dynamics. Experimental results demonstrate that, compared to traditional text-driven methods, SketchPlay offers significant advantages in expressiveness, and user experience. By providing an intuitive and engaging creation process, SketchPlay lowers the entry barrier for non-expert users and shows strong potential for applications in education, art, and immersive storytelling."}
{"id": "2512.22032", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.22032", "abs": "https://arxiv.org/abs/2512.22032", "authors": ["Ziyan Zhang", "Nan Gao", "Zhiqiang Nie", "Shantanu Pal", "Haining Zhang"], "title": "Context-Aware Intelligent Chatbot Framework Leveraging Mobile Sensing", "comment": "Accepted at Companion of the 2025 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp Companion '25), Espoo, Finland", "summary": "With the rapid advancement of large language models (LLMs), intelligent conversational assistants have demonstrated remarkable capabilities across various domains. However, they still mainly rely on explicit textual input and do not know the real world behaviors of users. This paper proposes a context-sensitive conversational assistant framework grounded in mobile sensing data. By collecting user behavior and environmental data through smartphones, we abstract these signals into 16 contextual scenarios and translate them into natural language prompts, thus improving the model's understanding of the user's state. We design a structured prompting system to guide the LLM in generating a more personalized and contextually relevant dialogue. This approach integrates mobile sensing with large language models, demonstrating the potential of passive behavioral data in intelligent conversation and offering a viable path toward digital health and personalized interaction."}
