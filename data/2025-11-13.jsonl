{"id": "2511.08980", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08980", "abs": "https://arxiv.org/abs/2511.08980", "authors": ["Haotian Yin", "Aleksander Plocharski", "Michal Jan Wlodarczyk", "Przemyslaw Musialski"], "title": "A Finite Difference Approximation of Second Order Regularization of Neural-SDFs", "comment": "SIGGRAPH Asia Technical Communications, 6 pages, 6 figures, preprint", "summary": "We introduce a finite-difference framework for curvature regularization in neural signed distance field (SDF) learning. Existing approaches enforce curvature priors using full Hessian information obtained via second-order automatic differentiation, which is accurate but computationally expensive. Others reduced this overhead by avoiding explicit Hessian assembly, but still required higher-order differentiation. In contrast, our method replaces these operations with lightweight finite-difference stencils that approximate second derivatives using the well known Taylor expansion with a truncation error of O(h^2), and can serve as drop-in replacements for Gaussian curvature and rank-deficiency losses. Experiments demonstrate that our finite-difference variants achieve reconstruction fidelity comparable to their automatic-differentiation counterparts, while reducing GPU memory usage and training time by up to a factor of two. Additional tests on sparse, incomplete, and non-CAD data confirm that the proposed formulation is robust and general, offering an efficient and scalable alternative for curvature-aware SDF learning."}
{"id": "2511.09361", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.09361", "abs": "https://arxiv.org/abs/2511.09361", "authors": ["Sizhuo Zhou", "Yuou Sun", "Bailin Deng", "Juyong Zhang"], "title": "Computational Caustic Design for Surface Light Source", "comment": null, "summary": "Designing freeform surfaces to control light based on real-world illumination patterns is challenging, as existing caustic lens designs often assume oversimplified point or parallel light sources. We propose representing surface light sources using an optimized set of point sources, whose parameters are fitted to the real light source's illumination using a novel differentiable rendering framework. Our physically-based rendering approach simulates light transmission using flux, without requiring prior knowledge of the light source's intensity distribution. To efficiently explore the light source parameter space during optimization, we apply a contraction mapping that converts the constrained problem into an unconstrained one. Using the optimized light source model, we then design the freeform lens shape considering flux consistency and normal integrability. Simulations and physical experiments show our method more accurately represents real surface light sources compared to point-source approximations, yielding caustic lenses that produce images closely matching the target light distributions."}
{"id": "2511.08659", "categories": ["cs.MA", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.08659", "abs": "https://arxiv.org/abs/2511.08659", "authors": ["Dave de Jonge"], "title": "Introduction to Automated Negotiation", "comment": null, "summary": "This book is an introductory textbook targeted towards computer science students who are completely new to the topic of automated negotiation. It does not require any prerequisite knowledge, except for elementary mathematics and basic programming skills.\n  This book comes with an simple toy-world negotiation framework implemented in Python that can be used by the readers to implement their own negotiation algorithms and perform experiments with them. This framework is small and simple enough that any reader who does not like to work in Python should be able to re-implement it very quickly in any other programming language of their choice."}
{"id": "2511.08763", "categories": ["cs.HC", "cs.MA", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.08763", "abs": "https://arxiv.org/abs/2511.08763", "authors": ["Mincong", "Huang", "Stefan T. Radev"], "title": "Modeling multi-agent motion dynamics in immersive rooms", "comment": null, "summary": "Immersive rooms are increasingly popular augmented reality systems that support multi-agent interactions within a virtual world. However, despite extensive content creation and technological developments, insights about perceptually-driven social dynamics, such as the complex movement patterns during virtual world navigation, remain largely underexplored. Computational models of motion dynamics can help us understand the underlying mechanism of human interaction in immersive rooms and develop applications that better support spatially distributed interaction. In this work, we propose a new agent-based model of emergent human motion dynamics. The model represents human agents as simple spatial geometries in the room that relocate and reorient themselves based on the salient virtual spatial objects they approach. Agent motion is modeled as an interactive process combining external diffusion-driven influences from the environment with internal self-propelling interactions among agents. Further, we leverage simulation-based inference (SBI) to show that the governing parameters of motion patterns can be estimated from simple observables. Our results indicate that the model successfully captures action-related agent properties but exposes local non-identifiability linked to environmental awareness. We argue that our simulation-based approach paves the way for creating adaptive, responsive immersive rooms -- spaces that adjust their interfaces and interactions based on human collective movement patterns and spatial attention."}
{"id": "2511.08978", "categories": ["cs.MM", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08978", "abs": "https://arxiv.org/abs/2511.08978", "authors": ["Jingtian Ma", "Jingyuan Wang", "Wayne Xin Zhao", "Guoping Liu", "Xiang Wen"], "title": "Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding", "comment": null, "summary": "Nowadays, navigation and ride-sharing apps have collected numerous images with spatio-temporal data. A core technology for analyzing such images, associated with spatiotemporal information, is Traffic Scene Understanding (TSU), which aims to provide a comprehensive description of the traffic scene. Unlike traditional spatio-temporal data analysis tasks, the dependence on both spatio-temporal and visual-textual data introduces distinct challenges to TSU task. However, recent research often treats TSU as a common image understanding task, ignoring the spatio-temporal information and overlooking the interrelations between different aspects of the traffic scene. To address these issues, we propose a novel SpatioTemporal Enhanced Model based on CILP (ST-CLIP) for TSU. Our model uses the classic vision-language model, CLIP, as the backbone, and designs a Spatio-temporal Context Aware Multiaspect Prompt (SCAMP) learning method to incorporate spatiotemporal information into TSU. The prompt learning method consists of two components: A dynamic spatio-temporal context representation module that extracts representation vectors of spatio-temporal data for each traffic scene image, and a bi-level ST-aware multi-aspect prompt learning module that integrates the ST-context representation vectors into word embeddings of prompts for the CLIP model. The second module also extracts low-level visual features and image-wise high-level semantic features to exploit interactive relations among different aspects of traffic scenes. To the best of our knowledge, this is the first attempt to integrate spatio-temporal information into visionlanguage models to facilitate TSU task. Experiments on two realworld datasets demonstrate superior performance in the complex scene understanding scenarios with a few-shot learning strategy."}
{"id": "2511.08710", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08710", "abs": "https://arxiv.org/abs/2511.08710", "authors": ["Romain Cosentino", "Sarath Shekkizhar", "Adam Earle"], "title": "Convergence dynamics of Agent-to-Agent Interactions with Misaligned objectives", "comment": null, "summary": "We develop a theoretical framework for agent-to-agent interactions in multi-agent scenarios. We consider the setup in which two language model based agents perform iterative gradient updates toward their respective objectives in-context, using the output of the other agent as input. We characterize the generation dynamics associated with the interaction when the agents have misaligned objectives, and show that this results in a biased equilibrium where neither agent reaches its target - with the residual errors predictable from the objective gap and the geometry induced by the prompt of each agent. We establish the conditions for asymmetric convergence and provide an algorithm that provably achieves an adversarial result, producing one-sided success. Experiments with trained transformer models as well as GPT$5$ for the task of in-context linear regression validate the theory. Our framework presents a setup to study, predict, and defend multi-agent systems; explicitly linking prompt design and interaction setup to stability, bias, and robustness."}
{"id": "2511.08880", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.08880", "abs": "https://arxiv.org/abs/2511.08880", "authors": ["Chayapatr Archiwaranguprok", "Constanze Albrecht", "Pattie Maes", "Karrie Karahalios", "Pat Pataranutaporn"], "title": "Simulating Psychological Risks in Human-AI Interactions: Real-Case Informed Modeling of AI-Induced Addiction, Anorexia, Depression, Homicide, Psychosis, and Suicide", "comment": "24 pages, 7 figures, 6 tables", "summary": "As AI systems become increasingly integrated into daily life, their potential to exacerbate or trigger severe psychological harms remains poorly understood and inadequately tested. This paper presents a proactive methodology for systematically exploring psychological risks in simulated human-AI interactions based on documented real-world cases involving AI-induced or AI-exacerbated addiction, anorexia, depression, homicide, psychosis, and suicide. We collected and analyzed 18 reported real-world cases where AI interactions contributed to severe psychological outcomes. From these cases, we developed a process to extract harmful interaction patterns and assess potential risks through 2,160 simulated scenarios using clinical staging models. We tested four major LLMs across multi-turn conversations to identify where psychological risks emerge: which harm domains, conversation stages, and contexts reveal system vulnerabilities. Through the analysis of 157,054 simulated conversation turns, we identify critical gaps in detecting psychological distress, responding appropriately to vulnerable users, and preventing harm escalation. Regression analysis reveals variability across persona types: LLMs tend to perform worse with elderly users but better with low- and middle-income groups compared to high-income groups. Clustering analysis of harmful responses reveals a taxonomy of fifteen distinct failure patterns organized into four categories of AI-enabled harm. This work contributes a novel methodology for identifying psychological risks, empirical evidence of common failure modes across systems, and a classification of harmful AI response patterns in high-stakes human-AI interactions."}
{"id": "2511.09448", "categories": ["cs.MM", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09448", "abs": "https://arxiv.org/abs/2511.09448", "authors": ["Lipisha Chaudhary", "Trisha Mittal", "Subhadra Gopalakrishnan", "Ifeoma Nwogu", "Jaclyn Pytlarz"], "title": "MCAD: Multimodal Context-Aware Audio Description Generation For Soccer", "comment": null, "summary": "Audio Descriptions (AD) are essential for making visual content accessible to individuals with visual impairments. Recent works have shown a promising step towards automating AD, but they have been limited to describing high-quality movie content using human-annotated ground truth AD in the process. In this work, we present an end-to-end pipeline, MCAD, that extends AD generation beyond movies to the domain of sports, with a focus on soccer games, without relying on ground truth AD. To address the absence of domain-specific AD datasets, we fine-tune a Video Large Language Model on publicly available movie AD datasets so that it learns the narrative structure and conventions of AD. During inference, MCAD incorporates multimodal contextual cues such as player identities, soccer events and actions, and commentary from the game. These cues, combined with input prompts to the fine-tuned VideoLLM, allow the system to produce complete AD text for each video segment. We further introduce a new evaluation metric, ARGE-AD, designed to accurately assess the quality of generated AD. ARGE-AD evaluates the generated AD for the presence of five characteristics: (i) usage of people's names, (ii) mention of actions and events, (iii) appropriate length of AD, (iv) absence of pronouns, and (v) overlap from commentary or subtitles. We present an in-depth analysis of our approach on both movie and soccer datasets. We also validate the use of this metric to quantitatively comment on the quality of generated AD using our metric across domains. Additionally, we contribute audio descriptions for 100 soccer game clips annotated by two AD experts."}
{"id": "2511.08762", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2511.08762", "abs": "https://arxiv.org/abs/2511.08762", "authors": ["Abdulkadir Bilge", "Eren Akyol", "Murat Kuscu"], "title": "Reservoir Computing-Based Detection for Molecular Communications", "comment": null, "summary": "Diffusion-based Molecular Communication (MC) is inherently challenged by severe inter-symbol interference (ISI). This is significantly amplified in mobile scenarios, where the channel impulse response (CIR) becomes time-varying and stochastic. Obtaining accurate Channel State Information (CSI) for traditional model-based detection is intractable in such dynamic environments. While deep learning (DL) offers adaptability, its complexity is unsuitable for resource-constrained micro/nanodevices. This paper proposes a low-complexity Reservoir Computing (RC) based detector. The RC architecture utilizes a fixed, recurrent non-linear reservoir to project the time-varying received signal into a high-dimensional state space. This effectively transforms the complex temporal detection problem into a simple linear classification task, capturing ISI dynamics without explicit channel modeling or complex retraining. Evaluated in a realistic 3D mobile MC simulation environment (Smoldyn), our RC detector significantly outperforms classical detectors and achieves superior performance compared to complex ML methods (LSTM, CNN, MLP) under severe ISI. Importantly, RC achieves this with significantly fewer trainable parameters (e.g., 300 vs. up to 264k for MLP) and ultra-low latency inference (approx. 1 $Î¼$s per symbol)."}
{"id": "2511.08926", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08926", "abs": "https://arxiv.org/abs/2511.08926", "authors": ["Zhuhui Li", "Chunbo Luo", "Liming Huang", "Luyu Qi", "Geyong Min"], "title": "Achieving Equilibrium under Utility Heterogeneity: An Agent-Attention Framework for Multi-Agent Multi-Objective Reinforcement Learning", "comment": null, "summary": "Multi-agent multi-objective systems (MAMOS) have emerged as powerful frameworks for modelling complex decision-making problems across various real-world domains, such as robotic exploration, autonomous traffic management, and sensor network optimisation. MAMOS offers enhanced scalability and robustness through decentralised control and more accurately reflects inherent trade-offs between conflicting objectives. In MAMOS, each agent uses utility functions that map return vectors to scalar values. Existing MAMOS optimisation methods face challenges in handling heterogeneous objective and utility function settings, where training non-stationarity is intensified due to private utility functions and the associated policies. In this paper, we first theoretically prove that direct access to, or structured modeling of, global utility functions is necessary for the Bayesian Nash Equilibrium under decentralised execution constraints. To access the global utility functions while preserving the decentralised execution, we propose an Agent-Attention Multi-Agent Multi-Objective Reinforcement Learning (AA-MAMORL) framework. Our approach implicitly learns a joint belief over other agents' utility functions and their associated policies during centralised training, effectively mapping global states and utilities to each agent's policy. In execution, each agent independently selects actions based on local observations and its private utility function to approximate a BNE, without relying on inter-agent communication. We conduct comprehensive experiments in both a custom-designed MAMO Particle environment and the standard MOMALand benchmark. The results demonstrate that access to global preferences and our proposed AA-MAMORL significantly improve performance and consistently outperform state-of-the-art methods."}
{"id": "2511.08917", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08917", "abs": "https://arxiv.org/abs/2511.08917", "authors": ["Kapil Garg", "Xinru Tang", "Jimin Heo", "Dwayne R. Morgan", "Darren Gergle", "Erik B. Sudderth", "Anne Marie Piper"], "title": "\"It's trained by non-disabled people\": Evaluating How Image Quality Affects Product Captioning with VLMs", "comment": "Paper under review", "summary": "Vision-Language Models (VLMs) are increasingly used by blind and low-vision (BLV) people to identify and understand products in their everyday lives, such as food, personal products, and household goods. Despite their prevalence, we lack an empirical understanding of how common image quality issues, like blur and misframing of items, affect the accuracy of VLM-generated captions and whether resulting captions meet BLV people's information needs. Grounded in a survey with 86 BLV people, we systematically evaluate how image quality issues affect captions generated by VLMs. We show that the best model recognizes products in images with no quality issues with 98% accuracy, but drops to 75% accuracy overall when quality issues are present, worsening considerably as issues compound. We discuss the need for model evaluations that center on disabled people's experiences throughout the process and offer concrete recommendations for HCI and ML researchers to make VLMs more reliable for BLV people."}
{"id": "2511.08971", "categories": ["cs.HC", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.08971", "abs": "https://arxiv.org/abs/2511.08971", "authors": ["Sicheng Yang", "Yukai Huang", "Weitong Cai", "Shitong Sun", "You He", "Jiankang Deng", "Hang Zhang", "Jifei Song", "Zhensong Zhang"], "title": "Plug-and-Play Clarifier: A Zero-Shot Multimodal Framework for Egocentric Intent Disambiguation", "comment": "16 pages, 9 figures, AAAI 2026", "summary": "The performance of egocentric AI agents is fundamentally limited by multimodal intent ambiguity. This challenge arises from a combination of underspecified language, imperfect visual data, and deictic gestures, which frequently leads to task failure. Existing monolithic Vision-Language Models (VLMs) struggle to resolve these multimodal ambiguous inputs, often failing silently or hallucinating responses. To address these ambiguities, we introduce the Plug-and-Play Clarifier, a zero-shot and modular framework that decomposes the problem into discrete, solvable sub-tasks. Specifically, our framework consists of three synergistic modules: (1) a text clarifier that uses dialogue-driven reasoning to interactively disambiguate linguistic intent, (2) a vision clarifier that delivers real-time guidance feedback, instructing users to adjust their positioning for improved capture quality, and (3) a cross-modal clarifier with grounding mechanism that robustly interprets 3D pointing gestures and identifies the specific objects users are pointing to. Extensive experiments demonstrate that our framework improves the intent clarification performance of small language models (4--8B) by approximately 30%, making them competitive with significantly larger counterparts. We also observe consistent gains when applying our framework to these larger models. Furthermore, our vision clarifier increases corrective guidance accuracy by over 20%, and our cross-modal clarifier improves semantic answer accuracy for referential grounding by 5%. Overall, our method provides a plug-and-play framework that effectively resolves multimodal ambiguity and significantly enhances user experience in egocentric interaction."}
{"id": "2511.09151", "categories": ["cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.09151", "abs": "https://arxiv.org/abs/2511.09151", "authors": ["Mu Zhou", "Junbin Long", "Yubiao Luo", "Zhong Sun"], "title": "Modeling Closed-loop Analog Matrix Computing Circuits with Interconnect Resistance", "comment": null, "summary": "Analog matrix computing (AMC) circuits based on resistive random-access memory (RRAM) have shown strong potential for accelerating matrix operations. However, as matrix size grows, interconnect resistance increasingly degrades computational accuracy and limits circuit scalability. Modeling and evaluating these effects are therefore critical for developing effective mitigation strategies. Traditional SPICE (Simulation Program with Integrated Circuit Emphasis) simulators, which rely on modified nodal analysis, become prohibitively slow for large-scale AMC circuits due to the quadratic growth of nodes and feedback connections. In this work, we model AMC circuits with interconnect resistance for two key operations-matrix inversion (INV) and eigenvector computation (EGV), and propose fast solving algorithms tailored for each case. The algorithms exploit the sparsity of the Jacobian matrix, enabling rapid and accurate solutions. Compared to SPICE, they achieve several orders of magnitude acceleration while maintaining high accuracy. We further extend the approach to open-loop matrix-vector multiplication (MVM) circuits, demonstrating similar efficiency gains. Finally, leveraging these fast solvers, we develop a bias-based compensation strategy that reduces interconnect-induced errors by over 50% for INV and 70% for EGV circuits. It also reveals the scaling behavior of the optimal bias with respect to matrix size and interconnect resistance."}
{"id": "2511.09171", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2511.09171", "abs": "https://arxiv.org/abs/2511.09171", "authors": ["Xinren Zhang", "Jiadong Yu", "Zixin Zhong"], "title": "Learning Efficient Communication Protocols for Multi-Agent Reinforcement Learning", "comment": null, "summary": "Multi-Agent Systems (MAS) have emerged as a powerful paradigm for modeling complex interactions among autonomous entities in distributed environments. In Multi-Agent Reinforcement Learning (MARL), communication enables coordination but can lead to inefficient information exchange, since agents may generate redundant or non-essential messages. While prior work has focused on boosting task performance with information exchange, the existing research lacks a thorough investigation of both the appropriate definition and the optimization of communication protocols (communication topology and message). To fill this gap, we introduce a generalized framework for learning multi-round communication protocols that are both effective and efficient. Within this framework, we propose three novel Communication Efficiency Metrics (CEMs) to guide and evaluate the learning process: the Information Entropy Efficiency Index (IEI) and Specialization Efficiency Index (SEI) for efficiency-augmented optimization, and the Topology Efficiency Index (TEI) for explicit evaluation. We integrate IEI and SEI as the adjusted loss functions to promote informative messaging and role specialization, while using TEI to quantify the trade-off between communication volume and task performance. Through comprehensive experiments, we demonstrate that our learned communication protocol can significantly enhance communication efficiency and achieves better cooperation performance with improved success rates."}
{"id": "2511.08971", "categories": ["cs.HC", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.08971", "abs": "https://arxiv.org/abs/2511.08971", "authors": ["Sicheng Yang", "Yukai Huang", "Weitong Cai", "Shitong Sun", "You He", "Jiankang Deng", "Hang Zhang", "Jifei Song", "Zhensong Zhang"], "title": "Plug-and-Play Clarifier: A Zero-Shot Multimodal Framework for Egocentric Intent Disambiguation", "comment": "16 pages, 9 figures, AAAI 2026", "summary": "The performance of egocentric AI agents is fundamentally limited by multimodal intent ambiguity. This challenge arises from a combination of underspecified language, imperfect visual data, and deictic gestures, which frequently leads to task failure. Existing monolithic Vision-Language Models (VLMs) struggle to resolve these multimodal ambiguous inputs, often failing silently or hallucinating responses. To address these ambiguities, we introduce the Plug-and-Play Clarifier, a zero-shot and modular framework that decomposes the problem into discrete, solvable sub-tasks. Specifically, our framework consists of three synergistic modules: (1) a text clarifier that uses dialogue-driven reasoning to interactively disambiguate linguistic intent, (2) a vision clarifier that delivers real-time guidance feedback, instructing users to adjust their positioning for improved capture quality, and (3) a cross-modal clarifier with grounding mechanism that robustly interprets 3D pointing gestures and identifies the specific objects users are pointing to. Extensive experiments demonstrate that our framework improves the intent clarification performance of small language models (4--8B) by approximately 30%, making them competitive with significantly larger counterparts. We also observe consistent gains when applying our framework to these larger models. Furthermore, our vision clarifier increases corrective guidance accuracy by over 20%, and our cross-modal clarifier improves semantic answer accuracy for referential grounding by 5%. Overall, our method provides a plug-and-play framework that effectively resolves multimodal ambiguity and significantly enhances user experience in egocentric interaction."}
{"id": "2511.09303", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2511.09303", "abs": "https://arxiv.org/abs/2511.09303", "authors": ["Alaa Awad Abdellatif", "Sergio Silva", "Eduardo Baltazar", "Bruno Oliveira", "Senhui Qiu", "Mohammud J. Bocus", "Kerstin Eder", "Robert J. Piechocki", "Nuno T. Almeida", "Helder Fontes"], "title": "RIoT Digital Twin: Modeling, Deployment, and Optimization of Reconfigurable IoT System with Optical-Radio Wireless Integration", "comment": null, "summary": "This paper proposes an optimized Reconfigurable Internet of Things (RIoT) framework that integrates optical and radio wireless technologies with a focus on energy efficiency, scalability, and adaptability. To address the inherent complexity of hybrid optical-radio environments, a high-fidelity Digital Twin (DT) is developed within the Network Simulator 3 (NS-3) platform. The DT models deploy subsystems of the RIoT architecture, including radio frequency (RF) communication, optical wireless communication (OWC), and energy harvesting and consumption mechanisms that enable autonomous operation. Real-time energy and power measurements from target hardware platforms are also incorporated to ensure accurate representation of physical behavior and enable runtime analysis and optimization. Building on this foundation, a proactive cross-layer optimization strategy is devised to balance energy efficiency and quality of service (QoS). The strategy dynamically reconfigures RIoT nodes by adapting transmission rates, wake/sleep scheduling, and access technology selection. Results demonstrate that the proposed framework, combining digital twin technology, hybrid optical-radio integration, and data-driven energy modeling, substantially enhances the performance, resilience, and sustainability of 6G IoT networks."}
{"id": "2511.09193", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09193", "abs": "https://arxiv.org/abs/2511.09193", "authors": ["Egor Yukhnevich", "Anton Andreychuk"], "title": "Enhancing PIBT via Multi-Action Operations", "comment": null, "summary": "PIBT is a rule-based Multi-Agent Path Finding (MAPF) solver, widely used as a low-level planner or action sampler in many state-of-the-art approaches. Its primary advantage lies in its exceptional speed, enabling action selection for thousands of agents within milliseconds by considering only the immediate next timestep. However, this short-horizon design leads to poor performance in scenarios where agents have orientation and must perform time-consuming rotation actions. In this work, we present an enhanced version of PIBT that addresses this limitation by incorporating multi-action operations. We detail the modifications introduced to improve PIBT's performance while preserving its hallmark efficiency. Furthermore, we demonstrate how our method, when combined with graph-guidance technique and large neighborhood search optimization, achieves state-of-the-art performance in the online LMAPF-T setting."}
{"id": "2511.09240", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.09240", "abs": "https://arxiv.org/abs/2511.09240", "authors": ["Jinghao Huang", "Siqi Yao", "Yu Zhang"], "title": "SimPath: Mitigating Motion Sickness in In - vehicle Infotainment Systems via Driving Condition Adaptation", "comment": "10 pages, 12 figures", "summary": "The problem of Motion Sickness (MS) among passengers significantly impacts the comfort and efficiency of In-Vehicle Infotainment Systems (IVIS) use. In this study, we innovatively designed SimPath, a visual design to effectively mitigate passengers' MS and boost their efficiency of using IVIS during driving. The study focuses on the problem of irregular motion conditions frequently encountered during actual driving. To validate the efficacy of this approach, two sets of real - vehicle experiments were carried out in real driving scenarios. The results demonstrate that this approach significantly reduces passenger's MS level to a certain extent. However, due to divided attention from visual content, it does not directly improve the IVIS efficiency. In conclusion, this study offers crucial insights for the design of a more intelligent and user friendly IVIS, based on the discussion of the principle, providing strong theoretical support and practical guidance for the development of future IVIS in autonomous vehicles."}
{"id": "2511.08763", "categories": ["cs.HC", "cs.MA", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.08763", "abs": "https://arxiv.org/abs/2511.08763", "authors": ["Mincong", "Huang", "Stefan T. Radev"], "title": "Modeling multi-agent motion dynamics in immersive rooms", "comment": null, "summary": "Immersive rooms are increasingly popular augmented reality systems that support multi-agent interactions within a virtual world. However, despite extensive content creation and technological developments, insights about perceptually-driven social dynamics, such as the complex movement patterns during virtual world navigation, remain largely underexplored. Computational models of motion dynamics can help us understand the underlying mechanism of human interaction in immersive rooms and develop applications that better support spatially distributed interaction. In this work, we propose a new agent-based model of emergent human motion dynamics. The model represents human agents as simple spatial geometries in the room that relocate and reorient themselves based on the salient virtual spatial objects they approach. Agent motion is modeled as an interactive process combining external diffusion-driven influences from the environment with internal self-propelling interactions among agents. Further, we leverage simulation-based inference (SBI) to show that the governing parameters of motion patterns can be estimated from simple observables. Our results indicate that the model successfully captures action-related agent properties but exposes local non-identifiability linked to environmental awareness. We argue that our simulation-based approach paves the way for creating adaptive, responsive immersive rooms -- spaces that adjust their interfaces and interactions based on human collective movement patterns and spatial attention."}
{"id": "2511.09309", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09309", "abs": "https://arxiv.org/abs/2511.09309", "authors": ["Yiwen Yin", "Zhian Hu", "Xiaoxi Xu", "Chun Yu", "Xintong Wu", "Wenyu Fan", "Yuanchun Shi"], "title": "TaskSense: Cognitive Chain Modeling and Difficulty Estimation for GUI Tasks", "comment": "22 pages, 5 figures", "summary": "Measuring GUI task difficulty is crucial for user behavior analysis and agent capability evaluation. Yet, existing benchmarks typically quantify difficulty based on motor actions (e.g., step counts), overlooking the cognitive demands underlying task completion. In this work, we propose Cognitive Chain, a novel framework that models task difficulty from a cognitive perspective. A cognitive chain decomposes the cognitive processes preceding a motor action into a sequence of cognitive steps (e.g., finding, deciding, computing), each with a difficulty index grounded in information theories. We develop an LLM-based method to automatically extract cognitive chains from task execution traces. Validation with linear regression shows that our estimated cognitive difficulty correlates well with user completion time (step-level R-square=0.46 after annotation). Assessment of state-of-the-art GUI agents shows reduced success on cognitively demanding tasks, revealing capability gaps and Human-AI consistency patterns. We conclude by discussing potential applications in agent training, capability assessment, and human-agent delegation optimization."}
{"id": "2511.09337", "categories": ["cs.HC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.09337", "abs": "https://arxiv.org/abs/2511.09337", "authors": ["Ziyong Ma", "Richard D. Boyce", "Adam Perer", "Venkatesh Sivaraman"], "title": "TempoQL: A Readable, Precise, and Portable Query System for Electronic Health Record Data", "comment": "Accepted as a Proceedings paper at Machine Learning for Health (ML4H) 2025", "summary": "Electronic health record (EHR) data is an essential data source for machine learning for health, but researchers and clinicians face steep barriers in extracting and validating EHR data for modeling. Existing tools incur trade-offs between expressivity and usability and are typically specialized to a single data standard, making it difficult to write temporal queries that are ready for modern model-building pipelines and adaptable to new datasets. This paper introduces TempoQL, a Python-based toolkit designed to lower these barriers. TempoQL provides a simple, human-readable language for temporal queries; support for multiple EHR data standards, including OMOP, MEDS, and others; and an interactive notebook-based query interface with optional large language model (LLM) authoring assistance. Through a performance evaluation and two use cases on different datasets, we demonstrate that TempoQL simplifies the creation of cohorts for machine learning while maintaining precision, speed, and reproducibility."}
{"id": "2511.09394", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.09394", "abs": "https://arxiv.org/abs/2511.09394", "authors": ["Danli Shi", "Xiaolan Chen", "Bingjie Yan", "Weiyi Zhang", "Pusheng Xu", "Jiancheng Yang", "Ruoyu Chen", "Siyu Huang", "Bowen Liu", "Xinyuan Wu", "Meng Xie", "Ziyu Gao", "Yue Wu", "Senlin Lin", "Kai Jin", "Xia Gong", "Yih Chung Tham", "Xiujuan Zhang", "Li Dong", "Yuzhou Zhang", "Jason Yam", "Guangming Jin", "Xiaohu Ding", "Haidong Zou", "Yalin Zheng", "Zongyuan Ge", "Mingguang He"], "title": "A multimodal AI agent for clinical decision support in ophthalmology", "comment": "28 pages, 5 figures", "summary": "Artificial intelligence has shown promise in medical imaging, yet most existing systems lack flexibility, interpretability, and adaptability - challenges especially pronounced in ophthalmology, where diverse imaging modalities are essential. We present EyeAgent, the first agentic AI framework for comprehensive and interpretable clinical decision support in ophthalmology. Using a large language model (DeepSeek-V3) as its central reasoning engine, EyeAgent interprets user queries and dynamically orchestrates 53 validated ophthalmic tools across 23 imaging modalities for diverse tasks including classification, segmentation, detection, image/report generation, and quantitative analysis. Stepwise ablation analysis demonstrated a progressive improvement in diagnostic accuracy, rising from a baseline of 69.71% (using only 5 general tools) to 80.79% when the full suite of 53 specialized tools was integrated. In an expert rating study on 200 real-world clinical cases, EyeAgent achieved 93.7% tool selection accuracy and received expert ratings of more than 88% across accuracy, completeness, safety, reasoning, and interpretability. In human-AI collaboration, EyeAgent matched or exceeded the performance of senior ophthalmologists and, when used as an assistant, improved overall diagnostic accuracy by 18.51% and report quality scores by 19%, with the greatest benefit observed among junior ophthalmologists. These findings establish EyeAgent as a scalable and trustworthy AI framework for ophthalmology and provide a blueprint for modular, multimodal, and clinically aligned next-generation AI systems."}
{"id": "2511.09454", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.GT", "econ.GN"], "pdf": "https://arxiv.org/pdf/2511.09454", "abs": "https://arxiv.org/abs/2511.09454", "authors": ["Tobias R. Rebholz", "Maxwell Uphoff", "Christian H. R. Bernges", "Florian Scholten"], "title": "Algorithmic Advice as a Strategic Signal on Competitive Markets", "comment": null, "summary": "As algorithms increasingly mediate competitive decision-making, their influence extends beyond individual outcomes to shaping strategic market dynamics. In two preregistered experiments, we examined how algorithmic advice affects human behavior in classic economic games with unique, non-collusive, and analytically traceable equilibria. In Experiment 1 (N = 107), participants played a Bertrand price competition with individualized or collective algorithmic recommendations. Initially, collusively upward-biased advice increased prices, particularly when individualized, but prices gradually converged toward equilibrium over the course of the experiment. However, participants avoided setting prices above the algorithm's recommendation throughout the experiment, suggesting that advice served as a soft upper bound for acceptable prices. In Experiment 2 (N = 129), participants played a Cournot quantity competition with equilibrium-aligned or strategically biased algorithmic recommendations. Here, individualized equilibrium advice supported stable convergence, whereas collusively downward-biased advice led to sustained underproduction and supracompetitive profits - hallmarks of tacit collusion. In both experiments, participants responded more strongly and consistently to individualized advice than collective advice, potentially due to greater perceived ownership of the former. These findings demonstrate that algorithmic advice can function as a strategic signal, shaping coordination even without explicit communication. The results echo real-world concerns about algorithmic collusion and underscore the need for careful design and oversight of algorithmic decision-support systems in competitive environments."}
{"id": "2511.09458", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.09458", "abs": "https://arxiv.org/abs/2511.09458", "authors": ["Rahul R. Divekar", "Sophia Guerra", "Lisette Gonzalez", "Natasha Boos"], "title": "Exploring The Interaction-Outcome Paradox: Seemingly Richer and More Self-Aware Interactions with LLMs May Not Yet Lead to Better Learning", "comment": null, "summary": "While Large Language Models (LLMs) have transformed the user interface for learning, moving from keyword search to natural language dialogue, their impact on educational outcomes remains unclear. We present a controlled study (N=20) that directly compares the learning interaction and outcomes between LLM and search-based interfaces. We found that although LLMs elicit richer and nuanced interactions from a learner, they do not produce broadly better learning outcomes. In this paper, we explore this the ``Interaction-Outcome Paradox.'' To explain this, we discuss the concept of a cognitive shift: the locus of student effort moves from finding and synthesizing disparate sources (search) to a more self-aware identification and articulation of their knowledge gaps and strategies to bridge those gaps (LLMs). This insight provides a new lens for evaluating educational technologies, suggesting that the future of learning tools lies not in simply enriching interaction, but in designing systems that scaffold productive cognitive work by leveraging this student expressiveness."}
