{"id": "2511.22924", "categories": ["cs.MA", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.22924", "abs": "https://arxiv.org/abs/2511.22924", "authors": ["Kaixiang Wang", "Zhaojiacheng Zhou", "Bunyod Suvonov", "Jiong Lou", "Jie LI"], "title": "AgentShield: Make MAS more secure and efficient", "comment": null, "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) offer powerful cooperative reasoning but remain vulnerable to adversarial attacks, where compromised agents can undermine the system's overall performance. Existing defenses either depend on single trusted auditors, creating single points of failure, or sacrifice efficiency for robustness. To resolve this tension, we propose \\textbf{AgentShield}, a distributed framework for efficient, decentralized auditing. AgentShield introduces a novel three-layer defense: \\textbf{(i) Critical Node Auditing} prioritizes high-influence agents via topological analysis; \\textbf{(ii) Light Token Auditing} implements a cascade protocol using lightweight sentry models for rapid discriminative verification; and \\textbf{(iii) Two-Round Consensus Auditing} triggers heavyweight arbiters only upon uncertainty to ensure global agreement. This principled design optimizes the robustness-efficiency trade-off. Experiments demonstrate that AgentShield achieves a 92.5\\% recovery rate and reduces auditing overhead by over 70\\% compared to existing methods, maintaining high collaborative accuracy across diverse MAS topologies and adversarial scenarios.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22420", "categories": ["cs.HC", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22420", "abs": "https://arxiv.org/abs/2511.22420", "authors": ["Sebe Vanbrabant", "Gustavo Rovelo Ruiz", "Davy Vanacken"], "title": "MATCH: Engineering Transparent and Controllable Conversational XAI Systems through Composable Building Blocks", "comment": "Submitted Version accepted for publication in an LNCS Volume \"Engineering Interactive Computer Systems - EICS 2025 - International Workshops and Doctoral Consortium\"", "summary": "While the increased integration of AI technologies into interactive systems enables them to solve an increasing number of tasks, the black-box problem of AI models continues to spread throughout the interactive system as a whole. Explainable AI (XAI) techniques can make AI models more accessible by employing post-hoc methods or transitioning to inherently interpretable models. While this makes individual AI models clearer, the overarching system architecture remains opaque. This challenge not only pertains to standard XAI techniques but also to human examination and conversational XAI approaches that need access to model internals to interpret them correctly and completely. To this end, we propose conceptually representing such interactive systems as sequences of structural building blocks. These include the AI models themselves, as well as control mechanisms grounded in literature. The structural building blocks can then be explained through complementary explanatory building blocks, such as established XAI techniques like LIME and SHAP. The flow and APIs of the structural building blocks form an unambiguous overview of the underlying system, serving as a communication basis for both human and automated agents, thus aligning human and machine interpretability of the embedded AI models. In this paper, we present our flow-based approach and a selection of building blocks as MATCH: a framework for engineering Multi-Agent Transparent and Controllable Human-centered systems. This research contributes to the field of (conversational) XAI by facilitating the integration of interpretability into existing interactive systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.21697", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.21697", "abs": "https://arxiv.org/abs/2511.21697", "authors": ["Julien Philip", "Li Ma", "Pascal Clausen", "Wenqi Xian", "Ahmet Levent Ta\u015fel", "Mingming He", "Xueming Yu", "David M. George", "Ning Yu", "Oliver Pilarski", "Paul Debevec"], "title": "Detail Enhanced Gaussian Splatting for Large-Scale Volumetric Capture", "comment": "10 pages, Accepted as a Journal paper at Siggraph Asia 2025. Webpage: https://eyeline-labs.github.io/DEGS/", "summary": "We present a unique system for large-scale, multi-performer, high resolution 4D volumetric capture providing realistic free-viewpoint video up to and including 4K resolution facial closeups. To achieve this, we employ a novel volumetric capture, reconstruction and rendering pipeline based on Dynamic Gaussian Splatting and Diffusion-based Detail Enhancement. We design our pipeline specifically to meet the demands of high-end media production. We employ two capture rigs: the Scene Rig, which captures multi-actor performances at a resolution which falls short of 4K production quality, and the Face Rig, which records high-fidelity single-actor facial detail to serve as a reference for detail enhancement. We first reconstruct dynamic performances from the Scene Rig using 4D Gaussian Splatting, incorporating new model designs and training strategies to improve reconstruction, dynamic range, and rendering quality. Then to render high-quality images for facial closeups, we introduce a diffusion-based detail enhancement model. This model is fine-tuned with high-fidelity data from the same actors recorded in the Face Rig. We train on paired data generated from low- and high-quality Gaussian Splatting (GS) models, using the low-quality input to match the quality of the Scene Rig, with the high-quality GS as ground truth. Our results demonstrate the effectiveness of this pipeline in bridging the gap between the scalable performance capture of a large-scale rig and the high-resolution standards required for film and media production.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.21994", "categories": ["cs.HC", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21994", "abs": "https://arxiv.org/abs/2511.21994", "authors": ["Megan Zheng", "Will Crichton", "Akshay Narayan", "Deepti Raghavan", "Nikos Vasilakis"], "title": "When Are Reactive Notebooks Not Reactive?", "comment": null, "summary": "Computational notebooks are convenient for programmers, but can easily become confusing and inconsistent due to the ability to incrementally edit a program that is running. Recent reactive notebook systems, such as Ipyflow, Marimo and Observable, strive to keep notebook state in sync with the current cell code by re-executing a minimal set of cells upon modification. However, each system defines reactivity a different way. Additionally, within any definition, we find simple notebook modifications that can break each system. Overall, these inconsistencies make it difficult for users to construct a mental model of their reactive notebook's implementation. This paper proposes Rex, a fine-grained test suite to discuss and assess reactivity capabilities within reactive notebook systems. We evaluate Rex on three existing reactive notebook systems and classify their failures with the aims of (i) helping programmers understand when reactivity fails and (ii) helping notebook implementations improve.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.21796", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2511.21796", "abs": "https://arxiv.org/abs/2511.21796", "authors": ["Shah Zayed Riam", "Zhenlin Pei", "Kyle Mooney", "Chenyun Pan", "Na Gong", "Jinhui Wang"], "title": "Sneak Path Current Modeling in Memristor Crossbar Arrays for Analog In-Memory Computing", "comment": null, "summary": "Memristor crossbar arrays have emerged as a key component for next-generation non-volatile memories, artificial neural networks, and analog in-memory computing (IMC) systems. By minimizing data transfer between the processor and memory, they offer substantial energy savings. However, a major design challenge in memristor crossbar arrays is the presence of sneak path currents, which degrade electrical performance, reduce noise margins, and limit reliable operations. This work presents a closed-form analytical framework based on IMEC A14 (1.4 nm) Technology for accurately estimating sneak path currents in memristor crossbar arrays. The proposed model captures the interdependence of key design parameters in memristor crossbar arrays, including array size, ON/OFF ratio of memristors, read voltage, and interconnect conditions, through mathematically derived relationships. It supports various practical configurations, such as different data patterns and connection strategies, enabling rapid and comprehensive sneak path current modeling. The sensitivity analysis includes how design parameters influence sneak path current and noise margin loss, underscoring the trade-offs involved in scaling crossbar arrays. Validation through SPICE simulations shows that the model achieves an error of less than 10.9% while being up to 4784 times faster than full circuit simulations. This analytical framework offers a powerful tool for quantitative assessment and pre-design/real-time optimization of memristor-based analog in-memory computing (IMC) architectures.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.21693", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.21693", "abs": "https://arxiv.org/abs/2511.21693", "authors": ["Joonhyung Bae", "Hyeyoon Cho", "Kirak Kim", "Dawon Park", "Taegyun Kwon", "Yoon-Seok Choi", "Hyeon Hur", "Shigeru Kai", "Yohei Wada", "Satoshi Obata", "Akira Maezawa", "Jaebum Park", "Jonghwa Park", "Juhan Nam"], "title": "Designing a Multimodal Viewer for Piano Performance Analysis -- a Pedagogy-First Approach", "comment": null, "summary": "Abstract instructions in piano education, such as \"raise your wrist\" and \"relax your tension,\" lead to varying interpretations among learners, preventing instructors from effectively conveying their intended pedagogical guidance. To address this problem, this study conducted systematic interviews with a piano professor with 18 years teaching experience, and two researchers derived seven core need groups through cross-validation. Based on these findings, we developed a web-based dashboard prototype integrating video, motion capture, and musical scores, enabling instructors to provide concrete, visual feedback instead of relying solely on abstract verbal instructions. Technical feasibility was validated through 109 performance datasets.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22288", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.22288", "abs": "https://arxiv.org/abs/2511.22288", "authors": ["Zhaorui Meng", "Lu Yin", "Yangqing Hou", "Anjun Chen", "Shihui Guo", "Yipeng Qin"], "title": "Improving Sparse IMU-based Motion Capture with Motion Label Smoothing", "comment": "Accepted by AAAI 2026", "summary": "Sparse Inertial Measurement Units (IMUs) based human motion capture has gained significant momentum, driven by the adaptation of fundamental AI tools such as recurrent neural networks (RNNs) and transformers that are tailored for temporal and spatial modeling. Despite these achievements, current research predominantly focuses on pipeline and architectural designs, with comparatively little attention given to regularization methods, highlighting a critical gap in developing a comprehensive AI toolkit for this task. To bridge this gap, we propose motion label smoothing, a novel method that adapts the classic label smoothing strategy from classification to the sparse IMU-based motion capture task. Specifically, we first demonstrate that a naive adaptation of label smoothing, including simply blending a uniform vector or a ``uniform'' motion representation (e.g., dataset-average motion or a canonical T-pose), is suboptimal; and argue that a proper adaptation requires increasing the entropy of the smoothed labels. Second, we conduct a thorough analysis of human motion labels, identifying three critical properties: 1) Temporal Smoothness, 2) Joint Correlation, and 3) Low-Frequency Dominance, and show that conventional approaches to entropy enhancement (e.g., blending Gaussian noise) are ineffective as they disrupt these properties. Finally, we propose the blend of a novel skeleton-based Perlin noise for motion label smoothing, designed to raise label entropy while satisfying motion properties. Extensive experiments applying our motion label smoothing to three state-of-the-art methods across four real-world IMU datasets demonstrate its effectiveness and robust generalization (plug-and-play) capability.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22056", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22056", "abs": "https://arxiv.org/abs/2511.22056", "authors": ["Fengze Li", "Jieming Ma", "Kan Liu", "Xiaohan Zhang", "Yangle Liu", "Yue Li"], "title": "EAST: Environment-Aware Stylized Transition Along the Reality-Virtuality Continuum", "comment": null, "summary": "In the Virtual Reality (VR) gaming industry, maintaining immersion during real-world interruptions remains a challenge, particularly during transitions along the reality-virtuality continuum (RVC). Existing methods tend to rely on digital replicas or simple visual transitions, neglecting to address the aesthetic discontinuities between real and virtual environments, especially in highly stylized VR games. This paper introduces the Environment-Aware Stylized Transition (EAST) framework, which employs a novel style-transferred 3D Gaussian Splatting (3DGS) technique to transfer real-world interruptions into the virtual environment with seamless aesthetic consistency. Rather than merely transforming the real world into game-like visuals, EAST minimizes the disruptive impact of interruptions by integrating real-world elements within the framework. Qualitative user studies demonstrate significant enhancements in cognitive comfort and emotional continuity during transitions, while quantitative experiments highlight EAST's ability to maintain visual coherence across diverse VR styles.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22408", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2511.22408", "abs": "https://arxiv.org/abs/2511.22408", "authors": ["Yizhi He", "Sayed Amir Hoseini", "Mahbub Hassan"], "title": "Quantifying Geometry Effects on Low-Cost Intelligent Reflecting Surfaces", "comment": "6 pages, 11 figures", "summary": "Intelligent Reflecting Surfaces (IRS) promise low-power coverage extension, yet practical deployments must curb hardware complexity and control overhead. This paper quantifies the performance impact of two cost-saving measures, column-wise element grouping and 1-bit (binary) phase quantization, relative to the ideal fully-controlled, continuous-phase baseline. A single-input single-output link is simulated at 26 GHz (mmWave) across three deployment geometries that vary the relative heights of access point, IRS and user equipment. Results show that switching from continuous to binary phase control reduces median SNR gain by approximately 4 dB, while adopting column-wise grouping introduces a similar penalty; combining both constraints incurs approximately 8 dB loss under height-offset deployments. When all nodes share the same height, the degradation from column-wise control becomes negligible, indicating deployment geometry can offset control-granularity limits. Despite the losses, a 32 x 32 column-wise binary IRS still delivers double-digit SNR gains over the no-IRS baseline in most positions, confirming its viability for cost-constrained scenarios. The study provides quantitative guidelines on when simplified IRS architectures can meet link-budget targets and where full element-wise control remains justified.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.21694", "categories": ["cs.MM", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.21694", "abs": "https://arxiv.org/abs/2511.21694", "authors": ["Meiyu Li", "Wei Ai", "Naeemul Hassan"], "title": "A Survey of Information Disorder on Video-Sharing Platforms", "comment": "Accepted by 2025 IEEE International Conference on Content-Based Multimedia Indexing", "summary": "Video sharing platforms (VSPs) have become central information hubs but also facilitate the spread of information disorder, from misleading narratives to fabricated content. This survey synthesizes research on VSPs' multimedia ecosystems across three dimensions: (1) types of information disorder, (2) methodological approaches, and (3) platform features. We conclude by identifying key challenges and open questions for future research.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.23029", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.23029", "abs": "https://arxiv.org/abs/2511.23029", "authors": ["Tai Inui", "Alexander Matsumura", "Edgar Simo-Serra"], "title": "Geodiffussr: Generative Terrain Texturing with Elevation Fidelity", "comment": null, "summary": "Large-scale terrain generation remains a labor-intensive task in computer graphics. We introduce Geodiffussr, a flow-matching pipeline that synthesizes text-guided texture maps while strictly adhering to a supplied Digital Elevation Map (DEM). The core mechanism is multi-scale content aggregation (MCA): DEM features from a pretrained encoder are injected into UNet blocks at multiple resolutions to enforce global-to-local elevation consistency. Compared with a non-MCA baseline, MCA markedly improves visual fidelity and strengthens height-appearance coupling (FID $\\downarrow$ 49.16%, LPIPS $\\downarrow$ 32.33%, $\u0394$dCor $\\downarrow$ to 0.0016). To train and evaluate Geodiffussr, we assemble a globally distributed, biome- and climate-stratified corpus of triplets pairing SRTM-derived DEMs with Sentinel-2 imagery and vision-grounded natural-language captions that describe visible land cover. We position Geodiffussr as a strong baseline and step toward controllable 2.5D landscape generation for coarse-scale ideation and previz, complementary to physically based terrain and ecosystem simulators.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.21698", "categories": ["cs.MM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21698", "abs": "https://arxiv.org/abs/2511.21698", "authors": ["Zhiyong Ma", "Jiahao Chen", "Qingyuan Chuai", "Zhengping Li"], "title": "TIP and Polish: Text-Image-Prototype Guided Multi-Modal Generation via Commonality-Discrepancy Modeling and Refinement", "comment": "Submitted to ICASSP2026", "summary": "Multi-modal generation struggles to ensure thematic coherence and style consistency. Semantically, existing methods suffer from cross-modal mismatch and lack explicit modeling of commonality and discrepancy. Methods that rely on fine-grained training fail to balance semantic precision with writing style consistency. These shortcomings lead to suboptimal generation quality. To tackle these issues, we propose \\textbf{\\textit{TIPPo}}, a simple yet effective framework with explicit input modeling and comprehensive optimization objectives. It extracts the input text and images via multi-modal encoder and adapters, then measures the visual prototype. \\textbf{T}extual, \\textbf{I}mage, and \\textbf{P}rototype signals are then fed to our proposed Dual Alignment Attention and Difference Operator modules before language model decoding. The proposed \\textbf{Po}lishPPO reinforces the style consistency, while the unsupervised contrastive learning during SFT mitigates inter-sample representation collapse. Experimental results demonstrate the promising performance of \\textbf{\\textit{TIPPo}} in automatic evaluation and LLM-based criteria for creativity and semantic consistency.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.23131", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.23131", "abs": "https://arxiv.org/abs/2511.23131", "authors": ["Manas Chaudhary", "Chandradeep Pokhariya", "Rahul Narain"], "title": "Towards Generalized Position-Based Dynamics", "comment": null, "summary": "The position-based dynamics (PBD) algorithm is a popular and versatile technique for real-time simulation of deformable bodies, but is only applicable to forces that can be expressed as linearly compliant constraints. In this work, we explore a generalization of PBD that is applicable to arbitrary nonlinear force models. We do this by reformulating the implicit time integration equations in terms of the individual forces in the system, to which applying Gauss-Seidel iterations naturally leads to a PBD-type algorithm. As we demonstrate, our method allows simulation of data-driven cloth models [Sperl et al. 2020] that cannot be represented by existing variations of position-based dynamics, enabling performance improvements over the baseline Newton-based solver for high mesh resolutions. We also show our method's applicability to volumetric neo-Hookean elasticity with an inversion barrier.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22337", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22337", "abs": "https://arxiv.org/abs/2511.22337", "authors": ["Sachin Kumar Singh", "Ko Watanabe", "Brian Moser", "Andreas Dengel"], "title": "HandyLabel: Towards Post-Processing to Real-Time Annotation Using Skeleton Based Hand Gesture Recognition", "comment": null, "summary": "The success of machine learning is deeply linked to the availability of high-quality training data, yet retrieving and manually labeling new data remains a time-consuming and error-prone process. Traditional annotation tools, such as Label Studio, often require post-processing, where users label data after it has been recorded. Post-processing is highly time-consuming and labor-intensive, especially with large datasets, and may lead to erroneous annotations due to the difficulty of subjects' memory tasks when labeling cognitive activities such as emotions or comprehension levels. In this work, we introduce HandyLabel, a real-time annotation tool that leverages hand gesture recognition to map hand signs for labeling. The application enables users to customize gesture mappings through a web-based interface, allowing for real-time annotations. To ensure the performance of HandyLabel, we evaluate several hand gesture recognition models on an open-source hand sign (HaGRID) dataset, with and without skeleton-based preprocessing. We discovered that ResNet50 with preprocessed skeleton-based images performs an F1-score of 0.923. To validate the usability of HandyLabel, a user study was conducted with 46 participants. The results suggest that 88.9% of participants preferred HandyLabel over traditional annotation tools.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.21780", "categories": ["cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.21780", "abs": "https://arxiv.org/abs/2511.21780", "authors": ["Yaoru Li", "Heyu Si", "Federico Landi", "Pilar Oplustil Gallegos", "Ioannis Koutsoumpas", "O. Ricardo Cortez Vazquez", "Ruiju Fu", "Qi Guo", "Xin Jin", "Shunyu Liu", "Mingli Song"], "title": "3MDiT: Unified Tri-Modal Diffusion Transformer for Text-Driven Synchronized Audio-Video Generation", "comment": null, "summary": "Text-to-video (T2V) diffusion models have recently achieved impressive visual quality, yet most systems still generate silent clips and treat audio as a secondary concern. Existing audio-video generation pipelines typically decompose the task into cascaded stages, which accumulate errors across modalities and are trained under separate objectives. Recent joint audio-video generators alleviate this issue but often rely on dual-tower architectures with ad-hoc cross-modal bridges and static, single-shot text conditioning, making it difficult to both reuse T2V backbones and to reason about how audio, video and language interact over time. To address these challenges, we propose 3MDiT, a unified tri-modal diffusion transformer for text-driven synchronized audio-video generation. Our framework models video, audio and text as jointly evolving streams: an isomorphic audio branch mirrors a T2V backbone, tri-modal omni-blocks perform feature-level fusion across the three modalities, and an optional dynamic text conditioning mechanism updates the text representation as audio and video evidence co-evolve. The design supports two regimes: training from scratch on audio-video data, and orthogonally adapting a pretrained T2V model without modifying its backbone. Experiments show that our approach generates high-quality videos and realistic audio while consistently improving audio-video synchronization and tri-modal alignment across a range of quantitative metrics.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22352", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22352", "abs": "https://arxiv.org/abs/2511.22352", "authors": ["Jarne Thys", "Davy Vanacken", "Gustavo Rovelo Ruiz"], "title": "Engineering Trustworthy Automation: Design Principles and Evaluation for AutoML Tools for Novices", "comment": "Submitted version accepted for publication in an LNCS Volume \"Engineering Interactive Computer Systems - EICS 2025 - International Workshops and Doctoral Consortium\"", "summary": "AutoML systems targeting novices often prioritize algorithmic automation over usability, leaving gaps in users' understanding, trust, and end-to-end workflow support. To address these issues, we propose an abstract pipeline that covers data intake, guided configuration, training, evaluation, and inference. To examine the abstract pipeline, we report a user study where we assess trust, understandability, and UX of a prototype implementation. In a 24-participant study, all participants successfully built their own models, UEQ ratings were positive, yet experienced users reported higher trust and understanding than novices. Based on this study, we propose four design principles to improve the design of AutoML systems targeting novices: (P1) support first-model success to enhance user self-efficacy, (P2) provide explanations to help users form correct mental models and develop appropriate levels of reliance, (P3) provide abstractions and context-aware assistance to keep users in their zone of proximal development, and (P4) ensure predictability and safeguards to strengthen users' sense of control.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22229", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.22229", "abs": "https://arxiv.org/abs/2511.22229", "authors": ["Yuyue Wang", "Xin Cheng", "Yihan Wu", "Xihua Wang", "Jinchuan Tian", "Ruihua Song"], "title": "VSpeechLM: A Visual Speech Language Model for Visual Text-to-Speech Task", "comment": "MM Asia 2025", "summary": "The task of Visual Text-to-Speech (VisualTTS), also known as video dubbing, aims to generate speech synchronized with the lip movements in an input video, in additional to being consistent with the content of input text and cloning the timbre of a reference speech. Existing VisualTTS models typically adopt lightweight architectures and design specialized modules to achieve the above goals respectively, yet the speech quality is not satisfied due to the model capacity and the limited data in VisualTTS. Recently, speech large language models (SpeechLLM) show the robust ability to generate high-quality speech. But few work has been done to well leverage temporal cues from video input in generating lip-synchronized speech. To generate both high-quality and lip-synchronized speech in VisualTTS tasks, we propose a novel Visual Speech Language Model called VSpeechLM based upon a SpeechLLM. To capture the synchronization relationship between text and video, we propose a text-video aligner. It first learns fine-grained alignment between phonemes and lip movements, and then outputs an expanded phoneme sequence containing lip-synchronization cues. Next, our proposed SpeechLLM based decoders take the expanded phoneme sequence as input and learns to generate lip-synchronized speech. Extensive experiments demonstrate that our VSpeechLM significantly outperforms previous VisualTTS methods in terms of overall quality, speaker similarity, and synchronization metrics.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22447", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.22447", "abs": "https://arxiv.org/abs/2511.22447", "authors": ["Xinyi Che", "Wenbo Wang", "Yuanbo Hou", "Mingjie Xie", "Qijun Zhao", "Jian Guan"], "title": "Angle-Optimized Partial Disentanglement for Multimodal Emotion Recognition in Conversation", "comment": "10 pages, 7 figures", "summary": "Multimodal Emotion Recognition in Conversation (MERC) aims to enhance emotion understanding by integrating complementary cues from text, audio, and visual modalities. Existing MERC approaches predominantly focus on cross-modal shared features, often overlooking modality-specific features that capture subtle yet critical emotional cues such as micro-expressions, prosodic variations, and sarcasm. Although related work in multimodal emotion recognition (MER) has explored disentangling shared and modality-specific features, these methods typically employ rigid orthogonal constraints to achieve full disentanglement, which neglects the inherent complementarity between feature types and may limit recognition performance. To address these challenges, we propose Angle-Optimized Feature Learning (AO-FL), a framework tailored for MERC that achieves partial disentanglement of shared and specific features within each modality through adaptive angular optimization. Specifically, AO-FL aligns shared features across modalities to ensure semantic consistency, and within each modality it adaptively models the angular relationship between its shared and modality-specific features to preserve both distinctiveness and complementarity. An orthogonal projection refinement further removes redundancy in specific features and enriches shared features with contextual information, yielding more discriminative multimodal representations. Extensive experiments confirm the effectiveness of AO-FL for MERC, demonstrating superior performance over state-of-the-art approaches. Moreover, AO-FL can be seamlessly integrated with various unimodal feature extractors and extended to other multimodal fusion tasks, such as MER, thereby highlighting its strong generalization beyond MERC.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22463", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.22463", "abs": "https://arxiv.org/abs/2511.22463", "authors": ["Xinyi Che", "Wenbo Wang", "Jian Guan", "Qijun Zhao"], "title": "Orthogonal Disentanglement with Projected Feature Alignment for Multimodal Emotion Recognition in Conversation", "comment": "10 pages, 1 figure", "summary": "Multimodal Emotion Recognition in Conversation (MERC) significantly enhances emotion recognition performance by integrating complementary emotional cues from text, audio, and visual modalities. While existing methods commonly utilize techniques such as contrastive learning and cross-attention mechanisms to align cross-modal emotional semantics, they typically overlook modality-specific emotional nuances like micro-expressions, tone variations, and sarcastic language. To overcome these limitations, we propose Orthogonal Disentanglement with Projected Feature Alignment (OD-PFA), a novel framework designed explicitly to capture both shared semantics and modality-specific emotional cues. Our approach first decouples unimodal features into shared and modality-specific components. An orthogonal disentanglement strategy (OD) enforces effective separation between these components, aided by a reconstruction loss to maintain critical emotional information from each modality. Additionally, a projected feature alignment strategy (PFA) maps shared features across modalities into a common latent space and applies a cross-modal consistency alignment loss to enhance semantic coherence. Extensive evaluations on widely-used benchmark datasets, IEMOCAP and MELD, demonstrate effectiveness of our proposed OD-PFA multimodal emotion recognition tasks, as compared with the state-of-the-art approaches.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22746", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22746", "abs": "https://arxiv.org/abs/2511.22746", "authors": ["Sekoul Krastev", "Hilary Sweatman", "Anni Sternisko", "Steve Rathje"], "title": "Epistemic Fragility in Large Language Models: Prompt Framing Systematically Modulates Misinformation Correction", "comment": null, "summary": "As large language models (LLMs) rapidly displace traditional expertise, their capacity to correct misinformation has become a core concern. We investigate the idea that prompt framing systematically modulates misinformation correction - something we term 'epistemic fragility'. We manipulated prompts by open-mindedness, user intent, user role, and complexity. Across ten misinformation domains, we generated 320 prompts and elicited 2,560 responses from four frontier LLMs, which were coded for strength of misinformation correction and rectification strategy use. Analyses showed that creative intent, expert role, and closed framing led to a significant reduction in correction likelihood and effectiveness of used strategy. We also found striking model differences: Gemini 2.5 Pro had 74% lower odds of strong correction than Claude Sonnet 4.5. These findings highlight epistemic fragility as an important structural property of LLMs, challenging current guardrails and underscoring the need for alignment strategies that prioritize epistemic integrity over conversational compliance.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22576", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.22576", "abs": "https://arxiv.org/abs/2511.22576", "authors": ["Janak Kapuriya", "Ali Hatami", "Paul Buitelaar"], "title": "A Progressive Evaluation Framework for Multicultural Analysis of Story Visualization", "comment": null, "summary": "Recent advancements in text-to-image generative models have improved narrative consistency in story visualization. However, current story visualization models often overlook cultural dimensions, resulting in visuals that lack authenticity and cultural fidelity. In this study, we conduct a comprehensive multicultural analysis of story visualization using current text-to-image models across multilingual settings on two datasets: FlintstonesSV and VIST. To assess cultural dimensions rigorously, we propose a Progressive Multicultural Evaluation Framework and introduce five story visualization metrics, Cultural Appropriateness, Visual Aesthetics, Cohesion, Semantic Consistency, and Object Presence, that are not addressed by existing metrics. We further automate assessment through an MLLM-as-Jury framework that approximates human judgment. Human evaluations show that models generate more coherent, visually appealing, and culturally appropriate stories for real-world datasets than for animated ones. The generated stories exhibit a stronger alignment with English-speaking cultures across all metrics except Cohesion, where Chinese performs better. In contrast, Hindi ranks lowest on all metrics except Visual Aesthetics, reflecting real-world cultural biases embedded in current models. This multicultural analysis provides a foundation for future research aimed at generating culturally appropriate and inclusive visual stories across diverse linguistic and cultural settings.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22789", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22789", "abs": "https://arxiv.org/abs/2511.22789", "authors": ["Alif Al Hasan", "Subarna Saha", "Mia Mohammad Imran"], "title": "Learning Programming in Informal Spaces: Using Emotion as a Lens to Understand Novice Struggles on r/learnprogramming", "comment": null, "summary": "Novice programmers experience emotional difficulties in informal online learning environments, where confusion and frustration can hinder motivation and learning outcomes. This study investigates novice programmers' emotional experiences in informal settings, identifies the causes of emotional struggle, and explores design opportunities for affect-aware support systems. We manually annotated 1,500 posts from r/learnprogramming using the Learning-Centered Emotions framework and conducted clustering and axial coding. Confusion, curiosity, and frustration were the most common emotions, often co-occurring and associated with early learning stages. Positive emotions were relatively rare. The primary emotional triggers included ambiguous errors, unclear learning pathways, and misaligned learning resources. We identify five key areas where novice programmers need support in informal learning spaces: stress relief and resilient motivation, topic explanation and resource recommendation, strategic decision-making and learning guidance, technical support, and acknowledgment of their challenges. Our findings highlight the need for intelligent, affect-sensitive mechanisms that provide timely support aligned with learners' emotional states.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22809", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.22809", "abs": "https://arxiv.org/abs/2511.22809", "authors": ["Yiwei Xu", "Saloni Dash", "Sungha Kang", "Wang Liao", "Emma S. Spiro"], "title": "AI summaries in online search influence users' attitudes", "comment": null, "summary": "This study examined how AI-generated summaries, which have become visually prominent in online search results, affect how users think about different issues. In a preregistered randomized controlled experiment, participants (N = 2,004) viewed mock search result pages varying in the presence (vs. absence), placement (top vs. middle), and stance (benefit-framed vs. harm-framed) of AI-generated summaries across four publicly debated topics. Compared to a no-summary control group, participants exposed to AI-generated summaries reported issue attitudes, behavioral intentions, and policy support that aligned more closely with the AI summary stance. The summaries placed at the top of the page produced stronger shifts in users' issue attitudes (but not behavioral intentions or policy support) than those placed at the middle of the page. We also observed moderating effects from issue familiarity and general trust toward AI. In addition, users perceived the AI summaries more useful when it emphasized health harms versus benefits. These findings suggest that AI-generated search summaries can significantly shape public perceptions, raising important implications for the design and regulation of AI-integrated information ecosystems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.22942", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22942", "abs": "https://arxiv.org/abs/2511.22942", "authors": ["Na Li", "Chuhao Wu", "Hongyang Zhou", "Huiran Yi", "Xuefei Wang", "Jie Cai", "Xinyi Fu", "John Carroll"], "title": "Body Management Information Practices on a Female-dominant Platform", "comment": "Preprint accepted at ICHEC 2025", "summary": "With growing awareness of long-term health and wellness, everyday body management has become a widespread practice. Social media platforms and health-related applications offer abundant information for those pursuing healthier lifestyles and more positive body images. While prior Human-Computer Interaction research has focused extensively on technology-mediated health interventions, the user-initiated practices of browsing and evaluating body management information remain underexplored. In this paper, we study a female-dominant social media platform in China to examine how users seek such information and how it shapes their lifestyle choices. Through semi-structured interviews with 18 users, we identify factors including consumerism, poster popularity, and perceived authenticity that influence decision-making, alongside challenges such as discerning reliable methods and managing body anxiety triggered by social media. We contribute insights into how content and media formats interact to shape users' information evaluation, and we outline design implications for supporting more reliable and healthy engagements with body management information.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.23173", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.23173", "abs": "https://arxiv.org/abs/2511.23173", "authors": ["Hoang Khang Phan", "Khang Le", "Tu Nhat Khang Nguyen", "Anh Van Dao", "Nhat Tan Le"], "title": "Robust In-the-Wild Exercise Recognition from a Single Wearable: Data-Side Fusion, Sensor Rotation, and Feature Engineering", "comment": null, "summary": "Monitoring physical exercises is vital for health promotion, with automated systems becoming standard in personal health surveillance. However, sensor placement variability and unconstrained movements limit their effectiveness. This study proposes the team \"3KA\"'s one-sensor workout activity recognition method using feature extraction and data augmentation in 2ndWEAR Dataset Challenge. From raw acceleration, angle and signal magnitude vector features were derived, followed by extraction of statistical, fractal/spectral, and higher-order differential features. A fused dataset combining left/right limb data was created, and augmented via sensor rotation and axis inversion. We utilized a soft voting model combining Hist Gradient Boosting with balanced weights and Extreme Gradient Boosting without. Under group 5-fold evaluation, the model achieved 58.83\\% macro F1 overall (61.72% arm, 55.95% leg). ANOVA F-score showed fractal/spectral features were most important for arm-based recognition but least for leg-based. The code to reproduce the experiments is publicly available via: https://github.com/Khanghcmut/WEAR\\_3K", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.23188", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.23188", "abs": "https://arxiv.org/abs/2511.23188", "authors": ["Yibo Meng", "Lyumanshan Ye", "Eve He", "Zhe Yan", "Zhiming Liu", "Yipeng Yu", "Yan Guan", "Xiaolan Ding"], "title": "Can Intelligent User Interfaces Engage in Philosophical Discussions? A Longitudinal Study of Philosophers' Evolving Perceptions", "comment": null, "summary": "This study investigates the evolving attitudes of philosophy scholars towards the participation of generative AI based Intelligent User Interfaces (IUIs) in philosophical discourse. We conducted a three year (2023--2025) mixed methods longitudinal study with 16 philosophy scholars and students. Qualitative data from annual interviews reveal a three stage evolution in attitude: from initial resistance and unfamiliarity, to instrumental acceptance of the IUI as a tool, and finally to a deep principled questioning of the IUI's fundamental capacity for genuine philosophical thought. Quantitative data from blind assessments, where participants rated anonymized philosophical answers from both humans and an IUI, complement these findings. While participants acknowledged the IUI's proficiency in tasks requiring formal logic and knowledge reproduction, they consistently identified significant shortcomings in areas demanding dialectical reasoning, originality and embodied understanding. The study concludes that participants do not see the IUI as a peer but rather as a sophisticated mirror whose capabilities and limitations provoke a deeper reflection on the unique and irreplaceable human dimensions of philosophical inquiry, such as intuition, value laden commitment and the courage to question fundamental premises.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.23376", "categories": ["cs.HC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23376", "abs": "https://arxiv.org/abs/2511.23376", "authors": ["Li Siyan", "Jason Zhang", "Akash Maharaj", "Yuanming Shi", "Yunyao Li"], "title": "Is Passive Expertise-Based Personalization Enough? A Case Study in AI-Assisted Test-Taking", "comment": "Accepted into Tailoring AI: Exploring Active and Passive LLM Personalization (PALS) workshop at EMNLP 2025", "summary": "Novice and expert users have different systematic preferences in task-oriented dialogues. However, whether catering to these preferences actually improves user experience and task performance remains understudied. To investigate the effects of expertise-based personalization, we first built a version of an enterprise AI assistant with passive personalization. We then conducted a user study where participants completed timed exams, aided by the two versions of the AI assistant. Preliminary results indicate that passive personalization helps reduce task load and improve assistant perception, but reveal task-specific limitations that can be addressed through providing more user agency. These findings underscore the importance of combining active and passive personalization to optimize user experience and effectiveness in enterprise task-oriented environments.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.23379", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.23379", "abs": "https://arxiv.org/abs/2511.23379", "authors": ["Yimeng Liu", "Misha Sra"], "title": "AugGen: Augmenting Task-Based Learning in Professional Creative Software with LLM-Generated Scaffolded UIs", "comment": "arXiv admin note: substantial text overlap with arXiv:2505.12101", "summary": "Professional creative software often presents steep learning curves due to complex interfaces, lack of structured task-aware guidance, and unfamiliar domain terminology. To address these challenges and augment user learning experience, we introduce AugGen, a method for generating scaffolded user interfaces that simplify interface complexity and support task-based learning. With the user's task, our method surfaces task-relevant tools to reduce distracting features, organizes the tools around task workflow stages to offer execution guidance, connects tools with domain concepts to foster learning engagement, and progressively discloses advanced features to manage learning progress. To evaluate the method, we used our LLM-assisted pipeline to generate two task-specific scaffolded UIs and deployed them in Blender, our professional 3D modeling testbed. We invited both beginner (N=32) and expert (N=8) users to evaluate our implemented interfaces. Results show that the scaffolded interfaces significantly reduced user-perceived task load, enhanced task performance via embedded guidance, and augmented concept learning during task execution.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.23384", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.23384", "abs": "https://arxiv.org/abs/2511.23384", "authors": ["Isabel Whiteley Tscherniak", "Niels Christopher Thiemann", "Ana McWhinney-Fern\u00e1ndez", "Iustin Curcean", "Leon Jokinen", "Sadat Hodzic", "Thomas E. Huber", "Daniel Pavlov", "Manuel Methasani", "Pietro Marcolongo", "Glenn Viktor Krafczyk", "Oscar Osvaldo Soto Rivera", "Thien Le", "Flaminia Pallotti", "Enrico A. Fazzi", "neuroTUM e."], "title": "Improving motor imagery decoding methods for an EEG-based mobile brain-computer interface in the context of the 2024 Cybathlon", "comment": null, "summary": "Motivated by the Cybathlon 2024 competition, we developed a modular, online EEG-based brain-computer interface to address these challenges, increasing accessibility for individuals with severe mobility impairments. Our system uses three mental and motor imagery classes to control up to five control signals. The pipeline consists of four modules: data acquisition, preprocessing, classification, and the transfer function to map classification output to control dimensions. We use three diagonalized structured state-space sequence layers as a deep learning classifier. We developed a training game for our pilot where the mental tasks control the game during quick-time events. We implemented a mobile web application for live user feedback. The components were designed with a human-centred approach in collaboration with the tetraplegic user. We achieve up to 84% classification accuracy in offline analysis using an S4D-layer-based model. In a competition setting, our pilot successfully completed one task; we attribute the reduced performance in this context primarily to factors such as stress and the challenging competition environment. Following the Cybathlon, we further validated our pipeline with the original pilot and an additional participant, achieving a success rate of 73% in real-time gameplay. We also compare our model to the EEGEncoder, which is slower in training but has a higher performance. The S4D model outperforms the reference machine learning models. We provide insights into developing a framework for portable BCIs, bridging the gap between the laboratory and daily life. Specifically, our framework integrates modular design, real-time data processing, user-centred feedback, and low-cost hardware to deliver an accessible and adaptable BCI solution, addressing critical gaps in current BCI applications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
