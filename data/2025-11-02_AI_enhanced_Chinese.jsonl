{"id": "2510.25957", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2510.25957", "abs": "https://arxiv.org/abs/2510.25957", "authors": ["Radha Kumaran", "You-Jin Kim", "Anne E Milner", "Tom Bullock", "Barry Giesbrecht", "Tobias H\u00f6llerer"], "title": "The Impact of Navigation Aids on Search Performance and Object Recall in Wide-Area Augmented Reality", "comment": "Conference Paper, 17 pages. Published at the 2023 CHI Conference on\n  Human Factors in Computing Systems", "summary": "Head-worn augmented reality (AR) is a hotly pursued and increasingly feasible\ncontender paradigm for replacing or complementing smartphones and watches for\ncontinual information consumption. Here, we compare three different AR\nnavigation aids (on-screen compass, on-screen radar and in-world vertical\narrows) in a wide-area outdoor user study (n=24) where participants search for\nhidden virtual target items amongst physical and virtual objects. We analyzed\nparticipants' search task performance, movements, eye-gaze, survey responses\nand object recall. There were two key findings. First, all navigational aids\nenhanced search performance relative to a control condition, with some benefit\nand strongest user preference for in-world arrows. Second, users recalled fewer\nphysical objects than virtual objects in the environment, suggesting reduced\nawareness of the physical environment. Together, these findings suggest that\nwhile navigational aids presented in AR can enhance search task performance,\nusers may pay less attention to the physical environment, which could have\nundesirable side-effects.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25974", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25974", "abs": "https://arxiv.org/abs/2510.25974", "authors": ["Mengtian Guo", "David Gotz", "Yue Wang"], "title": "Risks and Opportunities in Human-Machine Teaming in Operationalizing Machine Learning Target Variables", "comment": "23 pages, 6 figures", "summary": "Predictive modeling has the potential to enhance human decision-making.\nHowever, many predictive models fail in practice due to problematic problem\nformulation in cases where the prediction target is an abstract concept or\nconstruct and practitioners need to define an appropriate target variable as a\nproxy to operationalize the construct of interest. The choice of an appropriate\nproxy target variable is rarely self-evident in practice, requiring both domain\nknowledge and iterative data modeling. This process is inherently\ncollaborative, involving both domain experts and data scientists. In this work,\nwe explore how human-machine teaming can support this process by accelerating\niterations while preserving human judgment. We study the impact of two\nhuman-machine teaming strategies on proxy construction: 1) relevance-first:\nhumans leading the process by selecting relevant proxies, and 2)\nperformance-first: machines leading the process by recommending proxies based\non predictive performance. Based on a controlled user study of a proxy\nconstruction task (N = 20), we show that the performance-first strategy\nfacilitated faster iterations and decision-making, but also biased users\ntowards well-performing proxies that are misaligned with the application goal.\nOur study highlights the opportunities and risks of human-machine teaming in\noperationalizing machine learning target variables, yielding insights for\nfuture research to explore the opportunities and mitigate the risks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25978", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2510.25978", "abs": "https://arxiv.org/abs/2510.25978", "authors": ["You-Jin Kim", "Radha Kumaran", "Jingjing Luo", "Tom Bullock", "Barry Giesbrecht", "Tobias H\u00f6llerer"], "title": "On the Go with AR: Attention to Virtual and Physical Targets while Varying Augmentation Density", "comment": "Conference Paper, 16 pages. Published at the 2025 CHI Conference on\n  Human Factors in Computing Systems", "summary": "Augmented reality is projected to be a primary mode of information\nconsumption on the go, seamlessly integrating virtual content into the physical\nworld. However, the potential perceptual demands of viewing virtual annotations\nwhile navigating a physical environment could impact user efficacy and safety,\nand the implications of these demands are not well understood. Here, we\ninvestigate the impact of virtual path guidance and augmentation density\n(visual clutter) on search performance and memory. Participants walked along a\npredefined path, searching for physical or virtual items. They experienced two\nlevels of augmentation density, and either walked freely or with enforced speed\nand path guidance. Augmentation density impacted behavior and reduced awareness\nof uncommon objects in the environment. Analysis of search task performance and\npost-experiment item recall revealed differing attention to physical and\nvirtual objects. On the basis of these findings we outline considerations for\nAR apps designed for use on the go.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26015", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.26015", "abs": "https://arxiv.org/abs/2510.26015", "authors": ["Zhengtao Ma", "Rafael Gomez", "Togtokhtur Batbold", "Zishuo Zhu", "Yueteng Yu", "Ronald Schroeter"], "title": "Designing for Dignity while Driving: Interaction Needs of Blind and Low-Vision Passengers in Fully Automated Vehicles", "comment": null, "summary": "Fully automated vehicles (FAVs) hold promise for enhancing the mobility of\nblind and low-vision (BLV) individuals. To understand the situated interaction\nneeds of BLV passengers, we conducted six on-road, and in-lab focus groups with\n16 participants, immersing them in real-world driving conditions. Our thematic\nanalysis reveals that BLV participants express a high initial 'faith' in FAVs,\nbut require layered, value-sensitive information during the ride to cultivate\ntrust. The participants' modality preference for voice suggests re-evaluating\nthe role of haptics for BLV users in FAVs. Our findings show the importance of\na respectful interaction design in FAVs that both address BLV users' mobility\nchallenges and uphold their dignity. While others have advocated for a dignity\nlens, our contribution lies in grounding this framework in empirical findings\nand unpacking what it means to design for dignity in the context of FAVs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26289", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2510.26289", "abs": "https://arxiv.org/abs/2510.26289", "authors": ["Zijing Xu", "Yunfeng Kou", "Kunming Wu", "Hong Liu"], "title": "Contribution-Guided Asymmetric Learning for Robust Multimodal Fusion under Imbalance and Noise", "comment": null, "summary": "Multimodal learning faces two major challenges: modality imbalance and data\nnoise, which significantly affect the robustness and generalization ability of\nmodels. Existing methods achieve modality balance by suppressing dominant\nmodalities, but they neglect the inherent differences in the information value\nbetween modalities, potentially leading to convergence to suboptimal solutions.\nThis paper proposes an innovative modality compression paradigm,\nContribution-Guided Asymmetric Learning (CAL), which aims to enhance the\ncontribution of high-contribution modalities while compressing weak modalities\nto increase their contribution, allowing both to improve the performance of\nmultimodal information fusion. CAL is based on a modality contribution metric\nW^m combining the information quantity I(m) and confidence D(m), and it designs\nan asymmetric gradient acceleration mechanism and a contribution-aware\nAsymmetric Information Bottleneck (AIB) compression mechanism. The former\naccelerates the gradient update of modalities, while the latter dynamically\ncompresses the noise of low-contribution modalities.\n  On five benchmark datasets, including emotion recognition, scene recognition,\nand event localization tasks, CAL has shown outstanding performance in\nimbalanced fusion tasks and noise robustness tests. On CREMA-D, KS, and AVE,\nCAL achieves 79.30%, 74.82%, and 74.21% accuracy, significantly outperforming\nthe existing state-of-the-art model ARL. In high-noise robustness tests, CAL\nalso achieved leading performance under various attack strategies on the\nMVSA-Single and NYUD2 datasets. These results validate the significant\nadvantages of CAL in modality imbalance and noise interference. CAL, as a\nflexible and efficient framework, is easy to transfer to other tasks and has\nbroad adaptability and potential application prospects.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25779", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25779", "abs": "https://arxiv.org/abs/2510.25779", "authors": ["Gagan Bansal", "Wenyue Hua", "Zezhou Huang", "Adam Fourney", "Amanda Swearngin", "Will Epperson", "Tyler Payne", "Jake M. Hofman", "Brendan Lucier", "Chinmay Singh", "Markus Mobius", "Akshay Nambi", "Archana Yadav", "Kevin Gao", "David M. Rothschild", "Aleksandrs Slivkins", "Daniel G. Goldstein", "Hussein Mozannar", "Nicole Immorlica", "Maya Murad", "Matthew Vogel", "Subbarao Kambhampati", "Eric Horvitz", "Saleema Amershi"], "title": "Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets", "comment": null, "summary": "As LLM agents advance, they are increasingly mediating economic decisions,\nranging from product discovery to transactions, on behalf of users. Such\napplications promise benefits but also raise many questions about agent\naccountability and value for users. Addressing these questions requires\nunderstanding how agents behave in realistic market conditions. However,\nprevious research has largely evaluated agents in constrained settings, such as\nsingle-task marketplaces (e.g., negotiation) or structured two-agent\ninteractions. Real-world markets are fundamentally different: they require\nagents to handle diverse economic activities and coordinate within large,\ndynamic ecosystems where multiple agents with opaque behaviors may engage in\nopen-ended dialogues. To bridge this gap, we investigate two-sided agentic\nmarketplaces where Assistant agents represent consumers and Service agents\nrepresent competing businesses. To study these interactions safely, we develop\nMagentic-Marketplace -- a simulated environment where Assistants and Services\ncan operate. This environment enables us to study key market dynamics: the\nutility agents achieve, behavioral biases, vulnerability to manipulation, and\nhow search mechanisms shape market outcomes. Our experiments show that frontier\nmodels can approach optimal welfare -- but only under ideal search conditions.\nPerformance degrades sharply with scale, and all models exhibit severe\nfirst-proposal bias, creating 10-30x advantages for response speed over\nquality. These findings reveal how behaviors emerge across market conditions,\ninforming the design of fair and efficient agentic marketplaces.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26041", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2510.26041", "abs": "https://arxiv.org/abs/2510.26041", "authors": ["Jamie Ngoc Dinh", "You-Jin Kim", "Myungin Lee"], "title": "FractalBrain: A Neuro-interactive Virtual Reality Experience using Electroencephalogram (EEG) for Mindfulness", "comment": "Extended Abstracts (Interactivity), 4 pages. Published at the 2024\n  CHI Conference on Human Factors in Computing Systems", "summary": "Mindfulness has been studied and practiced in enhancing psychological\nwell-being while reducing neuroticism and psychopathological indicators.\nHowever, practicing mindfulness with continuous attention is challenging,\nespecially for beginners. In the proposed system, FractalBrain, we utilize an\ninteractive audiovisual fractal with a geometric repetitive pattern that has\nbeen demonstrated to induce meditative effects. FractalBrain presents an\nexperience combining a surreal virtual reality (VR) program with an\nelectroencephalogram (EEG) interface. While viewing an ever-changing\nfractal-inspired artwork in an immersive environment, the user's EEG stream is\nanalyzed and mapped into VR. These EEG data adaptively manipulates the\naudiovisual parameters in real-time, generating a distinct experience for each\nuser. The pilot feedback suggests the potential of the FractalBrain to\nfacilitate mindfulness and enhance attention.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25929", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25929", "abs": "https://arxiv.org/abs/2510.25929", "authors": ["Ziyi Wang", "Carmine Ventre", "Maria Polukarov"], "title": "Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion", "comment": null, "summary": "Algorithmic collusion has emerged as a central question in AI: Will the\ninteraction between different AI agents deployed in markets lead to collusion?\nMore generally, understanding how emergent behavior, be it a cartel or market\ndominance from more advanced bots, affects the market overall is an important\nresearch question.\n  We propose a hierarchical multi-agent reinforcement learning framework to\nstudy algorithmic collusion in market making. The framework includes a\nself-interested market maker (Agent~A), which is trained in an uncertain\nenvironment shaped by an adversary, and three bottom-layer competitors: the\nself-interested Agent~B1 (whose objective is to maximize its own PnL), the\ncompetitive Agent~B2 (whose objective is to minimize the PnL of its opponent),\nand the hybrid Agent~B$^\\star$, which can modulate between the behavior of the\nother two. To analyze how these agents shape the behavior of each other and\naffect market outcomes, we propose interaction-level metrics that quantify\nbehavioral asymmetry and system-level dynamics, while providing signals\npotentially indicative of emergent interaction patterns.\n  Experimental results show that Agent~B2 secures dominant performance in a\nzero-sum setting against B1, aggressively capturing order flow while tightening\naverage spreads, thus improving market execution efficiency. In contrast,\nAgent~B$^\\star$ exhibits a self-interested inclination when co-existing with\nother profit-seeking agents, securing dominant market share through adaptive\nquoting, yet exerting a milder adverse impact on the rewards of Agents~A and B1\ncompared to B2. These findings suggest that adaptive incentive control supports\nmore sustainable strategic co-existence in heterogeneous agent environments and\noffers a structured lens for evaluating behavioral design in algorithmic\ntrading systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26265", "categories": ["cs.HC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.26265", "abs": "https://arxiv.org/abs/2510.26265", "authors": ["Ling-Long Zou", "Qiang Tong", "Er-Xia Luo", "Sen-Zhe Xu", "Song-Hai Zhang", "Fang-Lue Zhang"], "title": "Look at That Distractor: Dynamic Translation Gain under Low Perceptual Load in Virtual Reality", "comment": null, "summary": "Redirected walking utilizes gain adjustments within perceptual thresholds to\nallow natural navigation in large scale virtual environments within confined\nphysical environments. Previous research has found that when users are\ndistracted by some scene elements, they are less sensitive to gain values.\nHowever, the effects on detection thresholds have not been quantitatively\nmeasured. In this paper, we present a novel method that dynamically adjusts\ntranslation gain by leveraging visual distractors. We place distractors within\nthe user's field of view and apply a larger translation gain when their\nattention is drawn to them. Because the magnitude of gain adjustment depends on\nthe user's level of engagement with the distractors, the redirection process\nremains smooth and unobtrusive. To evaluate our method, we developed a task\noriented virtual environment for a user study. Results show that introducing\ndistractors in the virtual environment significantly raises users' translation\ngain thresholds. Furthermore, assessments using the Simulator Sickness\nQuestionnaire and Igroup Presence Questionnaire indicate that the method\nmaintains user comfort and acceptance, supporting its effectiveness for RDW\nsystems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26069", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.26069", "abs": "https://arxiv.org/abs/2510.26069", "authors": ["Leixian Shen", "Yifang Wang", "Huamin Qu", "Xing Xie", "Haotian Li"], "title": "Interaction-Augmented Instruction: Modeling the Synergy of Prompts and Interactions in Human-GenAI Collaboration", "comment": "26 pages", "summary": "Text prompt is the most common way for human-generative AI (GenAI)\ncommunication. Though convenient, it is challenging to convey fine-grained and\nreferential intent. One promising solution is to combine text prompts with\nprecise GUI interactions, like brushing and clicking. However, there lacks a\nformal model to model synergistic designs between prompts and interactions,\nhindering their comparison and innovation. To fill this gap, via an iterative\nand deductive process, we develop the Interaction-Augmented Instruction (IAI)\nmodel, a compact entity-relation graph formalizing how the combination of\ninteractions and text prompts enhances human-generative AI communication. With\nthe model, we distill twelve recurring and composable atomic interaction\nparadigms from prior tools, verifying our model's capability to facilitate\nsystematic design characterization and comparison. Case studies further\ndemonstrate the model's utility in applying, refining, and extending these\nparadigms. These results illustrate our IAI model's descriptive,\ndiscriminative, and generative power for shaping future GenAI systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26585", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26585", "abs": "https://arxiv.org/abs/2510.26585", "authors": ["Fulin Lin", "Shaowen Chen", "Ruishan Fang", "Hongwei Wang", "Tao Lin"], "title": "Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems", "comment": null, "summary": "While Multi-Agent Systems (MAS) excel at complex tasks, their growing\nautonomy with operational complexity often leads to critical inefficiencies,\nsuch as excessive token consumption and failures arising from misinformation.\nExisting methods primarily focus on post-hoc failure attribution, lacking\nproactive, real-time interventions to enhance robustness and efficiency. To\nthis end, we introduce SupervisorAgent, a lightweight and modular framework for\nruntime, adaptive supervision that operates without altering the base agent's\narchitecture. Triggered by an LLM-free adaptive filter, SupervisorAgent\nintervenes at critical junctures to proactively correct errors, guide\ninefficient behaviors, and purify observations. On the challenging GAIA\nbenchmark, SupervisorAgent reduces the token consumption of the Smolagent\nframework by an average of 29.45% without compromising its success rate.\nExtensive experiments across five additional benchmarks (math reasoning, code\ngeneration, and question answering) and various SoTA foundation models validate\nthe broad applicability and robustness of our approach. The code is available\nat https://github.com/LINs-lab/SupervisorAgent.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26172", "categories": ["cs.HC", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.26172", "abs": "https://arxiv.org/abs/2510.26172", "authors": ["Shifu Chen", "Dazhen Deng", "Zhihong Xu", "Sijia Xu", "Tai-Quan Peng", "Yingcai Wu"], "title": "Linking Heterogeneous Data with Coordinated Agent Flows for Social Media Analysis", "comment": null, "summary": "Social media platforms generate massive volumes of heterogeneous data,\ncapturing user behaviors, textual content, temporal dynamics, and network\nstructures. Analyzing such data is crucial for understanding phenomena such as\nopinion dynamics, community formation, and information diffusion. However,\ndiscovering insights from this complex landscape is exploratory, conceptually\nchallenging, and requires expertise in social media mining and visualization.\nExisting automated approaches, though increasingly leveraging large language\nmodels (LLMs), remain largely confined to structured tabular data and cannot\nadequately address the heterogeneity of social media analysis. We present SIA\n(Social Insight Agents), an LLM agent system that links heterogeneous\nmulti-modal data -- including raw inputs (e.g., text, network, and behavioral\ndata), intermediate outputs, mined analytical results, and visualization\nartifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy\nthat connects insight types with suitable mining and visualization techniques,\nSIA enables agents to plan and execute coherent analysis strategies. To ensure\nmulti-modal integration, it incorporates a data coordinator that unifies\ntabular, textual, and network data into a consistent flow. Its interactive\ninterface provides a transparent workflow where users can trace, validate, and\nrefine the agent's reasoning, supporting both adaptability and trustworthiness.\nThrough expert-centered case studies and quantitative evaluation, we show that\nSIA effectively discovers diverse and meaningful insights from social media\nwhile supporting human-agent collaboration in complex analytical tasks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26740", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26740", "abs": "https://arxiv.org/abs/2510.26740", "authors": ["Ashwin Kumar", "William Yeoh"], "title": "A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation", "comment": null, "summary": "We introduce the General Incentives-based Framework for Fairness (GIFF), a\nnovel approach for fair multi-agent resource allocation that infers fair\ndecision-making from standard value functions. In resource-constrained\nsettings, agents optimizing for efficiency often create inequitable outcomes.\nOur approach leverages the action-value (Q-)function to balance efficiency and\nfairness without requiring additional training. Specifically, our method\ncomputes a local fairness gain for each action and introduces a counterfactual\nadvantage correction term to discourage over-allocation to already well-off\nagents. This approach is formalized within a centralized control setting, where\nan arbitrator uses the GIFF-modified Q-values to solve an allocation problem.\n  Empirical evaluations across diverse domains, including dynamic ridesharing,\nhomelessness prevention, and a complex job allocation task-demonstrate that our\nframework consistently outperforms strong baselines and can discover\nfar-sighted, equitable policies. The framework's effectiveness is supported by\na theoretical foundation; we prove its fairness surrogate is a principled lower\nbound on the true fairness improvement and that its trade-off parameter offers\nmonotonic tuning. Our findings establish GIFF as a robust and principled\nframework for leveraging standard reinforcement learning components to achieve\nmore equitable outcomes in complex multi-agent systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26251", "categories": ["cs.HC", "H.1.2"], "pdf": "https://arxiv.org/pdf/2510.26251", "abs": "https://arxiv.org/abs/2510.26251", "authors": ["Navid Ashrafi", "Philipp Graf", "Manuela Marquardt", "Francesco Vona", "Julia Schorlemmer", "Jan-Niklas Voigt-Antons"], "title": "Avatar Appearance Beyond Pixels -- User Ratings and Avatar Preferences within Health Applications", "comment": "17 pages, 3 figures, 1 table", "summary": "The appearance of a virtual avatar significantly influences its perceived\nappropriateness and the user's experience, particularly in healthcare\napplications. This study analyzed interactions with six avatars of varying\ncharacteristics in a patient-reported outcome measures (PROMs) application to\ninvestigate correlations between avatar ratings and user preferences.\nForty-seven participants completed a healthcare survey involving 30 PROMIS\nitems (Global Health and Physical Function) and then rated the avatars on\nwarmth, competence, attractiveness, and human-likeness, as well as their\nwillingness to share personal data. The results showed that competence was the\nmost critical factor in avatar selection, while human-likeness had minimal\nimpact on health data disclosure. Gender did not significantly affect the\nratings, but clothing style played a key role, with male avatars in\nprofessional attire rated higher in competence due to gender-stereotypical\nexpectations. In contrast, professional female avatars were rated lower in\nwarmth and attractiveness. These findings underline the importance of\nthoughtful avatar design in healthcare applications to enhance user experience\nand engagement.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26490", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.26490", "abs": "https://arxiv.org/abs/2510.26490", "authors": ["Alon Rosenbaum", "Yigal David", "Eran Kaufman", "Gilad Ravid", "Amit Ronen", "Assaf Krebs"], "title": "Scaffolding Creativity: How Divergent and Convergent LLM Personas Shape Human Machine Creative Problem-Solving", "comment": null, "summary": "Large language models (LLMs) are increasingly shaping creative work and\nproblem-solving; however, prior research suggests that they may diminish\nunassisted creativity. To address this tension, a coach-like LLM environment\nwas developed that embodies divergent and convergent thinking personas as two\ncomplementary processes. Effectiveness and user behavior were assessed through\na controlled experiment in which participants interacted with either persona,\nwhile a control group engaged with a standard LLM providing direct answers.\n  Notably, users' perceptions of which persona best supported their creativity\noften diverged from objective performance measures. Trait-based analyses\nrevealed that individual differences predict when people utilize divergent\nversus convergent personas, suggesting opportunities for adaptive sequencing.\nFurthermore, interaction patterns reflected the design thinking model,\ndemonstrating how persona-guided support shapes creative problem-solving.\n  Our findings provide design principles for creativity support systems that\nstrike a balance between exploration and convergence through persona-based\nguidance and personalization. These insights advance human-AI collaboration\ntools that scaffold rather than overshadow human creativity.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26508", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.26508", "abs": "https://arxiv.org/abs/2510.26508", "authors": ["Clara Colombatto", "Sean Rintel", "Lev Tankelevitch"], "title": "Metacognition and Confidence Dynamics in Advice Taking from Generative AI", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) can aid humans in a wide range of\ntasks, but its effectiveness critically depends on users being able to evaluate\nthe accuracy of GenAI outputs and their own expertise. Here we asked how\nconfidence in self and GenAI contributes to decisions to seek and rely on\nadvice from GenAI ('prospective confidence'), and how advice-taking in turn\nshapes this confidence ('retrospective confidence'). In a novel paradigm\ninvolving text generation, participants formulated plans for events, and could\nrequest advice from a GenAI (Study 1; N=200) or were randomly assigned to\nreceive advice (Study 2; N=300), which they could rely on or ignore. Advice\nrequests in Study 1 were related to higher prospective confidence in GenAI and\nlower confidence in self. Advice-seekers showed increased retrospective\nconfidence in GenAI, while those who declined advice showed increased\nconfidence in self. Random assignment in Study 2 revealed that advice exposure\nincreases confidence in GenAI and in self, suggesting that GenAI advice-taking\ncausally boosts retrospective confidence. These results were mirrored in advice\nreliance, operationalised as the textual similarity between GenAI advice and\nparticipants' responses, with reliance associated with increased retrospective\nconfidence in both GenAI and self. Critically, participants who chose to\nobtain/rely on advice provided more detailed responses (likely due to the\noutput's verbosity), but failed to check the output thoroughly, missing key\ninformation. These findings underscore a key role for confidence in\ninteractions with GenAI, shaped by both prior beliefs about oneself and the\nreliability of AI, and context-dependent exposure to advice.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
