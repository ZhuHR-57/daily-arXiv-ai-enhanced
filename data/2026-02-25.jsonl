{"id": "2602.20547", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.20547", "abs": "https://arxiv.org/abs/2602.20547", "authors": ["Griffin Pitts", "Sanaz Motamedi"], "title": "What Drives Students' Use of AI Chatbots? Technology Acceptance in Conversational AI", "comment": null, "summary": "Conversational AI tools have been rapidly adopted by students and are becoming part of their learning routines. To understand what drives this adoption, we draw on the Technology Acceptance Model (TAM) and examine how perceived usefulness and perceived ease of use relate to students' behavioral intention to use conversational AI that generates responses for learning tasks. We extend TAM by incorporating trust, perceived enjoyment, and subjective norms as additional factors that capture social and affective influences and uncertainty around AI outputs.\n  Using partial least squares structural equation modeling, we find perceived usefulness remains the strongest predictor of students' intention to use conversational AI. However, perceived ease of use does not exert a significant direct effect on behavioral intention once other factors are considered, operating instead indirectly through perceived usefulness. Trust and subjective norms significantly influence perceptions of usefulness, while perceived enjoyment exerts both a direct and indirect effect on usage intentions. These findings suggest that adoption decisions for conversational AI systems are influenced less by effort-related considerations and more by confidence in system outputs, affective engagement, and social context. Future research is needed to further examine how these acceptance relationships generalize across different conversational systems and usage contexts."}
{"id": "2602.20377", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2602.20377", "abs": "https://arxiv.org/abs/2602.20377", "authors": ["Shidong Wang", "Renato Pajarola"], "title": "Directly from Alpha to Omega: Controllable End-to-End Vector Floor Plan Generation", "comment": "accepted to IEEE Transactions on Visualization and Computer Graphics", "summary": "Automated floor plan generation aims to create residential layouts by arranging rooms within a given boundary, balancing topological, geometric, and aesthetic considerations. The existing methods typically use a multi-step pipeline with intermediate representations to decompose the prediction process into several sub-tasks, limiting model flexibility and imposing predefined solution paths. This often results in unreasonable outputs when applied to data unsuitable for these predefined paths, making it challenging for these methods to match human designers, who do not restrict themselves to a specific set of design workflows. To address these limitations, we introduce CE2EPlan, a controllable end-to-end topology- and geometry-enhanced diffusion model that removes restrictions on the generative process of AI design tools. Instead, it enables the model to learn how to design floor plans directly from data, capturing a wide range of solution paths from input boundaries to complete layouts. Extensive experiments demonstrate that our method surpasses all existing approaches using the multi-step pipeline, delivering higher-quality results with enhanced user control and greater diversity in output, bringing AI design tools closer to the versatility of human designers."}
{"id": "2602.20229", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.20229", "abs": "https://arxiv.org/abs/2602.20229", "authors": ["Tianjun Yao", "Zhaoyi Li", "Zhiqiang Shen"], "title": "HieraMAS: Optimizing Intra-Node LLM Mixtures and Inter-Node Topology for Multi-Agent Systems", "comment": "22 pages, 13 tables", "summary": "Multi-agent systems (MAS) built on large language models (LLMs) have shown strong performance across many tasks. Most existing approaches improve only one aspect at a time, such as the communication topology, role assignment, or LLM routing, while treating each agent as a single, indivisible unit. This misses the opportunity to use mixtures of LLMs within an agent to strengthen role-specific abilities. We propose HieraMAS, a hierarchical collaboration framework that combines intra-node LLM mixtures with an inter-node communication topology. HieraMAS introduces supernodes, where each functional role is implemented by multiple heterogeneous LLMs using a propose-synthesis structure. Optimizing HieraMAS creates unique credit-assignment challenges: final task performance depends heavily on the underlying LLMs' capabilities, which can lead reinforcement methods to incorrectly reward suboptimal configurations. To address this, we use a two-stage algorithm: (1) multi-level reward attribution, which provides fine-grained feedback at both the node level and the overall system level; (2) graph classification for topology selection, which treats choosing the communication structure as a holistic decision rather than optimizing edges one by one. Experiments on reasoning and coding benchmarks show that HieraMAS substantially outperforms existing methods while also delivering better cost-performance trade-offs."}
{"id": "2602.20767", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2602.20767", "abs": "https://arxiv.org/abs/2602.20767", "authors": ["Jiesheng Wu", "Shengrong Li"], "title": "SPP-SCL: Semi-Push-Pull Supervised Contrastive Learning for Image-Text Sentiment Analysis and Beyond", "comment": "Accepted and published by AAAI2026", "summary": "Existing Image-Text Sentiment Analysis (ITSA) methods may suffer from inconsistent intra-modal and inter-modal sentiment relationships. Therefore, we develop a method that balances before fusing to solve the issue of vision-language imbalance intra-modal and inter-modal sentiment relationships; that is, a Semi-Push-Pull Supervised Contrastive Learning (SPP-SCL) method is proposed. Specifically, the method is implemented using a novel two-step strategy, namely first using the proposed intra-modal supervised contrastive learning to pull the relationships between the intra-modal and then performing a well-designed conditional execution statement. If the statement result is false, our method will perform the second step, which is inter-modal supervised contrastive learning to push away the relationships between inter-modal. The two-step strategy will balance the intra-modal and inter-modal relationships to achieve the purpose of relationship consistency and finally perform cross-modal feature fusion for sentiment analysis and detection. Experimental studies on three public image-text sentiment and sarcasm detection datasets demonstrate that SPP-SCL significantly outperforms state-of-the-art methods by a large margin and is more discriminative in sentiment."}
{"id": "2602.20314", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.20314", "abs": "https://arxiv.org/abs/2602.20314", "authors": ["Lauren Vogelstein", "Vedya Konda", "Deborah Fields", "Yasmin Kafai", "Luis Morales-Navarro", "Dana√© Metaxa"], "title": "Rapid Testing, Duck Lips, and Tilted Cameras: Youth Everyday Algorithm Auditing Practices with Generative AI Filters", "comment": null, "summary": "Today's youth have extensive experience interacting with artificial intelligence and machine learning applications on popular social media platforms, putting youth in a unique position to examine, evaluate, and even challenge these applications. Algorithm auditing is a promising candidate for connecting youth's everyday practices in using AI applications with more formal scientific literacies (syncretic designs). In this paper, we analyze high school youth participants' everyday algorithm auditing practices when interacting with generative AI filters on TikTok, revealing thorough and extensive examinations, with youth rapidly testing filters with sophisticated camera variations and facial manipulations to identify filter limitations. In the discussion, we address how these findings can provide a foundation for developing designs that bring together everyday and more formal algorithm auditing."}
{"id": "2602.20348", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.20348", "abs": "https://arxiv.org/abs/2602.20348", "authors": ["Yibo Meng", "Bingyi Liu", "Ruiqi Chen", "Xin Chen", "Yan Guan"], "title": "52-Hz Whale Song: An Embodied VR Experience for Exploring Misunderstanding and Empathy", "comment": "Accepted as CHI 26 Poster", "summary": "Experiences of being misunderstood often stem not from a lack of voice, but from mismatches between how individuals express themselves and how others listen. Such communicative mismatches arise across many social settings, including situations involving linguistic and cultural displacement. While prior HCI research has explored empathy through virtual reality, many approaches rely on narrative explanation, positioning users as observers rather than embodied participants. We present 52-Hz Whale Song, an embodied VR experience that explores miscommunication through metaphor and perspective-shifting. Inspired by the real-world \"52-Hz whale,\" whose calls are not responded to by others, the experience uses this phenomenon as an experiential lens on communicative mismatch rather than representing any specific social group. Players progress through a three-act arc that moves from failed communication to agency and ultimately to mediation. A preliminary mixed-methods study (N = 30) suggests increased perspective-taking and reduced self-reported social distance in immigrant-related situations. This work highlights how embodied metaphor and role-shifting can support empathic engagement and offers transferable design insights for empathy-oriented interactive systems."}
{"id": "2602.20350", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.20350", "abs": "https://arxiv.org/abs/2602.20350", "authors": ["Yibo Meng", "Bingyi Liu", "Ruiqi Chen", "Yan Guan"], "title": "Misty Forest VR: Turning Real ADHD Attention Patterns into Shared Momentum for Youth Collaboration", "comment": "Accepted by CHI Poster 2026", "summary": "Attention Deficit Hyperactivity Disorder (ADHD) remains highly stigmatized in many cultural contexts, particularly in China, where ADHD-related behaviors are often moralized rather than understood as neurodevelopmental differences. As a result, challenges of self-perception, social misunderstanding, and collaboration between ADHD and non-ADHD individuals remain largely unaddressed. We present Misty Forest, a VR-based collaborative game that explores ADHD through asymmetric co-play. The system translates empirically grounded ADHD behavioral patterns -- such as fluctuating attention and time blindness -- into complementary roles that require mutual coordination between players. Rather than compensating for deficits, the design treats cognitive differences as a source of interdependence. In a controlled study with mixed ADHD--non-ADHD dyads, Misty Forest led to higher task completion, increased self-acceptance among ADHD participants, improved ADHD knowledge, and greater empathy among non-ADHD players. These findings suggest that neurodiversity-centered interactive design can foster understanding, reciprocity, and inclusive collaboration."}
{"id": "2602.20486", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20486", "abs": "https://arxiv.org/abs/2602.20486", "authors": ["Paras Sharma", "YuePing Sha", "Janet Shufor Bih Epse Fofang", "Brayden Yan", "Jess A. Turner", "Nicole Balay", "Hubert O. Asare", "Angela E. B. Stewart", "Erin Walker"], "title": "Hybrid LLM-Embedded Dialogue Agents for Learner Reflection: Designing Responsive and Theory-Driven Interactions", "comment": null, "summary": "Dialogue systems have long supported learner reflections, with theoretically grounded, rule-based designs offering structured scaffolding but often struggling to respond to shifts in engagement. Large Language Models (LLMs), in contrast, can generate context-sensitive responses but are not informed by decades of research on how learning interactions should be structured, raising questions about their alignment with pedagogical theories. This paper presents a hybrid dialogue system that embeds LLM responsiveness within a theory-aligned, rule-based framework to support learner reflections in a culturally responsive robotics summer camp. The rule-based structure grounds dialogue in self-regulated learning theory, while the LLM decides when and how to prompt deeper reflections, responding to evolving conversation context. We analyze themes across dialogues to explore how our hybrid system shaped learner reflections. Our findings indicate that LLM-embedded dialogues supported richer learner reflections on goals and activities, but also introduced challenges due to repetitiveness and misalignment in prompts, reducing engagement."}
{"id": "2602.20544", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.20544", "abs": "https://arxiv.org/abs/2602.20544", "authors": ["Hannah Kim", "Rahad Arman Nabid", "Jeni Sorathiya", "Minh Doan", "Elijah Jordan", "Rayhana Nasimova", "Sergei L. Kosakovsky Pond", "Stephen MacNeil"], "title": "Changing the Optics: Comparing Traditional and Retrieval-Augmented GenAI E-Tutorials in Interdisciplinary Learning", "comment": "6 pages (16 pages including appendix), 2 figures, 5 supplementary figures, 2 supplementary tables", "summary": "Understanding information-seeking behaviors in e-learning is critical, as learners must often make sense of complex and fragmented information, a challenge compounded in interdisciplinary fields with diverse prior knowledge. Compared to traditional e-tutorials, GenAI e-tutorials offer new ways to navigate information spaces, yet how they shape learners information-seeking behaviors remains unclear. To address this gap, we characterized behavioral differences between traditional and GenAI-mediated e-tutorial learning using the three search modes of orienteering. We conducted a between-subject study in which learners engaged with either a traditional e-tutorial or a GenAI e-tutorial accessing the same underlying information content. We found that the traditional users maintained greater awareness and focus of the information space, whereas GenAI users exhibited more proactive and exploratory behaviors with lower cognitive load due to the querying-driven interaction. These findings offer guidance for designing tutorials in e-learning."}
{"id": "2602.20547", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.20547", "abs": "https://arxiv.org/abs/2602.20547", "authors": ["Griffin Pitts", "Sanaz Motamedi"], "title": "What Drives Students' Use of AI Chatbots? Technology Acceptance in Conversational AI", "comment": null, "summary": "Conversational AI tools have been rapidly adopted by students and are becoming part of their learning routines. To understand what drives this adoption, we draw on the Technology Acceptance Model (TAM) and examine how perceived usefulness and perceived ease of use relate to students' behavioral intention to use conversational AI that generates responses for learning tasks. We extend TAM by incorporating trust, perceived enjoyment, and subjective norms as additional factors that capture social and affective influences and uncertainty around AI outputs.\n  Using partial least squares structural equation modeling, we find perceived usefulness remains the strongest predictor of students' intention to use conversational AI. However, perceived ease of use does not exert a significant direct effect on behavioral intention once other factors are considered, operating instead indirectly through perceived usefulness. Trust and subjective norms significantly influence perceptions of usefulness, while perceived enjoyment exerts both a direct and indirect effect on usage intentions. These findings suggest that adoption decisions for conversational AI systems are influenced less by effort-related considerations and more by confidence in system outputs, affective engagement, and social context. Future research is needed to further examine how these acceptance relationships generalize across different conversational systems and usage contexts."}
{"id": "2602.20594", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.20594", "abs": "https://arxiv.org/abs/2602.20594", "authors": ["Takaya Miyama", "Satoshi Nakamura", "Shota Yamanaka"], "title": "Improving Data Quality via Pre-Task Participant Screening in Crowdsourced GUI Experiments", "comment": "To appear at CHI 2026", "summary": "In crowdsourced user experiments that collect performance data from graphical user interface (GUI) interactions, some participants ignore instructions or act carelessly, threatening the validity of performance models. We investigate a pre-task screening method that requires simple GUI operations analogous to the main task and uses the resulting error as a continuous quality signal. Our pre-task is a brief image-resizing task in which workers match an on-screen card to a physical card; workers whose resizing error exceeds a threshold are excluded from the main experiment. The main task is a standardized pointing experiment with well-established models of movement time and error rate. Across mouse- and smartphone-based crowdsourced experiments, we show that reducing the proportion of workers exhibiting unexpected behavior and tightening the pre-task threshold systematically improve the goodness of fit and predictive accuracy of GUI performance models, demonstrating that brief pre-task screening can enhance data quality."}
{"id": "2602.20876", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.20876", "abs": "https://arxiv.org/abs/2602.20876", "authors": ["Runhua Zhang", "Ziqi Pan", "Kangyu Yuan", "Qiaoyi Chen", "Yulin Tian", "Huamin Qu", "Xiaojuan Ma"], "title": "When LLMs Enter Everyday Feminism on Chinese Social Media: Opportunities and Risks for Women's Empowerment", "comment": "This paper is conditionally accepted to CHI 2026", "summary": "Everyday digital feminism refers to the ordinary, often pragmatic ways women articulate lived experiences and cultivate solidarity in online spaces. In China, such practices flourish on RedNote through discussions under hashtags like ''women's growth''. Recently, DeepSeek-generated content has been taken up as a new voice in these conversations. Given widely recognized gender biases in LLMs, this raises critical concerns about how LLMs interact with everyday feminist practices. Through an analysis of 430 RedNote posts, 139 shared DeepSeek responses, and 3211 comments, we found that users predominantly welcomed DeepSeek's advice. Yet feminist critical discourse analysis revealed that these responses primarily encouraged women to self-optimize and pursue achievements within prevailing norms rather than challenge them. By interpreting this case, we discuss the opportunities and risks that LLMs introduce for everyday feminism as a pathway toward women's empowerment, and offer design implications for leveraging LLMs to better support such practices."}
{"id": "2602.20891", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.20891", "abs": "https://arxiv.org/abs/2602.20891", "authors": ["Zhengtao Xu", "Zimo Xia", "Zicheng Zhu", "Nattapat Boonprakong", "Yu-An Chen", "Rabih Zbib", "Casimiro Pio Carrino", "Yi-Chieh Lee"], "title": "InterPilot: Exploring the Design Space of AI-assisted Job Interview Support for HR Professionals", "comment": "7 pages, 2 figures", "summary": "Recruitment interviews are cognitively demanding interactions in which interviewers must simultaneously listen, evaluate candidates, take notes, and formulate follow-up questions. To better understand these challenges, we conducted a formative study with eight HR professionals, from which we derived key design goals for real-time AI support. Guided by these insights, we developed InterPilot, a prototype system that augments interviews through intelligent note-taking and post-interview summary, adaptive question generation, and real-time skill-evidence mapping. We evaluated the system with another seven HR professionals in mock interviews using a within-subjects design. Results show that InterPilot reduced documentation burden without increasing overall workload, but introduced usability trade-offs related to visual attention and interaction complexity. Qualitative findings further reveal tensions around trust and verification when AI suggests highly specific technical questions. We discuss implications for designing future real-time human-AI collaboration in professional settings, highlighting the need to balance assistance granularity, attentional demands, and human agency."}
{"id": "2602.21045", "categories": ["cs.HC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21045", "abs": "https://arxiv.org/abs/2602.21045", "authors": ["Anna Martin-Boyle", "Cara A. C. Leckey", "Martha C. Brown", "Harmanpreet Kaur"], "title": "PaperTrail: A Claim-Evidence Interface for Grounding Provenance in LLM-based Scholarly Q&A", "comment": "25 pages, 3 figures. Accepted at the ACM CHI conference on Human Factors in Computing Systems 2026", "summary": "Large language models (LLMs) are increasingly used in scholarly question-answering (QA) systems to help researchers synthesize vast amounts of literature. However, these systems often produce subtle errors (e.g., unsupported claims, errors of omission), and current provenance mechanisms like source citations are not granular enough for the rigorous verification that scholarly domain requires. To address this, we introduce PaperTrail, a novel interface that decomposes both LLM answers and source documents into discrete claims and evidence, mapping them to reveal supported assertions, unsupported claims, and information omitted from the source texts. We evaluated PaperTrail in a within-subjects study with 26 researchers who performed two scholarly editing tasks using PaperTrail and a baseline interface. Our results show that PaperTrail significantly lowered participants' trust compared to the baseline. However, this increased caution did not translate to behavioral changes, as people continued to rely on LLM-generated scholarly edits to avoid a cognitively burdensome task. We discuss the value of claim-evidence matching for understanding LLM trustworthiness in scholarly settings, and present design implications for cognition-friendly communication of provenance information."}
{"id": "2602.21059", "categories": ["cs.HC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21059", "abs": "https://arxiv.org/abs/2602.21059", "authors": ["Anna Martin-Boyle", "William Humphreys", "Martha Brown", "Cara Leckey", "Harmanpreet Kaur"], "title": "An Expert Schema for Evaluating Large Language Model Errors in Scholarly Question-Answering Systems", "comment": "24 pages, 2 figures. Accepted at ACM CHI conference on Human Factors in Computing Systems, 2026", "summary": "Large Language Models (LLMs) are transforming scholarly tasks like search and summarization, but their reliability remains uncertain. Current evaluation metrics for testing LLM reliability are primarily automated approaches that prioritize efficiency and scalability, but lack contextual nuance and fail to reflect how scientific domain experts assess LLM outputs in practice. We developed and validated a schema for evaluating LLM errors in scholarly question-answering systems that reflects the assessment strategies of practicing scientists. In collaboration with domain experts, we identified 20 error patterns across seven categories through thematic analysis of 68 question-answer pairs. We validated this schema through contextual inquiries with 10 additional scientists, which showed not only which errors experts naturally identify but also how structured evaluation schemas can help them detect previously overlooked issues. Domain experts use systematic assessment strategies, including technical precision testing, value-based evaluation, and meta-evaluation of their own practices. We discuss implications for supporting expert evaluation of LLM outputs, including opportunities for personalized, schema-driven tools that adapt to individual evaluation patterns and expertise levels."}
{"id": "2602.21127", "categories": ["cs.HC", "cs.AI", "cs.CR", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.21127", "abs": "https://arxiv.org/abs/2602.21127", "authors": ["Xinfeng Li", "Shenyu Dai", "Kelong Zheng", "Yue Xiao", "Gelei Deng", "Wei Dong", "Xiaofeng Wang"], "title": "\"Are You Sure?\": An Empirical Study of Human Perception Vulnerability in LLM-Driven Agentic Systems", "comment": null, "summary": "Large language model (LLM) agents are rapidly becoming trusted copilots in high-stakes domains like software development and healthcare. However, this deepening trust introduces a novel attack surface: Agent-Mediated Deception (AMD), where compromised agents are weaponized against their human users. While extensive research focuses on agent-centric threats, human susceptibility to deception by a compromised agent remains unexplored. We present the first large-scale empirical study with 303 participants to measure human susceptibility to AMD. This is based on HAT-Lab (Human-Agent Trust Laboratory), a high-fidelity research platform we develop, featuring nine carefully crafted scenarios spanning everyday and professional domains (e.g., healthcare, software development, human resources). Our 10 key findings reveal significant vulnerabilities and provide future defense perspectives. Specifically, only 8.6% of participants perceive AMD attacks, while domain experts show increased susceptibility in certain scenarios. We identify six cognitive failure modes in users and find that their risk awareness often fails to translate to protective behavior. The defense analysis reveals that effective warnings should interrupt workflows with low verification costs. With experiential learning based on HAT-Lab, over 90% of users who perceive risks report increased caution against AMD. This work provides empirical evidence and a platform for human-centric agent security research."}
{"id": "2602.21136", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.21136", "abs": "https://arxiv.org/abs/2602.21136", "authors": ["David Anugraha", "Vishakh Padmakumar", "Diyi Yang"], "title": "SparkMe: Adaptive Semi-Structured Interviewing for Qualitative Insight Discovery", "comment": null, "summary": "Qualitative insights from user experiences are critical for informing product and policy decisions, but collecting such data at scale is constrained by the time and availability of experts to conduct semi-structured interviews. Recent work has explored using large language models (LLMs) to automate interviewing, yet existing systems lack a principled mechanism for balancing systematic coverage of predefined topics with adaptive exploration, or the ability to pursue follow-ups, deep dives, and emergent themes that arise organically during conversation. In this work, we formulate adaptive semi-structured interviewing as an optimization problem over the interviewer's behavior. We define interview utility as a trade-off between coverage of a predefined interview topic guide, discovery of relevant emergent themes, and interview cost measured by length. Based on this formulation, we introduce SparkMe, a multi-agent LLM interviewer that performs deliberative planning via simulated conversation rollouts to select questions with high expected utility. We evaluate SparkMe through controlled experiments with LLM-based interviewees, showing that it achieves higher interview utility, improving topic guide coverage (+4.7% over the best baseline) and eliciting richer emergent insights while using fewer conversational turns than prior LLM interviewing approaches. We further validate SparkMe in a user study with 70 participants across 7 professions on the impact of AI on their workflows. Domain experts rate SparkMe as producing high-quality adaptive interviews that surface helpful profession-specific insights not captured by prior approaches. The code, datasets, and evaluation protocols for SparkMe are available as open-source at https://github.com/SALT-NLP/SparkMe."}
