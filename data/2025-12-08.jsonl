{"id": "2512.05121", "categories": ["cs.GR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05121", "abs": "https://arxiv.org/abs/2512.05121", "authors": ["Tianshun Han", "Benjia Zhou", "Ajian Liu", "Yanyan Liang", "Du Zhang", "Zhen Lei", "Jun Wan"], "title": "PESTalk: Speech-Driven 3D Facial Animation with Personalized Emotional Styles", "comment": null, "summary": "PESTalk is a novel method for generating 3D facial animations with personalized emotional styles directly from speech. It overcomes key limitations of existing approaches by introducing a Dual-Stream Emotion Extractor (DSEE) that captures both time and frequency-domain audio features for fine-grained emotion analysis, and an Emotional Style Modeling Module (ESMM) that models individual expression patterns based on voiceprint characteristics. To address data scarcity, the method leverages a newly constructed 3D-EmoStyle dataset. Evaluations demonstrate that PESTalk outperforms state-of-the-art methods in producing realistic and personalized facial animations."}
{"id": "2512.05310", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.05310", "abs": "https://arxiv.org/abs/2512.05310", "authors": ["Brandon Biggs", "David Sloan", "Brett Oppegaard", "Nicholas A. Giudice", "James M. Coughlan", "Bruce N. Walker"], "title": "Systematically Evaluating Equivalent Purpose for Digital Maps", "comment": "In press at Journal on Technology and Persons with Disabilities, volume 14", "summary": "Digital geographic maps remain largely inaccessible to blind and low-vision individuals (BLVIs), despite global legislation adopting the Web Content Accessibility Guidelines (WCAG). A critical gap exists in defining \"equivalent purpose\" for maps under WCAG Success Criterion 1.1.1, which requires that non-text content provide a text alternative that serves the \"equivalent purpose\". This paper proposes a systematic framework for evaluating map accessibility, called the Map Equivalent-Purpose Framework (MEP Framework), defining purpose through three items (Generalized, Spatial Information, and Spatial Relationships), and establishing 15 measurable criteria for equivalent information communication. Eight text map representations were evaluated against visual map baselines using the proposed MEP Framework. Results show that legacy methods such as tables and turn-by-turn directions fail to meet the MEP Framework criteria, while Audiom Maps, Multi User Domain (MUD) Maps, and Audio Descriptions meet the criteria. The evaluation highlights the necessity of holistic, systematic approaches to ensure non-visual maps convey all generalized spatial information and relationships present in visual maps. The MEP Framework provides a replicable methodology for comprehensively assessing digital map accessibility, clarifying WCAG's \"equivalent purpose\", and guiding compliant and usable map creation. Compliant maps will support BLVIs' participation in map-dependent professions and civic engagement."}
{"id": "2512.05447", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.05447", "abs": "https://arxiv.org/abs/2512.05447", "authors": ["Pengcheng Dai", "Dongming Wang", "Wenwu Yu", "Wei Ren"], "title": "Distributed scalable coupled policy algorithm for networked multi-agent reinforcement learning", "comment": null, "summary": "This paper studies networked multi-agent reinforcement learning (NMARL) with interdependent rewards and coupled policies. In this setting, each agent's reward depends on its own state-action pair as well as those of its direct neighbors, and each agent's policy is parameterized by its local parameters together with those of its $κ_{p}$-hop neighbors, with $κ_{p}\\geq 1$ denoting the coupled radius. The objective of the agents is to collaboratively optimize their policies to maximize the discounted average cumulative reward. To address the challenge of interdependent policies in collaborative optimization, we introduce a novel concept termed the neighbors' averaged $Q$-function and derive a new expression for the coupled policy gradient. Based on these theoretical foundations, we develop a distributed scalable coupled policy (DSCP) algorithm, where each agent relies only on the state-action pairs of its $κ_{p}$-hop neighbors and the rewards its their $(κ_{p}+1)$-hop neighbors. Specially, in the DSCP algorithm, we employ a geometric 2-horizon sampling method that does not require storing a full $Q$-table to obtain an unbiased estimate of the coupled policy gradient. Moreover, each agent interacts exclusively with its direct neighbors to obtain accurate policy parameters, while maintaining local estimates of other agents' parameters to execute its local policy and collect samples for optimization. These estimates and policy parameters are updated via a push-sum protocol, enabling distributed coordination of policy updates across the network. We prove that the joint policy produced by the proposed algorithm converges to a first-order stationary point of the objective function. Finally, the effectiveness of DSCP algorithm is demonstrated through simulations in a robot path planning environment, showing clear improvement over state-of-the-art methods."}
{"id": "2512.05342", "categories": ["cs.ET", "cs.AR", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.05342", "abs": "https://arxiv.org/abs/2512.05342", "authors": ["Saitao Zhang", "Yubiao Luo", "Shiqing Wang", "Pushen Zuo", "Yongxiang Li", "Lunshuai Pan", "Zheng Miao", "Zhong Sun"], "title": "First Demonstration of Second-order Training of Deep Neural Networks with In-memory Analog Matrix Computing", "comment": null, "summary": "Second-order optimization methods, which leverage curvature information, offer faster and more stable convergence than first-order methods such as stochastic gradient descent (SGD) and Adam. However, their practical adoption is hindered by the prohibitively high cost of inverting the second-order information matrix, particularly in large-scale neural network training. Here, we present the first demonstration of a second-order optimizer powered by in-memory analog matrix computing (AMC) using resistive random-access memory (RRAM), which performs matrix inversion (INV) in a single step. We validate the optimizer by training a two-layer convolutional neural network (CNN) for handwritten letter classification, achieving 26% and 61% fewer training epochs than SGD with momentum and Adam, respectively. On a larger task using the same second-order method, our system delivers a 5.88x improvement in throughput and a 6.9x gain in energy efficiency compared to state-of-the-art digital processors. These results demonstrate the feasibility and effectiveness of AMC circuits for second-order neural network training, opening a new path toward energy-efficient AI acceleration."}
{"id": "2512.05389", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.05389", "abs": "https://arxiv.org/abs/2512.05389", "authors": ["Yuxuan Chen", "Ian Leong Ting Lo", "Bao Guo", "Netitorn Kawmali", "Chun Kit Chan", "Ruoyu Wang", "Jia Pan", "Lei Yang"], "title": "CLIO: A Tour Guide Robot with Co-speech Actions for Visual Attention Guidance and Enhanced User Engagement", "comment": "10 pages, 7 figures, human-robot interaction", "summary": "While audio guides can offer rich information about an exhibit, it is challenging for visitors to focus on specific exhibit details based only on the verbal description. We present \\textit{CLIO}, a tour guide robot with co-speech actions to direct visitors' visual attention and thus enhance the overall user engagement in a guided tour. \\textit{CLIO} is equipped with designed actions to engage visitors. It builds eye contact with the visitor through tracking a visitor's face and blinking its eyes, or orient their attention by its head movement and laser pointer. We further use a Large Language Model (LLM) to coordinate the designed actions with a given narrative script for exhibition. We conducted a user study to evaluate the \\textit{CLIO} system in a mock-up exhibition of historical photographs. We collected feedback from questionnaires and quantitative data from a mobile eye tracker. Experimental results validated that the engaging actions are well designed and demonstrated its efficacy in guiding visual attention of the visitors. It was evidenced that \\textit{CLIO} achieved an enhanced engagement compared to the baseline system with only audio guidance."}
{"id": "2512.05397", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05397", "abs": "https://arxiv.org/abs/2512.05397", "authors": ["Rachel Poonsiriwong", "Chayapatr Archiwaranguprok", "Constanze Albrecht", "Peggy Yin", "Nattavudh Powthavee", "Hal Hershfield", "Monchai Lertsutthiwong", "Kavin Winson", "Pat Pataranutaporn"], "title": "Simulating Life Paths with Digital Twins: AI-Generated Future Selves Influence Decision-Making and Expand Human Choice", "comment": null, "summary": "Major life transitions demand high-stakes decisions, yet people often struggle to imagine how their future selves will live with the consequences. To support this limited capacity for mental time travel, we introduce AI-enabled digital twins that have ``lived through'' simulated life scenarios. Rather than predicting optimal outcomes, these simulations extend prospective cognition by making alternative futures vivid enough to support deliberation without assuming which path is best. We evaluate this idea in a randomized controlled study (N=192) using multimodal synthesis - facial age progression, voice cloning, and large language model dialogue - to create personalized avatars representing participants 30 years forward. Young adults 18 to 28 years old described pending binary decisions and were assigned to guided imagination or one of four avatar conditions: single-option, balanced dual-option, or expanded three-option with a system-generated novel alternative. Results showed asymmetric effects: single-sided avatars increased shifts toward the presented option, while balanced presentation produced movement toward both. Introducing a system-generated third option increased adoption of this new alternative compared to control, suggesting that AI-generated future selves can expand choice by surfacing paths that might otherwise go unnoticed. Participants rated evaluative reasoning and eudaimonic meaning-making as more important than emotional or visual vividness. Perceived persuasiveness and baseline agency predicted decision change. These findings advance understanding of AI-mediated episodic prospection and raise questions about autonomy in AI-augmented decisions."}
{"id": "2512.05433", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.05433", "abs": "https://arxiv.org/abs/2512.05433", "authors": ["Kim Marriott", "Matthew Butler", "Leona Holloway", "Bill Jolley", "Bongshin Lee", "Bruce Maguire", "Danielle Albers Szafir"], "title": "From Vision to Touch: Bridging Visual and Tactile Principles for Accessible Data Representation", "comment": "To be published by IEEE as part of the 2025 Visualization Conference (VIS)", "summary": "Tactile graphics are widely used to present maps and statistical diagrams to blind and low vision (BLV) people, with accessibility guidelines recommending their use for graphics where spatial relationships are important. Their use is expected to grow with the advent of commodity refreshable tactile displays. However, in stark contrast to visual information graphics, we lack a clear understanding of the benefits that well-designed tactile information graphics offer over text descriptions for BLV people. To address this gap, we introduce a framework considering the three components of encoding, perception and cognition to examine the known benefits for visual information graphics and explore their applicability to tactile information graphics. This work establishes a preliminary theoretical foundation for the tactile-first design of information graphics and identifies future research avenues."}
{"id": "2512.05438", "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.05438", "abs": "https://arxiv.org/abs/2512.05438", "authors": ["Benoit Marteau", "Shaun Q. Y. Tan", "Jieru Li", "Andrew Hornback", "Yishan Zhong", "Shaunna Wang", "Christian Lowson", "Jason Woloff", "Joshua M. Pahys", "Steven W. Hwang", "Coleman Hilton", "May D. Wang"], "title": "EXR: An Interactive Immersive EHR Visualization in Extended Reality", "comment": "11 pages, 6 figures. Preprint version. This paper has been accepted to IEEE ICIR 2025. This is the author-prepared version and not the final published version. The final version will appear in IEEE Xplo", "summary": "This paper presents the design and implementation of an Extended Reality (XR) platform for immersive, interactive visualization of Electronic Health Records (EHRs). The system extends beyond conventional 2D interfaces by visualizing both structured and unstructured patient data into a shared 3D environment, enabling intuitive exploration and real-time collaboration. The modular infrastructure integrates FHIR-based EHR data with volumetric medical imaging and AI-generated segmentation, ensuring interoperability with modern healthcare systems. The platform's capabilities are demonstrated using synthetic EHR datasets and computed tomography (CT)-derived spine models processed through an AI-powered segmentation pipeline. This work suggests that such integrated XR solutions could form the foundation for next-generation clinical decision-support tools, where advanced data infrastructures are directly accessible in an interactive and spatially rich environment."}
{"id": "2512.05450", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.05450", "abs": "https://arxiv.org/abs/2512.05450", "authors": ["Pawel Weichbroth"], "title": "Classification and taxonomy of mobile application usability issues", "comment": "55 pages, 5 figures, 9 tables, 129 references", "summary": "Despite years of research on testing the usability of mobile applications, our understanding of the issues their users experience still remains fragmented and underexplored. While most earlier studies has provided interesting insights, they have varying limitations in methodology, input diversity, and depth of analysis.On the contrary, this study employs a triangulation strategy, using two research methods (systematic literature review and interview) and two data sources (scholarly literature and expert knowledge) to explore the traits underlying usability issues. Our study contributes to the field of human-computer interaction (HCI) by presenting a catalog of 16 usability issue categories, enriched with corresponding keywords and extended into a taxonomy, as well as a novel three-tier app-user-resource (AUR) classification system. At the first app level, usability issues arise from user interface design, as well as from efficiency, errors, and operability. At the second user level, they influence cognitive load, effectiveness, ease of use, learnability, memorability, and understandability. At the third resource level, usability issues stem from network quality and hardware, such as battery life, CPU speed, physical device button size and availability, RAM capacity, and screen size. The root cause of the usability issues is the user interface design. Detailed findings and takeaways for both researchers and practitioners are also discussed. Further research could focus on developing a measurement model for the identified variables to confirm the direction and strength of their relationships with perceived usability. Software vendors can also benefit by updating existing quality assurance programs, reviews and audits tools, as well as testing checklists."}
{"id": "2512.05506", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.05506", "abs": "https://arxiv.org/abs/2512.05506", "authors": ["Junho Myung", "Hyunseung Lim", "Hana Oh", "Hyoungwook Jin", "Nayeon Kang", "So-Yeon Ahn", "Hwajung Hong", "Alice Oh", "Juho Kim"], "title": "When Scaffolding Breaks: Investigating Student Interaction with LLM-Based Writing Support in Real-Time K-12 EFL Classrooms", "comment": "Under Review", "summary": "Large language models (LLMs) are promising tools for scaffolding students' English writing skills, but their effectiveness in real-time K-12 classrooms remains underexplored. Addressing this gap, our study examines the benefits and limitations of using LLMs as real-time learning support, considering how classroom constraints, such as diverse proficiency levels and limited time, affect their effectiveness. We conducted a deployment study with 157 eighth-grade students in a South Korean middle school English class over six weeks. Our findings reveal that while scaffolding improved students' ability to compose grammatically correct sentences, this step-by-step approach demotivated lower-proficiency students and increased their system reliance. We also observed challenges to classroom dynamics, where extroverted students often dominated the teacher's attention, and the system's assistance made it difficult for teachers to identify struggling students. Based on these findings, we discuss design guidelines for integrating LLMs into real-time writing classes as inclusive educational tools."}
{"id": "2512.05519", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.05519", "abs": "https://arxiv.org/abs/2512.05519", "authors": ["Bohui Shen", "Shrikar Bhatta", "Alex Ireebanije", "Zexuan Liu", "Abhinav Choudhry", "Ece Gumusel", "Kyrie Zhixuan Zhou"], "title": "User Negotiations of Authenticity, Ownership, and Governance on AI-Generated Video Platforms: Evidence from Sora", "comment": null, "summary": "As AI-generated video platforms rapidly advance, ethical challenges such as copyright infringement emerge. This study examines how users make sense of AI-generated videos on OpenAI's Sora by conducting a qualitative content analysis of user comments. Through a thematic analysis, we identified four dynamics that characterize how users negotiate authenticity, authorship, and platform governance on Sora. First, users acted as critical evaluators of realism, assessing micro-details such as lighting, shadows, fluid motion, and physics to judge whether AI-generated scenes could plausibly exist. Second, users increasingly shifted from passive viewers to active creators, expressing curiosity about prompts, techniques, and creative processes. Text prompts were perceived as intellectual property, generating concerns about plagiarism and remixing norms. Third, users reported blurred boundaries between real and synthetic media, worried about misinformation, and even questioned the authenticity of other commenters, suspecting bot-generated engagement. Fourth, users contested platform governance: some perceived moderation as inconsistent or opaque, while others shared tactics for evading prompt censorship through misspellings, alternative phrasing, emojis, or other languages. Despite this, many users also enforced ethical norms by discouraging the misuse of real people's images or disrespectful content. Together, these patterns highlighted how AI-mediated platforms complicate notions of reality, creativity, and rule-making in emerging digital ecosystems. Based on the findings, we discuss governance challenges in Sora and how user negotiations inform future platform governance."}
{"id": "2512.05536", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.05536", "abs": "https://arxiv.org/abs/2512.05536", "authors": ["Johannes Ellemose", "Niklas Elmqvist"], "title": "Eye of the Beholder: Towards Measuring Visualization Complexity", "comment": null, "summary": "Constructing expressive and legible visualizations is a key activity for visualization designers. While numerous design guidelines exist, research on how specific graphical features affect perceived visual complexity remains limited. In this paper, we report on a crowdsourced study to collect human ratings of perceived complexity for diverse visualizations. Using these ratings as ground truth, we then evaluated three methods to estimate this perceived complexity: image analysis metrics, multilinear regression using manually coded visualization features, and automated feature extraction using a large language model (LLM). Image complexity metrics showed no correlation with human-perceived visualization complexity. Manual feature coding produced a reasonable predictive model but required substantial effort. In contrast, a zero-shot LLM (GPT-4o mini) demonstrated strong capabilities in both rating complexity and extracting relevant features. Our findings suggest that visualization complexity is truly in the eye of the beholder, yet can be effectively approximated using zero-shot LLM prompting, offering a scalable approach for evaluating the complexity of visualizations. The dataset and code for the study and data analysis can be found at https://osf.io/w85a4/"}
