{"id": "2511.04928", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2511.04928", "abs": "https://arxiv.org/abs/2511.04928", "authors": ["Mahek Desai", "Apoorva Rumale", "Marjan Asadinia", "Sherrene Bogle"], "title": "WIRE: Write Energy Reduction via Encoding in Phase Change Main Memories (PCM)", "comment": null, "summary": "Phase Change Memory (PCM) has rapidly progressed and surpassed Dynamic\nRandom-Access Memory (DRAM) in terms of scalability and standby energy\nefficiency. Altering a PCM cell's state during writes demands substantial\nenergy, posing a significant challenge to PCM's role as the primary main\nmemory. Prior research has explored methods to reduce write energy consumption,\nincluding the elimination of redundant writes, minimizing cell writes, and\nemploying compact row buffers for filtering PCM main memory accesses. However,\nthese techniques had certain drawbacks like bit-wise comparison of the stored\nvalues, preemptive updates increasing write cycles, and poor endurance. In this\npaper, we propose WIRE, a new coding mechanism through which most write\noperations force a maximum of one-bit flip. In this coding-based data storage\nmethod, we look at the frequent value stack and assign a code word to the most\nfrequent values such that they have a hamming distance of one. In most of the\nwrite accesses, writing a value needs one or fewer bit flips which can save\nconsiderable write energy. This technique can be augmented with a wear-leveling\nmechanism at the block level, and rotating the difference bit in the assigned\ncodes, increasing the lifetime of the PCM array at a low cost. Using a\nfull-system evaluation of our method and comparing it to the existing\nmechanisms, our experimental results for multi-threaded and multi-programmed\nworkloads revealed considerable improvement in lifetime and write energy as\nwell as bit flip reduction.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05020", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05020", "abs": "https://arxiv.org/abs/2511.05020", "authors": ["Yawei Cai", "Jiapeng Mi", "Nan Ji", "Haotian Rong", "Yawei Zhang", "Zhangti Li", "Wenbin Guo", "Rensong Xie"], "title": "DAFM: Dynamic Adaptive Fusion for Multi-Model Collaboration in Composed Image Retrieval", "comment": "10 pages,4 figures", "summary": "Composed Image Retrieval (CIR) is a cross-modal task that aims to retrieve\ntarget images from large-scale databases using a reference image and a\nmodification text. Most existing methods rely on a single model to perform\nfeature fusion and similarity matching. However, this paradigm faces two major\nchallenges. First, one model alone can't see the whole picture and the tiny\ndetails at the same time; it has to handle different tasks with the same\nweights, so it often misses the small but important links between image and\ntext. Second, the absence of dynamic weight allocation prevents adaptive\nleveraging of complementary model strengths, so the resulting embedding drifts\naway from the target and misleads the nearest-neighbor search in CIR. To\naddress these limitations, we propose Dynamic Adaptive Fusion (DAFM) for\nmulti-model collaboration in CIR. Rather than optimizing a single method in\nisolation, DAFM exploits the complementary strengths of heterogeneous models\nand adaptively rebalances their contributions. This not only maximizes\nretrieval accuracy but also ensures that the performance gains are independent\nof the fusion order, highlighting the robustness of our approach. Experiments\non the CIRR and FashionIQ benchmarks demonstrate consistent improvements. Our\nmethod achieves a Recall@10 of 93.21 and an Rmean of 84.43 on CIRR, and an\naverage Rmean of 67.48 on FashionIQ, surpassing recent strong baselines by up\nto 4.5%. These results confirm that dynamic multi-model collaboration provides\nan effective and general solution for CIR.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04964", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04964", "abs": "https://arxiv.org/abs/2511.04964", "authors": ["Lingyu Zhang", "Mitchell Wang", "Boyuan Chen"], "title": "Scientific judgment drifts over time in AI ideation", "comment": null, "summary": "Scientific discovery begins with ideas, yet evaluating early-stage research\nconcepts is a subtle and subjective human judgment. As large language models\n(LLMs) are increasingly tasked with generating scientific hypotheses, most\nsystems assume that scientists' evaluations form a fixed gold standard, and\nthat scientists' judgments do not change. Here we challenge this assumption. In\na two-wave study with 7,182 ratings from 57 active researchers across six\nscientific departments, each participant repeatedly evaluated a constant\n\"control\" research idea alongside AI-generated ideas. We show that scientists'\nratings of the very same idea systematically drift over time: overall quality\nscores increased by 0.61 points on a 0-10 scale (P = 0.005), and test-retest\nreliability was only moderate across core dimensions of scientific value,\nrevealing systematic temporal drift in perceived idea quality. Yet the internal\nstructure of judgment remained stable, such as the relative importance placed\non originality, feasibility, clarity. We then aligned an LLM-based ideation\nsystem to first-wave human ratings and used it to select new ideas. Although\nalignment improved agreement with Wave-1 evaluations, its apparent gains\ndisappeared once drift in human standards was accounted for. Thus, tuning to a\nfixed human snapshot produced improvements that were transient rather than\npersistent. These findings reveal that human evaluation of scientific ideas is\nnot static but a dynamic process with stable priorities and requires shifting\ncalibration. Treating one-time human ratings as immutable ground truth risks\noverstating progress in AI-assisted ideation and obscuring the challenge of\nco-evolving with changing expert standards. Drift-aware evaluation protocols\nand longitudinal benchmarks may therefore be essential for building AI systems\nthat reliably augment, rather than overfit to, human scientific judgment.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04690", "categories": ["cs.MM", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04690", "abs": "https://arxiv.org/abs/2511.04690", "authors": ["Christofer Valencia", "Alexis Llumigus\u00edn", "Silvia Alvarez", "Abrahan Arias", "Christian Mejia-Escobar"], "title": "Automatizaci\u00f3n de Informes Geot\u00e9cnicos para Macizos Rocosos con IA", "comment": "17 pages, in Spanish language", "summary": "Geotechnical reports are crucial for assessing the stability of rock\nformations and ensuring safety in modern engineering. Traditionally, these\nreports are prepared manually based on field observations using compasses,\nmagnifying glasses, and notebooks. This method is slow, prone to errors, and\nsubjective in its interpretations. To overcome these limitations, the use of\nartificial intelligence techniques is proposed for the automatic generation of\nreports through the processing of images and field data. The methodology was\nbased on the collection of photographs of rock outcrops and manual samples with\ntheir respective descriptions, as well as on the reports prepared during the\nGeotechnical Studies course. These resources were used to define the report\noutline, prompt engineering, and validate the responses of a multimodal large\nlanguage model (MLLM). The iterative refinement of prompts until structured and\nspecific instructions were obtained for each section of the report proved to be\nan effective alternative to the costly process of fine-tuning the MLLM. The\nsystem evaluation establishes values of 0.455 and 0.653 for the BLEU and\nROUGE-L metrics, respectively, suggesting that automatic descriptions are\ncomparable to those made by experts. This tool, accessible via the web, with an\nintuitive interface and the ability to export to standardized formats,\nrepresents an innovation and an important contribution for professionals and\nstudents of field geology.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05269", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05269", "abs": "https://arxiv.org/abs/2511.05269", "authors": ["Ishan Kavathekar", "Hemang Jain", "Ameya Rathod", "Ponnurangam Kumaraguru", "Tanuja Ganu"], "title": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems", "comment": "Accepted at ICML 2025 MAS Workshop. This version includes additional\n  experiments and analysis", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities as\nautonomous agents through tool use, planning, and decision-making abilities,\nleading to their widespread adoption across diverse tasks. As task complexity\ngrows, multi-agent LLM systems are increasingly used to solve problems\ncollaboratively. However, safety and security of these systems remains largely\nunder-explored. Existing benchmarks and datasets predominantly focus on\nsingle-agent settings, failing to capture the unique vulnerabilities of\nmulti-agent dynamics and co-ordination. To address this gap, we introduce\n$\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent\n$\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the\nrobustness and safety of multi-agent LLM systems. TAMAS includes five distinct\nscenarios comprising 300 adversarial instances across six attack types and 211\ntools, along with 100 harmless tasks. We assess system performance across ten\nbackbone LLMs and three agent interaction configurations from Autogen and\nCrewAI frameworks, highlighting critical challenges and failure modes in\ncurrent multi-agent deployments. Furthermore, we introduce Effective Robustness\nScore (ERS) to assess the tradeoff between safety and task effectiveness of\nthese frameworks. Our findings show that multi-agent systems are highly\nvulnerable to adversarial attacks, underscoring the urgent need for stronger\ndefenses. TAMAS provides a foundation for systematically studying and improving\nthe safety of multi-agent LLM systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05109", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.05109", "abs": "https://arxiv.org/abs/2511.05109", "authors": ["Benjamin Kahl", "Marcus Hebel", "Michael Arens"], "title": "Efficient representation of 3D spatial data for defense-related applications", "comment": null, "summary": "Geospatial sensor data is essential for modern defense and security, offering\nindispensable 3D information for situational awareness. This data, gathered\nfrom sources like lidar sensors and optical cameras, allows for the creation of\ndetailed models of operational environments. In this paper, we provide a\ncomparative analysis of traditional representation methods, such as point\nclouds, voxel grids, and triangle meshes, alongside modern neural and implicit\ntechniques like Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting\n(3DGS). Our evaluation reveals a fundamental trade-off: traditional models\noffer robust geometric accuracy ideal for functional tasks like line-of-sight\nanalysis and physics simulations, while modern methods excel at producing\nhigh-fidelity, photorealistic visuals but often lack geometric reliability.\nBased on these findings, we conclude that a hybrid approach is the most\npromising path forward. We propose a system architecture that combines a\ntraditional mesh scaffold for geometric integrity with a neural representation\nlike 3DGS for visual detail, managed within a hierarchical scene structure to\nensure scalability and performance.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04995", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04995", "abs": "https://arxiv.org/abs/2511.04995", "authors": ["Amol Harsh", "Brainerd Prince", "Siddharth Siddharth", "Deepan Raj Prabakar Muthirayan", "Kabir S Bhalla", "Esraaj Sarkar Gupta", "Siddharth Sahu"], "title": "Enhancing Public Speaking Skills in Engineering Students Through AI", "comment": null, "summary": "This research-to-practice full paper was inspired by the persistent challenge\nin effective communication among engineering students. Public speaking is a\nnecessary skill for future engineers as they have to communicate technical\nknowledge with diverse stakeholders. While universities offer courses or\nworkshops, they are unable to offer sustained and personalized training to\nstudents. Providing comprehensive feedback on both verbal and non-verbal\naspects of public speaking is time-intensive, making consistent and\nindividualized assessment impractical. This study integrates research on verbal\nand non-verbal cues in public speaking to develop an AI-driven assessment model\nfor engineering students. Our approach combines speech analysis, computer\nvision, and sentiment detection into a multi-modal AI system that provides\nassessment and feedback. The model evaluates (1) verbal communication (pitch,\nloudness, pacing, intonation), (2) non-verbal communication (facial\nexpressions, gestures, posture), and (3) expressive coherence, a novel\nintegration ensuring alignment between speech and body language. Unlike\nprevious systems that assess these aspects separately, our model fuses multiple\nmodalities to deliver personalized, scalable feedback. Preliminary testing\ndemonstrated that our AI-generated feedback was moderately aligned with expert\nevaluations. Among the state-of-the-art AI models evaluated, all of which were\nLarge Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro\nemerged as the best-performing, showing the strongest agreement with human\nannotators. By eliminating reliance on human evaluators, this AI-driven public\nspeaking trainer enables repeated practice, helping students naturally align\ntheir speech with body language and emotion, crucial for impactful and\nprofessional communication.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05360", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05360", "abs": "https://arxiv.org/abs/2511.05360", "authors": ["Daniel Berio", "Michael Stroh", "Sylvain Calinon", "Frederic Fol Leymarie", "Oliver Deussen", "Ariel Shamir"], "title": "Neural Image Abstraction Using Long Smoothing B-Splines", "comment": null, "summary": "We integrate smoothing B-splines into a standard differentiable vector\ngraphics (DiffVG) pipeline through linear mapping, and show how this can be\nused to generate smooth and arbitrarily long paths within image-based deep\nlearning systems. We take advantage of derivative-based smoothing costs for\nparametric control of fidelity vs. simplicity tradeoffs, while also enabling\nstylization control in geometric and image spaces. The proposed pipeline is\ncompatible with recent vector graphics generation and vectorization methods. We\ndemonstrate the versatility of our approach with four applications aimed at the\ngeneration of stylized vector graphics: stylized space-filling path generation,\nstroke-based image abstraction, closed-area image abstraction, and stylized\ntext generation.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04997", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04997", "abs": "https://arxiv.org/abs/2511.04997", "authors": ["Walter L. Leite", "Huibin Zhang", "Shibani Rana", "Yide Hao", "Amber D. Hatch", "Lingchen Kong", "Huan Kuang"], "title": "Do intelligent tutoring systems benefit K-12 students? A meta-analysis and evaluation of heterogeneity of treatment effects in the U.S", "comment": null, "summary": "To expand the use of intelligent tutoring systems (ITS) in K-12 schools, it\nis essential to understand the conditions under which their use is most\nbeneficial. This meta-analysis evaluated the heterogeneity of ITS effects\nacross studies focusing on elementary, middle, and high schools in the U.S. It\nincluded 18 studies with 77 effect sizes across 11 ITS. Overall, there was a\nsignificant positive effect size of ITS on U.S. K-12 students' learning\noutcomes (g=0.271, SE=0.011, p=0.001). Furthermore, effect sizes were similar\nacross elementary and middle schools, and for low-achieving students, but were\nlower in studies including rural schools. A MetaForest analysis showed that\nproviding worked-out examples, intervention duration, intervention condition,\ntype of learning outcome, and immediate measurement were the most important\nmoderators of treatment effects.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05066", "categories": ["cs.HC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.05066", "abs": "https://arxiv.org/abs/2511.05066", "authors": ["Philipp Schaad", "Tal Ben-Nun", "Torsten Hoefler"], "title": "VEIL: Reading Control Flow Graphs Like Code", "comment": null, "summary": "Control flow graphs (CFGs) are essential tools for understanding program\nbehavior, yet the size of real-world CFGs makes them difficult to interpret.\nWith thousands of nodes and edges, sophisticated graph drawing algorithms are\nrequired to present them on screens in ways that make them readable and\nunderstandable. However, being designed for general graphs, these algorithms\nfrequently break the natural flow of execution, placing later instructions\nbefore earlier ones and obscuring critical program structures. In this paper,\nwe introduce a set of criteria specifically tailored for CFG visualization,\nfocusing on preserving execution order and making complex structures easier to\nfollow. Building on these criteria, we present VEIL, a new layout algorithm\nthat uses dominator analysis to produce clearer, more intuitive CFG layouts.\nThrough a study of CFGs from real-world applications, we show how our method\nimproves readability and provides improved layout performance compared to state\nof the art graph drawing techniques.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05025", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05025", "abs": "https://arxiv.org/abs/2511.05025", "authors": ["Hala Sheta"], "title": "8bit-GPT: Exploring Human-AI Interaction on Obsolete Macintosh Operating Systems", "comment": "NeurIPS Creative AI Track 2025: Humanity", "summary": "The proliferation of assistive chatbots offering efficient, personalized\ncommunication has driven widespread over-reliance on them for decision-making,\ninformation-seeking and everyday tasks. This dependence was found to have\nadverse consequences on information retention as well as lead to superficial\nemotional attachment. As such, this work introduces 8bit-GPT; a language model\nsimulated on a legacy Macintosh Operating System, to evoke reflection on the\nnature of Human-AI interaction and the consequences of anthropomorphic\nrhetoric. Drawing on reflective design principles such as slow-technology and\ncounterfunctionality, this work aims to foreground the presence of chatbots as\na tool by defamiliarizing the interface and prioritizing inefficient\ninteraction, creating a friction between the familiar and not.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05094", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.05094", "abs": "https://arxiv.org/abs/2511.05094", "authors": ["Zhaoyang Li", "Shangzhuo Xie", "Qianqian Yang"], "title": "FM4Com: Foundation Model for Scene-Adaptive Communication Strategy Optimization", "comment": null, "summary": "The emergence of sixth-generation (6G) networks heralds an intelligent\ncommunication ecosystem driven by AI-native air interfaces. However, current\nphysical-layer designs-typically following modular and isolated optimization\nparadigms-fail to achieve global end-to-end optimality due to neglected\ninter-module dependencies. Although large language models (LLMs) have recently\nbeen applied to communication tasks such as beam prediction and resource\nallocation, existing studies remain limited to single-task or single-modality\nscenarios and lack the ability to jointly reason over communication states and\nuser intents for personalized strategy adaptation. To address these\nlimitations, this paper proposes a novel multimodal communication\ndecision-making model based on reinforcement learning. The proposed model\nsemantically aligns channel state information (CSI) and textual user\ninstructions, enabling comprehensive understanding of both physical-layer\nconditions and communication intents. It then generates physically realizable,\nuser-customized link construction strategies that dynamically adapt to changing\nenvironments and preference tendencies. A two-stage reinforcement learning\nframework is employed: the first stage expands the experience pool via\nheuristic exploration and behavior cloning to obtain a near-optimal\ninitialization, while the second stage fine-tunes the model through\nmulti-objective reinforcement learning considering bit error rate, throughput,\nand complexity. Experimental results demonstrate that the proposed model\nsignificantly outperforms conventional planning-based algorithms under\nchallenging channel conditions, achieving robust, efficient, and personalized\n6G link construction.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05136", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.05136", "abs": "https://arxiv.org/abs/2511.05136", "authors": ["Patrice Labedan", "Nicolas Drougard"], "title": "Interface Homme-Machine pour l'Identification des Liaisons de Coins", "comment": "21 pages, in French language. 25 figures, projet ACCADIL (Ancient\n  Coin Classification Algorithms for DIe Links), Travaux connexes:\n  arXiv:2502.01186", "summary": "ACCADIL is a project that led to the development of software tools for the\nidentification of coin die links from coin photographs. It provides a\ncomputational algorithm based on computer vision and classification techniques,\nalong with an online interface for the interactive verification of results.\nThis guide briefly describes the algorithmic principles, the preparation of\ndata prior to analysis, and the features offered by the interface: dataset\naddition, visualization modes (overlay, side-by-side, magnifier, transparency),\nresult export, and distance visualization. ACCADIL thus provides numismatists\nwith a comprehensive tool for the analysis of die links within a coin\ncollection.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05304", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.05304", "abs": "https://arxiv.org/abs/2511.05304", "authors": ["Akhil Ajikumar", "Sahil Mayenkar", "Steven Yoo", "Sakib Reza", "Mohsen Moghaddam"], "title": "psiUnity: A Platform for Multimodal Data-Driven XR", "comment": null, "summary": "Extended reality (XR) research increasingly relies on the ability to stream\nand synchronize multimodal data between headsets and immersive applications for\ndata-driven interaction and experimentation. However, developers face a\ncritical gap: the Platform for Situated Intelligence (psi), which excels at\ndeterministic temporal alignment and multimodal data management, has been\nlargely inaccessible to the dominant Unity/MRTK ecosystem used for HoloLens\ndevelopment. We introduce psiUnity, an open-source C# integration that bridges\npsi's .NET libraries with Unity 2022.3 and MRTK3 for HoloLens 2. psiUnity\nenables bidirectional, real-time streaming of head pose, hand tracking, gaze,\nIMU, audio, and depth sensor data (AHAT and long-throw) with microsecond-level\ntemporal precision, allowing Unity applications to both consume and produce\nsynchronized multimodal data streams. By embedding psi's native serialization,\nlogging, and temporal coordination directly within Unity's architecture,\npsiUnity extends psi beyond its previous StereoKit limitations and empowers the\nHRI, HCI, and embodied-AI communities to develop reproducible, data-driven XR\ninteractions and experiments within the familiar Unity environment. The\nintegration is available at https://github.com/sailgt/psiUnity.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05346", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.05346", "abs": "https://arxiv.org/abs/2511.05346", "authors": ["Olaf V. Adan", "Dimitra Dritsa", "Steven Houben"], "title": "Semantic Interactivity: leveraging NLP to enable a shared interaction approach for joint activities", "comment": "15 pages, 14 figures", "summary": "Collocated collaboration, where individuals work together in the same\nphysical space and time, remains a cornerstone of effective teamwork. However,\nmost collaborative systems are designed to support individual tasks rather than\njoint activities; they enable interactions for users to complete tasks rather\nthan interactivity to engage in shared experiences. In this work, we introduce\nan NLP-driven mechanism that enables semantic interactivity through a shared\ninteraction mechanism. This mechanism was developed as part of CollEagle, an\ninteractive tabletop system that supports shared externalisation practices by\noffering a low-effort way for users to create, curate, organise, and structure\ninformation to capture the essence of collaborative discussions. Our\npreliminary study highlights the potential for semantic interactivity to\nmediate group interactions, suggesting that the interaction approach paves the\nway for designing novel collaborative interfaces. We contribute our\nimplementation and offer insights for future research to enable semantic\ninteractivity in systems that support joint activities.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05400", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.05400", "abs": "https://arxiv.org/abs/2511.05400", "authors": ["Ma Xiaofan", "Yan Lirong", "Zhao Weijia", "Zeng Weiping", "Wu Huiyue"], "title": "Designing Hierarchical Exploratory Experiences for Ethnic Costumes: A Cultural Gene-Based Perspective", "comment": null, "summary": "Ethnic clothing is a vital carrier of cultural identity, yet its digital\npreservation often results in static displays that fail to convey deep cultural\nmeaning or foster user engagement. Existing practices lack a systematic design\nframework for translating the hierarchical cultural connotations of these\ngarments into dynamic, personalized, and identity-promoting digital\nexperiences. To address this gap, this paper proposes a Three-Layer Cultural\nGene Framework that systematically decodes ethnic costumes from their\nsurface-level visual symbols, through their mid-level socio-cultural contexts,\nto their inner-layer spiritual core. Based on this framework, we designed and\nimplemented an interactive digital platform featuring two key innovations: a\n\"gene-first\" exploratory path that encourages curiosity-driven discovery, and\nan AI-powered co-creation experience. This generative feature allows users to\nco-create personalized narratives and images based on their understanding of\nthe \"inner-layer\" genes, transforming them from passive observers into active\nco-creators. A mixed-methods user study (N=24) was conducted to evaluate the\nplatform. The findings demonstrate that our approach effectively enhances\nusers' cultural cognition, deepens their affective connection, and\nsignificantly promotes their sense of cultural identity. This research\ncontributes a validated framework and a practical exemplar for designing\ngenerative, identity-building digital experiences for cultural heritage,\noffering a new pathway for its preservation and revitalization in the digital\nage.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05410", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05410", "abs": "https://arxiv.org/abs/2511.05410", "authors": ["Justin D. Weisz", "Michael Muller", "Kush R. Varshney"], "title": "Story Arena: A Multi-Agent Environment for Envisioning the Future of Software Engineering", "comment": "8 pages. Appeared in the 2025 Workshop on The End of Programming (as\n  we know it): Envisioning Radical Re-Conceptualizations of Co-Coding with AI,\n  held in conjunction with the Aarhus 2025 Decennial Conference, August 18-22,\n  2025", "summary": "What better way to understand the impact of AI on software engineering than\nto ask AI itself? We constructed Story Arena, a multi-agent \"writer's room\" in\nwhich multiple AI agents, independently imbued with a position statement on the\nfuture of software engineering, converse with each other to develop a shared\nvision. They then use this shared vision to collaboratively construct a design\nfiction that depicts this vision in narrative form. We present \"The Code of\nTrust,\" a short fiction that investigates themes of human comprehension, trust,\ncontent ownership, augmentation vs. replacement, and uncertain futures in\nhuman-AI co-creation.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
