<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 20]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [MazeMate: An LLM-Powered Chatbot to Support Computational Thinking in Gamified Programming Learning](https://arxiv.org/abs/2511.03727)
*Chenyu Hou,Hua Yu,Gaoxia Zhu,John Derek Anas,Jiao Liu,Yew Soon Ong*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Computational Thinking (CT) is a foundational problem-solving skill, and
gamified programming environments are a widely adopted approach to cultivating
it. While large language models (LLMs) provide on-demand programming support,
current applications rarely foster CT development. We present MazeMate, an
LLM-powered chatbot embedded in a 3D Maze programming game, designed to deliver
adaptive, context-sensitive scaffolds aligned with CT processes in maze solving
and maze design. We report on the first classroom implementation with 247
undergraduates. Students rated MazeMate as moderately helpful, with higher
perceived usefulness for maze solving than for maze design. Thematic analysis
confirmed support for CT processes such as decomposition, abstraction, and
algorithmic thinking, while also revealing limitations in supporting maze
design, including mismatched suggestions and fabricated algorithmic solutions.
These findings demonstrate the potential of LLM-based scaffolding to support CT
and underscore directions for design refinement to enhance MazeMate usability
in authentic classrooms.

</details>


### [2] [Efficient On-Device Agents via Adaptive Context Management](https://arxiv.org/abs/2511.03728)
*Sanidhya Vijayvargiya,Rahul Lokesh*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: On-device AI agents offer the potential for personalized, low-latency
assistance, but their deployment is fundamentally constrained by limited memory
capacity, which restricts usable context. This reduced practical context window
creates a trade-off between supporting rich, stateful interactions with complex
tool capabilities and maintaining on-device feasibility. We break this
trade-off with a framework for context-efficient on-device agents, driven by
three synergistic optimizations (1) a dynamic memory system using specialized
LoRA adapters to distill conversational history into a compressed, and
structured Context State Object; (2) a minimalist serialization format for tool
schemas to minimize token overhead per tool; and (3) a just-in-time
schema-passing mechanism that loads full tool definitions only upon tool
selection. We instantiate this framework by adapting a 3B parameter SLM to
context-efficient trajectories and rigorously evaluate it against a
conventional baseline on complex user tasks. Our agent matches, or exceeds, the
performance of a conventional baseline while dramatically compressing context,
achieving more than a 6-fold reduction in initial system prompt context and a
10- to 25-fold reduction in context growth rate based on the interaction
verbosity, demonstrating that strategic context management is key to unlocking
capable and persistent on-device AI.

</details>


### [3] [Beyond Chat: a Framework for LLMs as Human-Centered Support Systems](https://arxiv.org/abs/2511.03729)
*Zhiyin Zhou*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models are moving beyond transactional question answering to
act as companions, coaches, mediators, and curators that scaffold human growth,
decision-making, and well-being. This paper proposes a role-based framework for
human-centered LLM support systems, compares real deployments across domains,
and identifies cross-cutting design principles: transparency, personalization,
guardrails, memory with privacy, and a balance of empathy and reliability. It
outlines evaluation metrics that extend beyond accuracy to trust, engagement,
and longitudinal outcomes. It also analyzes risks including over-reliance,
hallucination, bias, privacy exposure, and unequal access, and proposes future
directions spanning unified evaluation, hybrid human-AI models, memory
architectures, cross-domain benchmarking, and governance. The goal is to
support responsible integration of LLMs in sensitive settings where people need
accompaniment and guidance, not only answers.

</details>


### [4] [Not All Explanations are Created Equal: Investigating the Pitfalls of Current XAI Evaluation](https://arxiv.org/abs/2511.03730)
*Joe Shymanski,Jacob Brue,Sandip Sen*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Explainable Artificial Intelligence (XAI) aims to create transparency in
modern AI models by offering explanations of the models to human users. There
are many ways in which researchers have attempted to evaluate the quality of
these XAI models, such as user studies or proposed objective metrics like
"fidelity". However, these current XAI evaluation techniques are ad hoc at best
and not generalizable. Thus, most studies done within this field conduct simple
user surveys to analyze the difference between no explanations and those
generated by their proposed solution. We do not find this to provide adequate
evidence that the explanations generated are of good quality since we believe
any kind of explanation will be "better" in most metrics when compared to none
at all. Thus, our study looks to highlight this pitfall: most explanations,
regardless of quality or correctness, will increase user satisfaction. We also
propose that emphasis should be placed on actionable explanations. We
demonstrate the validity of both of our claims using an agent assistant to
teach chess concepts to users. The results of this chapter will act as a call
to action in the field of XAI for more comprehensive evaluation techniques for
future research in order to prove explanation quality beyond user satisfaction.
Additionally, we present an analysis of the scenarios in which placebic or
actionable explanations would be most useful.

</details>


### [5] [MimiTalk: Revolutionizing Qualitative Research with Dual-Agent AI](https://arxiv.org/abs/2511.03731)
*Fengming Liu,Shubin Yu*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present MimiTalk, a dual-agent constitutional AI framework designed for
scalable and ethical conversational data collection in social science research.
The framework integrates a supervisor model for strategic oversight and a
conversational model for question generation. We conducted three studies: Study
1 evaluated usability with 20 participants; Study 2 compared 121 AI interviews
to 1,271 human interviews from the MediaSum dataset using NLP metrics and
propensity score matching; Study 3 involved 10 interdisciplinary researchers
conducting both human and AI interviews, followed by blind thematic analysis.
Results across studies indicate that MimiTalk reduces interview anxiety,
maintains conversational coherence, and outperforms human interviews in
information richness, coherence, and stability. AI interviews elicit technical
insights and candid views on sensitive topics, while human interviews better
capture cultural and emotional nuances. These findings suggest that dual-agent
constitutional AI supports effective human-AI collaboration, enabling
replicable, scalable and quality-controlled qualitative research.

</details>


### [6] [Towards Aligning Multimodal LLMs with Human Experts: A Focus on Parent-Child Interaction](https://arxiv.org/abs/2511.04366)
*Weiyan Shi,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While multimodal large language models (MLLMs) are increasingly applied in
human-centred AI systems, their ability to understand complex social
interactions remains uncertain. We present an exploratory study on aligning
MLLMs with speech-language pathologists (SLPs) in analysing joint attention in
parent-child interactions, a key construct in early social-communicative
development. Drawing on interviews and video annotations with three SLPs, we
characterise how observational cues of gaze, action, and vocalisation inform
their reasoning processes. We then test whether an MLLM can approximate this
workflow through a two-stage prompting, separating observation from judgment.
Our findings reveal that alignment is more robust at the observation layer,
where experts share common descriptors, than at the judgement layer, where
interpretive criteria diverge. We position this work as a case-based probe into
expert-AI alignment in complex social behaviour, highlighting both the
feasibility and the challenges of applying MLLMs to socially situated
interaction analysis.

</details>


### [7] [Conversational Collective Intelligence (CCI) using Hyperchat AI in an Authentic Forecasting Task](https://arxiv.org/abs/2511.03732)
*Hans Schumann,Louis Rosenberg,Ganesh Mani,Gregg Willcox*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hyperchat AI is a novel agentic technology that enables thoughtful
conversations among networked human groups of potentially unlimited size. It
allows large teams to discuss complex issues, brainstorm ideas, surface risks,
assess alternatives and efficiently converge on optimized solutions that
amplify the group's Collective Intelligence (CI). A formal study was conducted
to quantify the forecasting accuracy of human groups using Hyperchat AI to
conversationally predict the outcome of Major League Baseball (MLB) games.
During an 8-week period, networked groups of approximately 24 sports fans were
tasked with collaboratively forecasting the winners of 59 baseball games
through real-time conversation facilitated by AI agents. The results showed
that when debating the games using Hyperchat AI technology, the groups
converged on High Confidence predictions that significantly outperformed Vegas
betting markets. Specifically, groups were 78% accurate in their High
Confidence picks, a statistically strong result vs the Vegas odds of 57%
(p=0.020). Had the groups bet against the spread (ATS) on these games, they
would have achieved a 46% ROI against Vegas betting markets. In addition, High
Confidence forecasts that were generated through above-average conversation
rates were 88% accurate, suggesting that real-time interactive deliberation is
central to amplified accuracy.

</details>


### [8] [HACI: A Haptic-Audio Code Interface to Improve Educational Outcomes for Visually Impaired Introductory Programming Students](https://arxiv.org/abs/2511.03733)
*Pratham Gandhi*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This thesis introduces the Haptic-Audio Code Interface (HACI), an educational
tool designed to enhance programming education for visually impaired (VI)
students by integrating haptic and audio feedback to compensate for the absence
of visual cues. HACI consists of a non-resource-intensive web application
supporting JavaScript program development, execution, and debugging, connected
via a cable to an Arduino-powered glove with six integrated haptic motors to
provide physical feedback to VI programmers. Motivated by the need to provide
equitable educational opportunities in computer science, HACI aims to improve
non-visual code navigation, comprehension, summarizing, editing, and debugging
for students with visual impairments while minimizing cognitive load. This work
details HACI's design principles, technical implementation, and a preliminary
evaluation through a pilot study conducted with undergraduate Computer Science
students. Findings indicate that HACI aids in the non-visual navigation and
understanding of programming constructs, although challenges remain in refining
feedback mechanisms to ensure consistency and reliability, as well as
supplementing the current functionality with a more feature-reach and
customizable accessible learning experience which will allow visually impaired
students to fully utilize interleaved haptic and audio feedback. The study
underscores the transformative potential of haptic and audio feedback in
educational practices for the visually impaired, setting a foundation for
future research and development in accessible programming education. This
thesis contributes to the field of accessible technology by demonstrating how
tactile and auditory feedback can be effectively integrated into educational
tools, thereby broadening accessibility in STEM education.

</details>


### [9] [Human Resource Management and AI: A Contextual Transparency Database](https://arxiv.org/abs/2511.03916)
*Ellen Simpson,Ryan Ermovick,Mona Sloane*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: AI tools are proliferating in human resources management (HRM) and
recruiting, helping to mediate access to the labor market. As these systems
spread, profession-specific transparency needs emerging from black-boxed
systems in HRM move into focus. Prior work often frames transparency
technically or abstractly, but we contend AI transparency is a social project
shaped by materials, meanings, and competencies of practice. This paper
introduces the Talent Acquisition and Recruiting AI (TARAI) Index, situating AI
systems within the social practice of recruiting by examining product
functionality, claims, assumptions, and AI clarity. Built through an iterative,
mixed-methods process, the database demonstrates how transparency emerges: not
as a fixed property, but as a dynamic outcome shaped by professional practices,
interactions, and competencies. By centering social practice, our work offers a
grounded, actionable approach to understanding and articulating AI transparency
in HR and provides a blueprint for participatory database design for contextual
transparency in professional practice.

</details>


### [10] [SnappyMeal: Design and Longitudinal Evaluation of a Multimodal AI Food Logging Application](https://arxiv.org/abs/2511.03907)
*Liam Bakar,Zachary Englhardt,Vidya Srinivas,Girish Narayanswamy,Dilini Nissanka,Shwetak Patel,Vikram Iyer*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Food logging, both self-directed and prescribed, plays a critical role in
uncovering correlations between diet, medical, fitness, and health outcomes.
Through conversations with nutritional experts and individuals who practice
dietary tracking, we find current logging methods, such as handwritten and
app-based journaling, are inflexible and result in low adherence and
potentially inaccurate nutritional summaries. These findings, corroborated by
prior literature, emphasize the urgent need for improved food logging methods.
In response, we propose SnappyMeal, an AI-powered dietary tracking system that
leverages multimodal inputs to enable users to more flexibly log their food
intake. SnappyMeal introduces goal-dependent follow-up questions to
intelligently seek missing context from the user and information retrieval from
user grocery receipts and nutritional databases to improve accuracy. We
evaluate SnappyMeal through publicly available nutrition benchmarks and a
multi-user, 3-week, in-the-wild deployment capturing over 500 logged food
instances. Users strongly praised the multiple available input methods and
reported a strong perceived accuracy. These insights suggest that multimodal AI
systems can be leveraged to significantly improve dietary tracking flexibility
and context-awareness, laying the groundwork for a new class of intelligent
self-tracking applications.

</details>


### [11] [Revealing AI Reasoning Increases Trust but Crowds Out Unique Human Knowledge](https://arxiv.org/abs/2511.04050)
*Zenan Chen,Ruijiang Gao,Yingzhi Liang*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Effective human-AI collaboration requires humans to accurately gauge AI
capabilities and calibrate their trust accordingly. Humans often have
context-dependent private information, referred to as Unique Human Knowledge
(UHK), that is crucial for deciding whether to accept or override AI's
recommendations. We examine how displaying AI reasoning affects trust and UHK
utilization through a pre-registered, incentive-compatible experiment (N =
752). We find that revealing AI reasoning, whether brief or extensive, acts as
a powerful persuasive heuristic that significantly increases trust and
agreement with AI recommendations. Rather than helping participants
appropriately calibrate their trust, this transparency induces over-trust that
crowds out UHK utilization. Our results highlight the need for careful
consideration when revealing AI reasoning and call for better information
design in human-AI collaboration systems.

</details>


### [12] ["Everyone Else Does It": The Rise of Preprinting Culture in Computing Disciplines](https://arxiv.org/abs/2511.04081)
*Kyrie Zhixuan Zhou,Justin Eric Chen,Xiang Zheng,Yaoyao Qian,Yunpeng Xiao,Kai Shu*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Preprinting has become a norm in fast-paced computing fields such as
artificial intelligence (AI) and human-computer interaction (HCI). In this
paper, we conducted semistructured interviews with 15 academics in these fields
to reveal their motivations and perceptions of preprinting. The results found a
close relationship between preprinting and characteristics of the fields,
including the huge number of papers, competitiveness in career advancement,
prevalence of scooping, and imperfect peer review system - preprinting comes to
the rescue in one way or another for the participants. Based on the results, we
reflect on the role of preprinting in subverting the traditional publication
mode and outline possibilities of a better publication ecosystem. Our study
contributes by inspecting the community aspects of preprinting practices
through talking to academics.

</details>


### [13] [Scaffolding Metacognition in Programming Education: Understanding Student-AI Interactions and Design Implications](https://arxiv.org/abs/2511.04144)
*Boxuan Ma,Huiyong Li,Gen Li,Li Chen,Cheng Tang,Yinjie Xie,Chenghao Gu,Atsushi Shimada,Shin'ichi Konomi*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative AI tools such as ChatGPT now provide novice programmers with
unprecedented access to instant, personalized support. While this holds clear
promise, their influence on students' metacognitive processes remains
underexplored. Existing work has largely focused on correctness and usability,
with limited attention to whether and how students' use of AI assistants
supports or bypasses key metacognitive processes. This study addresses that gap
by analyzing student-AI interactions through a metacognitive lens in
university-level programming courses. We examined more than 10,000 dialogue
logs collected over three years, complemented by surveys of students and
educators. Our analysis focused on how prompts and responses aligned with
metacognitive phases and strategies. Synthesizing these findings across data
sources, we distill design considerations for AI-powered coding assistants that
aim to support rather than supplant metacognitive engagement. Our findings
provide guidance for developing educational AI tools that strengthen students'
learning processes in programming education.

</details>


### [14] [Graph Neural Networks for User Satisfaction Classification in Human-Computer Interaction](https://arxiv.org/abs/2511.04166)
*Rui Liu,Runsheng Zhang,Shixiao Wang*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study focuses on the problem of user satisfaction classification and
proposes a framework based on graph neural networks to address the limitations
of traditional methods in handling complex interaction relationships and
multidimensional features. User behaviors, interface elements, and their
potential connections are abstracted into a graph structure, and joint modeling
of nodes and edges is used to capture semantics and dependencies in the
interaction process. Graph convolution and attention mechanisms are introduced
to fuse local features and global context, and global pooling with a
classification layer is applied to achieve automated satisfaction
classification. The method extracts deep patterns from structured data and
improves adaptability and robustness in multi-source heterogeneous and dynamic
environments. To verify effectiveness, a public user satisfaction survey
dataset from Kaggle is used, and results are compared with multiple baseline
models across several performance metrics. Experiments show that the method
outperforms existing approaches in accuracy, F1-Score, AUC, and Precision,
demonstrating the advantage of graph-based modeling in satisfaction prediction
tasks. The study not only enriches the theoretical framework of user modeling
but also highlights its practical value in optimizing human-computer
interaction experience.

</details>


### [15] [Active Domain Adaptation for mmWave-based HAR via Renyi Entropy-based Uncertainty Estimation](https://arxiv.org/abs/2511.04219)
*Mingzhi Lin,Teng Huang,Han Ding,Cui Zhao,Fei Wang,Ge Wang,Wei Xi*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human Activity Recognition (HAR) using mmWave radar provides a non-invasive
alternative to traditional sensor-based methods but suffers from domain shift,
where model performance declines in new users, positions, or environments. To
address this, we propose mmADA, an Active Domain Adaptation (ADA) framework
that efficiently adapts mmWave-based HAR models with minimal labeled data.
mmADA enhances adaptation by introducing Renyi Entropy-based uncertainty
estimation to identify and label the most informative target samples.
Additionally, it leverages contrastive learning and pseudo-labeling to refine
feature alignment using unlabeled data. Evaluations with a TI IWR1443BOOST
radar across multiple users, positions, and environments show that mmADA
achieves over 90% accuracy in various cross-domain settings. Comparisons with
five baselines confirm its superior adaptation performance, while further tests
on unseen users, environments, and two additional open-source datasets validate
its robustness and generalization.

</details>


### [16] [Vitessce Link: A Mixed Reality and 2D Display Hybrid Approach for Visual Analysis of 3D Tissue Maps](https://arxiv.org/abs/2511.04262)
*Eric Mörth,Morgan L. Turner,Cydney Nielsen,Xianhao Carton Liu,Mark Keller,Lisa Choy,John Conroy,Tabassum Kakar,Clarence Yapp,Alex Wong,Peter Sorger,Liam McLaughlin,Sanjay Jain,Johanna Beyer,Hanspeter Pfister,Chen Zhu-Tian,Nils Gehlenborg*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Advances in spatial omics and high-resolution imaging enable the creation of
three-dimensional (3D) tissue maps that capture cellular organization and
interactions in situ. While these data provide critical insights into tissue
function and disease, their exploration is often constrained by tools limited
to 2D displays or stereoscopic rendering without analytical integration. We
present Vitessce Link, a web-based hybrid framework that unites a 3D
stereoscopic view in mixed reality with a synchronized 2D display environment.
Users can navigate volumetric data with intuitive hand gestures while
controlling channels, filters, and derived data views through the Vitessce
platform. Built on open standards and running entirely in the browser, Vitessce
Link minimizes friction, supports integration with computational notebooks, and
synchronizes interactions across devices via a lightweight WebSocket
architecture. Case studies in nephrology and oncology demonstrate how the
hybrid approach enhances segmentation evaluation, distance measurement, and
interpretation of spatial relationships. Vitessce Link establishes a paradigm
for integrative, web-native analysis of 3D tissue maps.

</details>


### [17] [HPC-Vis: A Visual Analytic System for Interactive Exploration of Historical Painter Cohorts](https://arxiv.org/abs/2511.04383)
*Yingping Yang,Guangtao You,Jiayi Chen,Jiazhou Chen*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: More than ten thousand Chinese historical painters are recorded in the
literature; their cohort analysis has always been a key area of research on
Chinese painting history for both professional historians and amateur
enthusiasts. However, these painters have very diverse artistic styles and an
extremely complex network of inheritance relationships (e.g., master-apprentice
or style imitation relationships); traditional cohort analysis methods not only
heavily rely on field experience, but also cost a lot of time and effort with
numerous but scattered historical documents. In this paper, we propose HPC-Vis,
a visual analytical system for interactive exploration of historical painter
cohorts. Firstly, a three-stage reconstruction algorithm for inheritance
relationships of painters is proposed, which automatically converts the complex
relationship graph of historical painters into a forest structure that contains
multiple trees with clear inheriting chains, and we visually encoded this
forest as a mountain map to intuitively show potential cohorts of historical
painters. Secondly, a unified artistic style label system with three levels
(i.e., subjects, techniques, and emotions) is established by using large
language models, and it is further visually encoded as a new foldable nested
doughnut chart. Finally, a visually guided human-computer collaborative
interactive exploration mechanism is constructed, in which a painter cohort
recommendation model is designed by integrating style, identity, time, space,
and relationships. Two case studies and a user study demonstrate the advantage
of HPC-Vis on assisting historians in discovering, defining, and validating
cohorts of historical painters.

</details>


### [18] [Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop Refinement of LLM Judges](https://arxiv.org/abs/2511.04478)
*Hyo Jin Do,Zahra Ashktorab,Jasmina Gajcin,Erik Miehling,Martín Santillán Cooper,Qian Pan,Elizabeth M. Daly,Werner Geyer*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, but
its effectiveness is often limited by the scarcity of diverse, representative
data for refining criteria. We present a tool that integrates synthetic data
generation into the LLM-as-a-judge workflow, empowering users to create
tailored and challenging test cases with configurable domains, personas,
lengths, and desired outcomes, including borderline cases. The tool also
supports AI-assisted inline editing of existing test cases. To enhance
transparency and interpretability, it reveals the prompts and explanations
behind each generation. In a user study (N=24), 83% of participants preferred
the tool over manually creating or selecting test cases, as it allowed them to
rapidly generate diverse synthetic data without additional workload. The
generated synthetic data proved as effective as hand-crafted data for both
refining evaluation criteria and aligning with human preferences. These
findings highlight synthetic data as a promising alternative, particularly in
contexts where efficiency and scalability are critical.

</details>


### [19] [Perceptions of AI Bad Behavior: Variations on Discordant Non-Performance](https://arxiv.org/abs/2511.04487)
*Jaime Banks*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Popular discourses are thick with narratives of generative AI's problematic
functions and outcomes, yet there is little understanding of how non-experts
consider AI activities to constitute bad behavior. This study starts to bridge
that gap through inductive analysis of interviews with non-experts (N = 28)
focusing on large-language models in general and their bad behavior,
specifically. Results suggest bad behaviors are not especially salient when
people discuss AI generally but the notion of AI behaving badly is easily
engaged when prompted, and bad behavior becomes even more salient when
evaluating specific AI behaviors. Types of observed behaviors considered bad
mostly align with their inspiring moral foundations; across all observed
behaviors, some variations on non-performance and social discordance were
present. By scaffolding findings at the intersections of moral foundations
theory, construal level theory, and moral dyadism, a tentative framework for
considering AI bad behavior is proposed.

</details>


### [20] [Students' Acceptance of Arduino Technology Integration in Student-Led Science Inquiry: Insights from the Technology Acceptance Model](https://arxiv.org/abs/2511.04614)
*Seok-Hyun Ga,Chun-Yen Chang,Sonya Martin*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study examines high school students' acceptance of Arduino technology in
a student-led, inquiry-based science class, using the extended Technology
Acceptance Model (TAM2) as a guiding framework. Through qualitative analysis of
interviews and classroom observations, we explored how students perceived
Arduino's usefulness and ease of use. Going beyond traditional quantitative TAM
studies, this qualitative TAM research provides a nuanced, in-depth
understanding of the contextual factors shaping technology acceptance. Key
findings reveal that acceptance was driven not only by instrumental factors
like job relevance and output quality but also by the unique sociocultural
context of the Korean education system, where technology use was perceived as
valuable for university admissions (subjective norm and image). Critically,
unlike earlier research that emphasized programming challenges, participants in
this study found Arduino accessible and intuitive, thanks to integrated visual
block-coding tools. These findings highlight the importance of both
technological design and pedagogical support in shaping students' experiences.
Implications for science curriculum design, teacher preparation, and equitable
technology integration in secondary education are discussed.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [21] [On the Brittleness of CLIP Text Encoders](https://arxiv.org/abs/2511.04247)
*Allie Tran,Luca Rossetto*

Main category: cs.MM

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal co-embedding models, especially CLIP, have advanced the state of
the art in zero-shot classification and multimedia information retrieval in
recent years by aligning images and text in a shared representation space.
However, such modals trained on a contrastive alignment can lack stability
towards small input perturbations. Especially when dealing with manually
expressed queries, minor variations in the query can cause large differences in
the ranking of the best-matching results. In this paper, we present a
systematic analysis of the effect of multiple classes of non-semantic query
perturbations in an multimedia information retrieval scenario. We evaluate a
diverse set of lexical, syntactic, and semantic perturbations across multiple
CLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video
collection. Across models, we find that syntactic and semantic perturbations
drive the largest instabilities, while brittleness is concentrated in trivial
surface edits such as punctuation and case. Our results highlight robustness as
a critical dimension for evaluating vision-language models beyond benchmark
accuracy.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [22] [Shellular Metamaterial Design via Compact Electric Potential Parametrization](https://arxiv.org/abs/2511.04025)
*Chang Liu,Bohan Wang*

Main category: cs.GR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a compact yet highly expressive design space for shellular
metamaterials. By employing only a few dozen degrees of freedom, this design
space represents geometries ranging from simple planar configurations to
complex triply periodic minimal surfaces. Coupled with this representation, we
develop an efficient GPU-based homogenization pipeline that evaluates the
structure in under 20 ms and computes the corresponding effective elastic
tensor in near-real-time (0.5 s). The high speed of this evaluation facilitates
an exhaustive exploration of the design space and supports an inverse-design
scheme that tailors the shellular structure to specific macroscopic target
property. Structures derived through this approach exhibit not only geometric
diversity but also a wide spectrum of mechanical responses, covering a broad
range of material properties. Moreover, they achieve up to 91.86% of
theoretical upper bounds, a level of performance comparable to state-of-the-art
shellular structures with low solid volume. Finally, our prototypes, fabricated
via additive manufacturing, confirm the practical manufacturability of these
designs, underscoring their potential for real-world engineering applications.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [23] [OpenMENA: An Open-Source Memristor Interfacing and Compute Board for Neuromorphic Edge-AI Applications](https://arxiv.org/abs/2511.03747)
*Ali Safa,Farida Mohsen,Zainab Ali,Bo Wang,Amine Bermak*

Main category: cs.ET

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Memristive crossbars enable in-memory multiply-accumulate and local
plasticity learning, offering a path to energy-efficient edge AI. To this end,
we present Open-MENA (Open Memristor-in-Memory Accelerator), which, to our
knowledge, is the first fully open memristor interfacing system integrating (i)
a reproducible hardware interface for memristor crossbars with mixed-signal
read-program-verify loops; (ii) a firmware-software stack with high-level APIs
for inference and on-device learning; and (iii) a Voltage-Incremental
Proportional-Integral (VIPI) method to program pre-trained weights into analog
conductances, followed by chip-in-the-loop fine-tuning to mitigate device
non-idealities. OpenMENA is validated on digit recognition, demonstrating the
flow from weight transfer to on-device adaptation, and on a real-world robot
obstacle-avoidance task, where the memristor-based model learns to map
localization inputs to motor commands. OpenMENA is released as open source to
democratize memristor-enabled edge-AI research.

</details>


### [24] [Implementation of transformer-based LLMs with large-scale optoelectronic neurons on a CMOS image sensor platform](https://arxiv.org/abs/2511.04136)
*Neil Na,Chih-Hao Cheng,Shou-Chen Hsu,Che-Fu Liang,Chung-Chih Lin,Nathaniel Y. Na,Andrew I. Shieh,Erik Chen,Haisheng Rong,Richard A. Soref*

Main category: cs.ET

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The recent rapid deployment of datacenter infrastructures for performing
large language models (LLMs) and related artificial intelligence (AI)
applications in the clouds is predicted to incur an exponentially growing
energy consumption in the near-term future. In this paper, we propose and
analyze the implementation of the transformer model, which is the cornerstone
of the modern LLMs, with novel large-scale optoelectronic neurons (OENs)
constructed over the commercially available complementary
metal-oxide-semiconductor (CMOS) image sensor (CIS) platform. With all of the
required optoelectronic devices and electronic circuits integrated in a chiplet
only about 2 cm by 3 cm in size, 175 billon parameters in the case of GPT-3 are
shown to perform inference at an unprecedented speed of 12.6 POPS using only a
40 nm CMOS process node, along with a high power efficiency of 74 TOPS/W and a
high area efficiency of 19 TOPS/mm2, both surpassing the related digital
electronics by roughly two orders of magnitude. The influence of the
quantization formats and the hardware induced errors are numerically
investigated, and are shown to have a minimal impact. Our study presents a new
yet practical path toward analog neural processing units (NPUs) to complement
existing digital processing units.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [25] [OptiMA: A Transaction-Based Framework with Throughput Optimization for Very Complex Multi-Agent Systems](https://arxiv.org/abs/2511.03761)
*Umut Çalıkyılmaz,Nitin Nayak,Jinghua Groppe,Sven Groppe*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In recent years, the research of multi-agent systems has taken a direction to
explore larger and more complex models to fulfill sophisticated tasks. We point
out two possible pitfalls that might be caused by increasing complexity;
susceptibilities to faults, and performance bottlenecks. To prevent the former
threat, we propose a transaction-based framework to design very complex
multi-agent systems (VCMAS). To address the second threat, we offer to
integrate transaction scheduling into the proposed framework. We implemented
both of these ideas to develop the OptiMA framework and show that it is able to
facilitate the execution of VCMAS with more than a hundred agents. We also
demonstrate the effect of transaction scheduling on such a system by showing
improvements up to more than 16\%. Furthermore, we also performed a theoretical
analysis on the transaction scheduling problem and provided practical tools
that can be used for future research on it.

</details>


### [26] [ASAP: an Agentic Solution to Auto-optimize Performance of Large-Scale LLM Training](https://arxiv.org/abs/2511.03844)
*Yuran Ding,Xinwei Chen,Xiaofan Zhang,Zongwei Zhou*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Optimizing large-language model (LLM) training on distributed domain-specific
accelerator systems presents significant challenges due to its complex
optimization space. Existing optimization methods, however, rely on
time-consuming manual tuning or resource-intensive black-box searches, which
struggle to keep pace with the rapidly evolving LLM domain, leading to slow
development and underutilized resources. To address this, we introduce ASAP, an
Agentic Solution to Auto-optimize Performance of Large-Scale LLM Training. It
is a multi-agent system, featuring Coordinator, Analyzer, and Proposal agents,
which integrates LLM reasoning with insights from performance profiling tools,
roofline analysis, and a knowledge base of best practices and successful past
optimizations from human experts. Our proposed design can automate the
diagnosis of performance bottlenecks and recommend optimized sharding
configurations with reasoning, thus effectively improving the efficiency of
distributed LLM training. Experiments have shown that the ASAP-generated
sharding configurations can contribute up to 28% training step time reduction
and 1.43 times throughput improvement. When combined with additional
optimization from human experts, throughput can be further increased to 2.58
times. The proposed ASAP promises to provide a scalable and explainable
methodology for AI-assisted performance engineering in large-scale LLM
training.

</details>


### [27] [Multi-Agent Collaborative Framework For Math Problem Generation](https://arxiv.org/abs/2511.03958)
*Kia Karbasi,Kevin Hong,Mohammad Amin Samadi,Gregory Pottie*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automatic question generation (AQG) for mathematics education remains an
elusive goal for Intelligent Tutoring Systems and educators. While pre-trained
transformer-based language models have significantly advanced natural language
generation, they often struggle to precisely control problem complexity and
cognitive demands. In this paper, we introduce a collaborative multi-agent
framework as a novel method of incorporating inference-time computation into
AQG. This approach leverages multiple agents that iteratively refine generated
question-answer pairs to better balance complexity and cognitive demand. We
evaluate the generated questions on five meta-evaluation criteria: relevance,
importance, clarity, difficulty matching, answerability, to assess the system's
ability to control the required complexity and quality of the questions.
Preliminary evaluations show that this collaborative multi-agent framework
elevates the quality of generated educational content by fostering a more
nuanced balance between cognitive challenge and clarity. These promising
outcomes suggest that integrating collaborative multi-agent workflows can yield
more controlled, pedagogically valuable content that can help advance automated
educational content generation and adaptive learning environments.

</details>
