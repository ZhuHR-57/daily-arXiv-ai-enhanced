<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 11]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [Scientific judgment drifts over time in AI ideation](https://arxiv.org/abs/2511.04964)
*Lingyu Zhang,Mitchell Wang,Boyuan Chen*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scientific discovery begins with ideas, yet evaluating early-stage research
concepts is a subtle and subjective human judgment. As large language models
(LLMs) are increasingly tasked with generating scientific hypotheses, most
systems assume that scientists' evaluations form a fixed gold standard, and
that scientists' judgments do not change. Here we challenge this assumption. In
a two-wave study with 7,182 ratings from 57 active researchers across six
scientific departments, each participant repeatedly evaluated a constant
"control" research idea alongside AI-generated ideas. We show that scientists'
ratings of the very same idea systematically drift over time: overall quality
scores increased by 0.61 points on a 0-10 scale (P = 0.005), and test-retest
reliability was only moderate across core dimensions of scientific value,
revealing systematic temporal drift in perceived idea quality. Yet the internal
structure of judgment remained stable, such as the relative importance placed
on originality, feasibility, clarity. We then aligned an LLM-based ideation
system to first-wave human ratings and used it to select new ideas. Although
alignment improved agreement with Wave-1 evaluations, its apparent gains
disappeared once drift in human standards was accounted for. Thus, tuning to a
fixed human snapshot produced improvements that were transient rather than
persistent. These findings reveal that human evaluation of scientific ideas is
not static but a dynamic process with stable priorities and requires shifting
calibration. Treating one-time human ratings as immutable ground truth risks
overstating progress in AI-assisted ideation and obscuring the challenge of
co-evolving with changing expert standards. Drift-aware evaluation protocols
and longitudinal benchmarks may therefore be essential for building AI systems
that reliably augment, rather than overfit to, human scientific judgment.

</details>


### [2] [Enhancing Public Speaking Skills in Engineering Students Through AI](https://arxiv.org/abs/2511.04995)
*Amol Harsh,Brainerd Prince,Siddharth Siddharth,Deepan Raj Prabakar Muthirayan,Kabir S Bhalla,Esraaj Sarkar Gupta,Siddharth Sahu*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This research-to-practice full paper was inspired by the persistent challenge
in effective communication among engineering students. Public speaking is a
necessary skill for future engineers as they have to communicate technical
knowledge with diverse stakeholders. While universities offer courses or
workshops, they are unable to offer sustained and personalized training to
students. Providing comprehensive feedback on both verbal and non-verbal
aspects of public speaking is time-intensive, making consistent and
individualized assessment impractical. This study integrates research on verbal
and non-verbal cues in public speaking to develop an AI-driven assessment model
for engineering students. Our approach combines speech analysis, computer
vision, and sentiment detection into a multi-modal AI system that provides
assessment and feedback. The model evaluates (1) verbal communication (pitch,
loudness, pacing, intonation), (2) non-verbal communication (facial
expressions, gestures, posture), and (3) expressive coherence, a novel
integration ensuring alignment between speech and body language. Unlike
previous systems that assess these aspects separately, our model fuses multiple
modalities to deliver personalized, scalable feedback. Preliminary testing
demonstrated that our AI-generated feedback was moderately aligned with expert
evaluations. Among the state-of-the-art AI models evaluated, all of which were
Large Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro
emerged as the best-performing, showing the strongest agreement with human
annotators. By eliminating reliance on human evaluators, this AI-driven public
speaking trainer enables repeated practice, helping students naturally align
their speech with body language and emotion, crucial for impactful and
professional communication.

</details>


### [3] [Do intelligent tutoring systems benefit K-12 students? A meta-analysis and evaluation of heterogeneity of treatment effects in the U.S](https://arxiv.org/abs/2511.04997)
*Walter L. Leite,Huibin Zhang,Shibani Rana,Yide Hao,Amber D. Hatch,Lingchen Kong,Huan Kuang*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: To expand the use of intelligent tutoring systems (ITS) in K-12 schools, it
is essential to understand the conditions under which their use is most
beneficial. This meta-analysis evaluated the heterogeneity of ITS effects
across studies focusing on elementary, middle, and high schools in the U.S. It
included 18 studies with 77 effect sizes across 11 ITS. Overall, there was a
significant positive effect size of ITS on U.S. K-12 students' learning
outcomes (g=0.271, SE=0.011, p=0.001). Furthermore, effect sizes were similar
across elementary and middle schools, and for low-achieving students, but were
lower in studies including rural schools. A MetaForest analysis showed that
providing worked-out examples, intervention duration, intervention condition,
type of learning outcome, and immediate measurement were the most important
moderators of treatment effects.

</details>


### [4] [VEIL: Reading Control Flow Graphs Like Code](https://arxiv.org/abs/2511.05066)
*Philipp Schaad,Tal Ben-Nun,Torsten Hoefler*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Control flow graphs (CFGs) are essential tools for understanding program
behavior, yet the size of real-world CFGs makes them difficult to interpret.
With thousands of nodes and edges, sophisticated graph drawing algorithms are
required to present them on screens in ways that make them readable and
understandable. However, being designed for general graphs, these algorithms
frequently break the natural flow of execution, placing later instructions
before earlier ones and obscuring critical program structures. In this paper,
we introduce a set of criteria specifically tailored for CFG visualization,
focusing on preserving execution order and making complex structures easier to
follow. Building on these criteria, we present VEIL, a new layout algorithm
that uses dominator analysis to produce clearer, more intuitive CFG layouts.
Through a study of CFGs from real-world applications, we show how our method
improves readability and provides improved layout performance compared to state
of the art graph drawing techniques.

</details>


### [5] [8bit-GPT: Exploring Human-AI Interaction on Obsolete Macintosh Operating Systems](https://arxiv.org/abs/2511.05025)
*Hala Sheta*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The proliferation of assistive chatbots offering efficient, personalized
communication has driven widespread over-reliance on them for decision-making,
information-seeking and everyday tasks. This dependence was found to have
adverse consequences on information retention as well as lead to superficial
emotional attachment. As such, this work introduces 8bit-GPT; a language model
simulated on a legacy Macintosh Operating System, to evoke reflection on the
nature of Human-AI interaction and the consequences of anthropomorphic
rhetoric. Drawing on reflective design principles such as slow-technology and
counterfunctionality, this work aims to foreground the presence of chatbots as
a tool by defamiliarizing the interface and prioritizing inefficient
interaction, creating a friction between the familiar and not.

</details>


### [6] [FM4Com: Foundation Model for Scene-Adaptive Communication Strategy Optimization](https://arxiv.org/abs/2511.05094)
*Zhaoyang Li,Shangzhuo Xie,Qianqian Yang*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The emergence of sixth-generation (6G) networks heralds an intelligent
communication ecosystem driven by AI-native air interfaces. However, current
physical-layer designs-typically following modular and isolated optimization
paradigms-fail to achieve global end-to-end optimality due to neglected
inter-module dependencies. Although large language models (LLMs) have recently
been applied to communication tasks such as beam prediction and resource
allocation, existing studies remain limited to single-task or single-modality
scenarios and lack the ability to jointly reason over communication states and
user intents for personalized strategy adaptation. To address these
limitations, this paper proposes a novel multimodal communication
decision-making model based on reinforcement learning. The proposed model
semantically aligns channel state information (CSI) and textual user
instructions, enabling comprehensive understanding of both physical-layer
conditions and communication intents. It then generates physically realizable,
user-customized link construction strategies that dynamically adapt to changing
environments and preference tendencies. A two-stage reinforcement learning
framework is employed: the first stage expands the experience pool via
heuristic exploration and behavior cloning to obtain a near-optimal
initialization, while the second stage fine-tunes the model through
multi-objective reinforcement learning considering bit error rate, throughput,
and complexity. Experimental results demonstrate that the proposed model
significantly outperforms conventional planning-based algorithms under
challenging channel conditions, achieving robust, efficient, and personalized
6G link construction.

</details>


### [7] [Interface Homme-Machine pour l'Identification des Liaisons de Coins](https://arxiv.org/abs/2511.05136)
*Patrice Labedan,Nicolas Drougard*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: ACCADIL is a project that led to the development of software tools for the
identification of coin die links from coin photographs. It provides a
computational algorithm based on computer vision and classification techniques,
along with an online interface for the interactive verification of results.
This guide briefly describes the algorithmic principles, the preparation of
data prior to analysis, and the features offered by the interface: dataset
addition, visualization modes (overlay, side-by-side, magnifier, transparency),
result export, and distance visualization. ACCADIL thus provides numismatists
with a comprehensive tool for the analysis of die links within a coin
collection.

</details>


### [8] [psiUnity: A Platform for Multimodal Data-Driven XR](https://arxiv.org/abs/2511.05304)
*Akhil Ajikumar,Sahil Mayenkar,Steven Yoo,Sakib Reza,Mohsen Moghaddam*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Extended reality (XR) research increasingly relies on the ability to stream
and synchronize multimodal data between headsets and immersive applications for
data-driven interaction and experimentation. However, developers face a
critical gap: the Platform for Situated Intelligence (psi), which excels at
deterministic temporal alignment and multimodal data management, has been
largely inaccessible to the dominant Unity/MRTK ecosystem used for HoloLens
development. We introduce psiUnity, an open-source C# integration that bridges
psi's .NET libraries with Unity 2022.3 and MRTK3 for HoloLens 2. psiUnity
enables bidirectional, real-time streaming of head pose, hand tracking, gaze,
IMU, audio, and depth sensor data (AHAT and long-throw) with microsecond-level
temporal precision, allowing Unity applications to both consume and produce
synchronized multimodal data streams. By embedding psi's native serialization,
logging, and temporal coordination directly within Unity's architecture,
psiUnity extends psi beyond its previous StereoKit limitations and empowers the
HRI, HCI, and embodied-AI communities to develop reproducible, data-driven XR
interactions and experiments within the familiar Unity environment. The
integration is available at https://github.com/sailgt/psiUnity.

</details>


### [9] [Semantic Interactivity: leveraging NLP to enable a shared interaction approach for joint activities](https://arxiv.org/abs/2511.05346)
*Olaf V. Adan,Dimitra Dritsa,Steven Houben*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Collocated collaboration, where individuals work together in the same
physical space and time, remains a cornerstone of effective teamwork. However,
most collaborative systems are designed to support individual tasks rather than
joint activities; they enable interactions for users to complete tasks rather
than interactivity to engage in shared experiences. In this work, we introduce
an NLP-driven mechanism that enables semantic interactivity through a shared
interaction mechanism. This mechanism was developed as part of CollEagle, an
interactive tabletop system that supports shared externalisation practices by
offering a low-effort way for users to create, curate, organise, and structure
information to capture the essence of collaborative discussions. Our
preliminary study highlights the potential for semantic interactivity to
mediate group interactions, suggesting that the interaction approach paves the
way for designing novel collaborative interfaces. We contribute our
implementation and offer insights for future research to enable semantic
interactivity in systems that support joint activities.

</details>


### [10] [Designing Hierarchical Exploratory Experiences for Ethnic Costumes: A Cultural Gene-Based Perspective](https://arxiv.org/abs/2511.05400)
*Ma Xiaofan,Yan Lirong,Zhao Weijia,Zeng Weiping,Wu Huiyue*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ethnic clothing is a vital carrier of cultural identity, yet its digital
preservation often results in static displays that fail to convey deep cultural
meaning or foster user engagement. Existing practices lack a systematic design
framework for translating the hierarchical cultural connotations of these
garments into dynamic, personalized, and identity-promoting digital
experiences. To address this gap, this paper proposes a Three-Layer Cultural
Gene Framework that systematically decodes ethnic costumes from their
surface-level visual symbols, through their mid-level socio-cultural contexts,
to their inner-layer spiritual core. Based on this framework, we designed and
implemented an interactive digital platform featuring two key innovations: a
"gene-first" exploratory path that encourages curiosity-driven discovery, and
an AI-powered co-creation experience. This generative feature allows users to
co-create personalized narratives and images based on their understanding of
the "inner-layer" genes, transforming them from passive observers into active
co-creators. A mixed-methods user study (N=24) was conducted to evaluate the
platform. The findings demonstrate that our approach effectively enhances
users' cultural cognition, deepens their affective connection, and
significantly promotes their sense of cultural identity. This research
contributes a validated framework and a practical exemplar for designing
generative, identity-building digital experiences for cultural heritage,
offering a new pathway for its preservation and revitalization in the digital
age.

</details>


### [11] [Story Arena: A Multi-Agent Environment for Envisioning the Future of Software Engineering](https://arxiv.org/abs/2511.05410)
*Justin D. Weisz,Michael Muller,Kush R. Varshney*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: What better way to understand the impact of AI on software engineering than
to ask AI itself? We constructed Story Arena, a multi-agent "writer's room" in
which multiple AI agents, independently imbued with a position statement on the
future of software engineering, converse with each other to develop a shared
vision. They then use this shared vision to collaboratively construct a design
fiction that depicts this vision in narrative form. We present "The Code of
Trust," a short fiction that investigates themes of human comprehension, trust,
content ownership, augmentation vs. replacement, and uncertain futures in
human-AI co-creation.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [12] [WIRE: Write Energy Reduction via Encoding in Phase Change Main Memories (PCM)](https://arxiv.org/abs/2511.04928)
*Mahek Desai,Apoorva Rumale,Marjan Asadinia,Sherrene Bogle*

Main category: cs.ET

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Phase Change Memory (PCM) has rapidly progressed and surpassed Dynamic
Random-Access Memory (DRAM) in terms of scalability and standby energy
efficiency. Altering a PCM cell's state during writes demands substantial
energy, posing a significant challenge to PCM's role as the primary main
memory. Prior research has explored methods to reduce write energy consumption,
including the elimination of redundant writes, minimizing cell writes, and
employing compact row buffers for filtering PCM main memory accesses. However,
these techniques had certain drawbacks like bit-wise comparison of the stored
values, preemptive updates increasing write cycles, and poor endurance. In this
paper, we propose WIRE, a new coding mechanism through which most write
operations force a maximum of one-bit flip. In this coding-based data storage
method, we look at the frequent value stack and assign a code word to the most
frequent values such that they have a hamming distance of one. In most of the
write accesses, writing a value needs one or fewer bit flips which can save
considerable write energy. This technique can be augmented with a wear-leveling
mechanism at the block level, and rotating the difference bit in the assigned
codes, increasing the lifetime of the PCM array at a low cost. Using a
full-system evaluation of our method and comparing it to the existing
mechanisms, our experimental results for multi-threaded and multi-programmed
workloads revealed considerable improvement in lifetime and write energy as
well as bit flip reduction.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [13] [DAFM: Dynamic Adaptive Fusion for Multi-Model Collaboration in Composed Image Retrieval](https://arxiv.org/abs/2511.05020)
*Yawei Cai,Jiapeng Mi,Nan Ji,Haotian Rong,Yawei Zhang,Zhangti Li,Wenbin Guo,Rensong Xie*

Main category: cs.GR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Composed Image Retrieval (CIR) is a cross-modal task that aims to retrieve
target images from large-scale databases using a reference image and a
modification text. Most existing methods rely on a single model to perform
feature fusion and similarity matching. However, this paradigm faces two major
challenges. First, one model alone can't see the whole picture and the tiny
details at the same time; it has to handle different tasks with the same
weights, so it often misses the small but important links between image and
text. Second, the absence of dynamic weight allocation prevents adaptive
leveraging of complementary model strengths, so the resulting embedding drifts
away from the target and misleads the nearest-neighbor search in CIR. To
address these limitations, we propose Dynamic Adaptive Fusion (DAFM) for
multi-model collaboration in CIR. Rather than optimizing a single method in
isolation, DAFM exploits the complementary strengths of heterogeneous models
and adaptively rebalances their contributions. This not only maximizes
retrieval accuracy but also ensures that the performance gains are independent
of the fusion order, highlighting the robustness of our approach. Experiments
on the CIRR and FashionIQ benchmarks demonstrate consistent improvements. Our
method achieves a Recall@10 of 93.21 and an Rmean of 84.43 on CIRR, and an
average Rmean of 67.48 on FashionIQ, surpassing recent strong baselines by up
to 4.5%. These results confirm that dynamic multi-model collaboration provides
an effective and general solution for CIR.

</details>


### [14] [Efficient representation of 3D spatial data for defense-related applications](https://arxiv.org/abs/2511.05109)
*Benjamin Kahl,Marcus Hebel,Michael Arens*

Main category: cs.GR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Geospatial sensor data is essential for modern defense and security, offering
indispensable 3D information for situational awareness. This data, gathered
from sources like lidar sensors and optical cameras, allows for the creation of
detailed models of operational environments. In this paper, we provide a
comparative analysis of traditional representation methods, such as point
clouds, voxel grids, and triangle meshes, alongside modern neural and implicit
techniques like Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting
(3DGS). Our evaluation reveals a fundamental trade-off: traditional models
offer robust geometric accuracy ideal for functional tasks like line-of-sight
analysis and physics simulations, while modern methods excel at producing
high-fidelity, photorealistic visuals but often lack geometric reliability.
Based on these findings, we conclude that a hybrid approach is the most
promising path forward. We propose a system architecture that combines a
traditional mesh scaffold for geometric integrity with a neural representation
like 3DGS for visual detail, managed within a hierarchical scene structure to
ensure scalability and performance.

</details>


### [15] [Neural Image Abstraction Using Long Smoothing B-Splines](https://arxiv.org/abs/2511.05360)
*Daniel Berio,Michael Stroh,Sylvain Calinon,Frederic Fol Leymarie,Oliver Deussen,Ariel Shamir*

Main category: cs.GR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We integrate smoothing B-splines into a standard differentiable vector
graphics (DiffVG) pipeline through linear mapping, and show how this can be
used to generate smooth and arbitrarily long paths within image-based deep
learning systems. We take advantage of derivative-based smoothing costs for
parametric control of fidelity vs. simplicity tradeoffs, while also enabling
stylization control in geometric and image spaces. The proposed pipeline is
compatible with recent vector graphics generation and vectorization methods. We
demonstrate the versatility of our approach with four applications aimed at the
generation of stylized vector graphics: stylized space-filling path generation,
stroke-based image abstraction, closed-area image abstraction, and stylized
text generation.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [16] [Automatización de Informes Geotécnicos para Macizos Rocosos con IA](https://arxiv.org/abs/2511.04690)
*Christofer Valencia,Alexis Llumigusín,Silvia Alvarez,Abrahan Arias,Christian Mejia-Escobar*

Main category: cs.MM

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Geotechnical reports are crucial for assessing the stability of rock
formations and ensuring safety in modern engineering. Traditionally, these
reports are prepared manually based on field observations using compasses,
magnifying glasses, and notebooks. This method is slow, prone to errors, and
subjective in its interpretations. To overcome these limitations, the use of
artificial intelligence techniques is proposed for the automatic generation of
reports through the processing of images and field data. The methodology was
based on the collection of photographs of rock outcrops and manual samples with
their respective descriptions, as well as on the reports prepared during the
Geotechnical Studies course. These resources were used to define the report
outline, prompt engineering, and validate the responses of a multimodal large
language model (MLLM). The iterative refinement of prompts until structured and
specific instructions were obtained for each section of the report proved to be
an effective alternative to the costly process of fine-tuning the MLLM. The
system evaluation establishes values of 0.455 and 0.653 for the BLEU and
ROUGE-L metrics, respectively, suggesting that automatic descriptions are
comparable to those made by experts. This tool, accessible via the web, with an
intuitive interface and the ability to export to standardized formats,
represents an innovation and an important contribution for professionals and
students of field geology.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [17] [TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems](https://arxiv.org/abs/2511.05269)
*Ishan Kavathekar,Hemang Jain,Ameya Rathod,Ponnurangam Kumaraguru,Tanuja Ganu*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities as
autonomous agents through tool use, planning, and decision-making abilities,
leading to their widespread adoption across diverse tasks. As task complexity
grows, multi-agent LLM systems are increasingly used to solve problems
collaboratively. However, safety and security of these systems remains largely
under-explored. Existing benchmarks and datasets predominantly focus on
single-agent settings, failing to capture the unique vulnerabilities of
multi-agent dynamics and co-ordination. To address this gap, we introduce
$\textbf{T}$hreats and $\textbf{A}$ttacks in $\textbf{M}$ulti-$\textbf{A}$gent
$\textbf{S}$ystems ($\textbf{TAMAS}$), a benchmark designed to evaluate the
robustness and safety of multi-agent LLM systems. TAMAS includes five distinct
scenarios comprising 300 adversarial instances across six attack types and 211
tools, along with 100 harmless tasks. We assess system performance across ten
backbone LLMs and three agent interaction configurations from Autogen and
CrewAI frameworks, highlighting critical challenges and failure modes in
current multi-agent deployments. Furthermore, we introduce Effective Robustness
Score (ERS) to assess the tradeoff between safety and task effectiveness of
these frameworks. Our findings show that multi-agent systems are highly
vulnerable to adversarial attacks, underscoring the urgent need for stronger
defenses. TAMAS provides a foundation for systematically studying and improving
the safety of multi-agent LLM systems.

</details>
