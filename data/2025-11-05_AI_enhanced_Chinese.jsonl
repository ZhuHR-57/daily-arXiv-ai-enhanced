{"id": "2511.01912", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01912", "abs": "https://arxiv.org/abs/2511.01912", "authors": ["Wenzhe Fan", "Ning Yan", "Masood Mortazavi"], "title": "EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory", "comment": null, "summary": "Planning has been a cornerstone of artificial intelligence for solving\ncomplex problems, and recent progress in LLM-based multi-agent frameworks have\nbegun to extend this capability. However, the role of human-like memory within\nthese frameworks remains largely unexplored. Understanding how agents\ncoordinate through memory is critical for natural language planning, where\niterative reasoning, constraint tracking, and error correction drive the\nsuccess. Inspired by working memory model in cognitive psychology, we present\nEvoMem, a multi-agent framework built on a dual-evolving memory mechanism. The\nframework consists of three agents (Constraint Extractor, Verifier, and Actor)\nand two memory modules: Constraint Memory (CMem), which evolves across queries\nby storing task-specific rules and constraints while remains fixed within a\nquery, and Query-feedback Memory (QMem), which evolves within a query by\naccumulating feedback across iterations for solution refinement. Both memory\nmodules are reset at the end of each query session. Evaluations on trip\nplanning, meeting planning, and calendar scheduling show consistent performance\nimprovements, highlighting the effectiveness of EvoMem. This success\nunderscores the importance of memory in enhancing multi-agent planning.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02217", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02217", "abs": "https://arxiv.org/abs/2511.02217", "authors": ["Manonmani Sekar", "Nasim Nezamoddini"], "title": "Optimizing Multi-Lane Intersection Performance in Mixed Autonomy Environments", "comment": null, "summary": "One of the main challenges in managing traffic at multilane intersections is\nensuring smooth coordination between human-driven vehicles (HDVs) and connected\nautonomous vehicles (CAVs). This paper presents a novel traffic signal control\nframework that combines Graph Attention Networks (GAT) with Soft Actor-Critic\n(SAC) reinforcement learning to address this challenge. GATs are used to model\nthe dynamic graph- structured nature of traffic flow to capture spatial and\ntemporal dependencies between lanes and signal phases. The proposed SAC is a\nrobust off-policy reinforcement learning algorithm that enables adaptive signal\ncontrol through entropy-optimized decision making. This design allows the\nsystem to coordinate the signal timing and vehicle movement simultaneously with\nobjectives focused on minimizing travel time, enhancing performance, ensuring\nsafety, and improving fairness between HDVs and CAVs. The model is evaluated\nusing a SUMO-based simulation of a four-way intersection and incorporating\ndifferent traffic densities and CAV penetration rates. The experimental results\ndemonstrate the effectiveness of the GAT-SAC approach by achieving a 24.1%\nreduction in average delay and up to 29.2% fewer traffic violations compared to\ntraditional methods. Additionally, the fairness ratio between HDVs and CAVs\nimproved to 1.59, indicating more equitable treatment across vehicle types.\nThese findings suggest that the GAT-SAC framework holds significant promise for\nreal-world deployment in mixed-autonomy traffic systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02304", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02304", "abs": "https://arxiv.org/abs/2511.02304", "authors": ["Beyazit Yalcinkaya", "Marcell Vazquez-Chanlatte", "Ameesh Shah", "Hanna Krasowski", "Sanjit A. Seshia"], "title": "Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning", "comment": null, "summary": "We study the problem of learning multi-task, multi-agent policies for\ncooperative, temporal objectives, under centralized training, decentralized\nexecution. In this setting, using automata to represent tasks enables the\ndecomposition of complex tasks into simpler sub-tasks that can be assigned to\nagents. However, existing approaches remain sample-inefficient and are limited\nto the single-task case. In this work, we present Automata-Conditioned\nCooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for\nlearning task-conditioned, decentralized team policies. We identify the main\nchallenges to ACC-MARL's feasibility in practice, propose solutions, and prove\nthe correctness of our approach. We further show that the value functions of\nlearned policies can be used to assign tasks optimally at test time.\nExperiments show emergent task-aware, multi-step coordination among agents,\ne.g., pressing a button to unlock a door, holding the door, and\nshort-circuiting tasks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.01894", "categories": ["cs.GR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01894", "abs": "https://arxiv.org/abs/2511.01894", "authors": ["Fangbing Liu", "Pengfei Duan", "Wen Li", "Yi He"], "title": "LGCC: Enhancing Flow Matching Based Text-Guided Image Editing with Local Gaussian Coupling and Context Consistency", "comment": null, "summary": "Recent advancements have demonstrated the great potential of flow\nmatching-based Multimodal Large Language Models (MLLMs) in image editing.\nHowever, state-of-the-art works like BAGEL face limitations, including detail\ndegradation, content inconsistency, and inefficiency due to their reliance on\nrandom noise initialization. To address these issues, we propose LGCC, a novel\nframework with two key components: Local Gaussian Noise Coupling (LGNC) and\nContent Consistency Loss (CCL). LGNC preserves spatial details by modeling\ntarget image embeddings and their locally perturbed counterparts as coupled\npairs, while CCL ensures semantic alignment between edit instructions and image\nmodifications, preventing unintended content removal. By integrating LGCC with\nthe BAGEL pre-trained model via curriculum learning, we significantly reduce\ninference steps, improving local detail scores on I2EBench by 1.60% and overall\nscores by 0.53%. LGCC achieves 3x -- 5x speedup for lightweight editing and 2x\nfor universal editing, requiring only 40% -- 50% of the inference time of BAGEL\nor Flux. These results demonstrate LGCC's ability to preserve detail, maintain\ncontextual integrity, and enhance inference speed, offering a cost-efficient\nsolution without compromising editing quality.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02079", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2511.02079", "abs": "https://arxiv.org/abs/2511.02079", "authors": ["Jamie Ngoc Dinh", "Snehesh Shrestha", "You-Jin Kim", "Jun Nishida", "Myungin Lee"], "title": "NeuResonance: Exploring Feedback Experiences for Fostering the Inter-brain Synchronization", "comment": "Conference Paper, 16 pages. Published at the 2025 CHI Conference on\n  Human Factors in Computing Systems", "summary": "When several individuals collaborate on a shared task, their brain activities\noften synchronize. This phenomenon, known as Inter-brain Synchronization (IBS),\nis notable for inducing prosocial outcomes such as enhanced interpersonal\nfeelings, including closeness, trust, empathy, and more. Further strengthening\nthe IBS with the aid of external feedback would be beneficial for scenarios\nwhere those prosocial feelings play a vital role in interpersonal\ncommunication, such as rehabilitation between a therapist and a patient, motor\nskill learning between a teacher and a student, and group performance art. This\npaper investigates whether visual, auditory, and haptic feedback of the IBS\nlevel can further enhance its intensity, offering design recommendations for\nfeedback systems in IBS. We report findings when three different types of\nfeedback were provided: IBS level feedback by means of on-body projection\nmapping, sonification using chords, and vibration bands attached to the wrist.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02234", "categories": ["cs.MM", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.02234", "abs": "https://arxiv.org/abs/2511.02234", "authors": ["Jiawei Liu", "Enis Berk \u00c7oban", "Zarina Schevchenko", "Hao Tang", "Zhigang Zhu", "Michael I Mandel", "Johanna Devaney"], "title": "An Evaluation of Interleaved Instruction Tuning on Semantic Reasoning Performance in an Audio MLLM", "comment": null, "summary": "Standard training for Multi-modal Large Language Models (MLLMs) involves\nconcatenating non-textual information, like vision or audio, with a text\nprompt. This approach may not encourage deep integration of modalities,\nlimiting the model's ability to leverage the core language model's reasoning\ncapabilities. This work examined the impact of interleaved instruction tuning\nin an audio MLLM, where audio tokens are interleaved within the prompt. Using\nthe Listen, Think, and Understand (LTU) model as a testbed, we conduct an\nexperiment using the Synonym and Hypernym Audio Reasoning Dataset (SHARD), our\nnewly created reasoning benchmark for audio-based semantic reasoning focusing\non synonym and hypernym recognition. Our findings show that while even\nzero-shot interleaved prompting improves performance on our reasoning tasks, a\nsmall amount of fine-tuning using interleaved training prompts improves the\nresults further, however, at the expense of the MLLM's audio labeling ability.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02133", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02133", "abs": "https://arxiv.org/abs/2511.02133", "authors": ["Suyang Li", "Fernando Fajardo-Rojas", "Diego Gomez-Gualdron", "Remco Chang", "Mingwei Li"], "title": "AlloyLens: A Visual Analytics Tool for High-throughput Alloy Screening and Inverse Design", "comment": "IEEE VIS 2025 Scientific Visualization Contest Honorable Mention\n  Links: PyPI package (Jupyter widget): https://pypi.org/project/alloylens/ Web\n  demo: http://susiesyli.com/alloylens-web/ GitHub:\n  https://github.com/susiesyli/alloylens SciVis contest 2025:\n  https://sciviscontest2025.github.io/data/", "summary": "Designing multi-functional alloys requires exploring high-dimensional\ncomposition-structure-property spaces, yet current tools are limited to\nlow-dimensional projections and offer limited support for sensitivity or\nmulti-objective tradeoff reasoning. We introduce AlloyLens, an interactive\nvisual analytics system combining a coordinated scatterplot matrix (SPLOM),\ndynamic parameter sliders, gradient-based sensitivity curves, and nearest\nneighbor recommendations. This integrated approach reveals latent structure in\nsimulation data, exposes the local impact of compositional changes, and\nhighlights tradeoffs when exact matches are absent. We validate the system\nthrough case studies co-developed with domain experts spanning structural,\nthermal, and electrical alloy design.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02478", "categories": ["cs.MM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02478", "abs": "https://arxiv.org/abs/2511.02478", "authors": ["Bingyan Xie", "Yongpeng Wu", "Yuxuan Shi", "Biqian Feng", "Wenjun Zhang", "Jihong Park", "Tony Quek"], "title": "Wireless Video Semantic Communication with Decoupled Diffusion Multi-frame Compensation", "comment": null, "summary": "Existing wireless video transmission schemes directly conduct video coding in\npixel level, while neglecting the inner semantics contained in videos. In this\npaper, we propose a wireless video semantic communication framework with\ndecoupled diffusion multi-frame compensation (DDMFC), abbreviated as WVSC-D,\nwhich integrates the idea of semantic communication into wireless video\ntransmission scenarios. WVSC-D first encodes original video frames as semantic\nframes and then conducts video coding based on such compact representations,\nenabling the video coding in semantic level rather than pixel level. Moreover,\nto further reduce the communication overhead, a reference semantic frame is\nintroduced to substitute motion vectors of each frame in common video coding\nmethods. At the receiver, DDMFC is proposed to generate compensated current\nsemantic frame by a two-stage conditional diffusion process. With both the\nreference frame transmission and DDMFC frame compensation, the bandwidth\nefficiency improves with satisfying video transmission performance.\nExperimental results verify the performance gain of WVSC-D over other DL-based\nmethods e.g. DVSC about 1.8 dB in terms of PSNR.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02233", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02233", "abs": "https://arxiv.org/abs/2511.02233", "authors": ["Songyang Liu", "Yunpeng Tan", "Shuai Li"], "title": "Learning Spatial Awareness for Laparoscopic Surgery with AI Assisted Visual Feedback", "comment": null, "summary": "Laparoscopic surgery constrains surgeons spatial awareness because procedures\nare performed through a monocular, two-dimensional (2D) endoscopic view.\nConventional training methods using dry-lab models or recorded videos provide\nlimited depth cues, often leading trainees to misjudge instrument position and\nperform ineffective or unsafe maneuvers. To address this limitation, we present\nan AI-assisted training framework developed in NVIDIA Isaac Sim that couples\nthe standard 2D laparoscopic feed with synchronized three-dimensional (3D)\nvisual feedback delivered through a mixed-reality (MR) interface. While\ntrainees operate using the clinical 2D view, validated AI modules continuously\nlocalize surgical instruments and detect instrument-tissue interactions in the\nbackground. When spatial misjudgments are detected, 3D visual feedback are\ndisplayed to trainees, while preserving the original operative perspective. Our\nframework considers various surgical tasks including navigation, manipulation,\ntransfer, cutting, and suturing. Visually similar 2D cases can be disambiguated\nthrough the added 3D context, improving depth perception, contact awareness,\nand tool orientation understanding.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02367", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02367", "abs": "https://arxiv.org/abs/2511.02367", "authors": ["Shuning Zhang", "Zhaoxin Li", "Changxi Wen", "Ying Ma", "Simin Li", "Gengrui Zhang", "Ziyi Zhang", "Yibo Meng", "Hantao Zhao", "Xin Yi", "Hewu Li"], "title": "The Pervasive Blind Spot: Benchmarking VLM Inference Risks on Everyday Personal Videos", "comment": null, "summary": "The proliferation of Vision-Language Models (VLMs) introduces profound\nprivacy risks from personal videos. This paper addresses the critical yet\nunexplored inferential privacy threat, the risk of inferring sensitive personal\nattributes over the data. To address this gap, we crowdsourced a dataset of 508\neveryday personal videos from 58 individuals. We then conducted a benchmark\nstudy evaluating VLM inference capabilities against human performance. Our\nfindings reveal three critical insights: (1) VLMs possess superhuman\ninferential capabilities, significantly outperforming human evaluators,\nleveraging a shift from object recognition to behavioral inference from\ntemporal streams. (2) Inferential risk is strongly correlated with factors such\nas video characteristics and prompting strategies. (3) VLM-driven explanation\ntowards the inference is unreliable, as we revealed a disconnect between the\nmodel-generated explanations and evidential impact, identifying ubiquitous\nobjects as misleading confounders.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02370", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02370", "abs": "https://arxiv.org/abs/2511.02370", "authors": ["Adnan Hoq", "Matthew Facciani", "Tim Weninger"], "title": "AI Credibility Signals Outrank Institutions and Engagement in Shaping News Perception on Social Media", "comment": null, "summary": "AI-generated content is rapidly becoming a salient component of online\ninformation ecosystems, yet its influence on public trust and epistemic\njudgments remains poorly understood. We present a large-scale mixed-design\nexperiment (N = 1,000) investigating how AI-generated credibility scores affect\nuser perception of political news. Our results reveal that AI feedback\nsignificantly moderates partisan bias and institutional distrust, surpassing\ntraditional engagement signals such as likes and shares. These findings\ndemonstrate the persuasive power of generative AI and suggest a need for design\nstrategies that balance epistemic influence with user autonomy.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02428", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02428", "abs": "https://arxiv.org/abs/2511.02428", "authors": ["Michelle Bak", "Kexin Quan", "Tre Tomaszewski", "Jessie Chin"], "title": "Can Conversational AI Counsel for Change? A Theory-Driven Approach to Supporting Dietary Intentions in Ambivalent Individuals", "comment": null, "summary": "Adherence to healthy diets reduces chronic illness risk, yet rates remain\nlow. Large Language Models (LLMs) are increasingly used for health\ncommunication but often struggle to engage individuals with ambivalent\nintentions at a pivotal stage of the Transtheoretical Model (TTM). We developed\nCounselLLM, an open-source model enhanced through persona design and few-shot,\ndomain-specific prompts grounded in TTM and Motivational Interviewing (MI). In\ncontrolled evaluations, CounselLLM showed stronger use of TTM subprocesses and\nMI affirmations than human counselors, with comparable linguistic robustness\nbut expressed in more concrete terms. A user study then tested CounselLLM in an\ninteractive counseling setting against a baseline system. While knowledge and\nperceptions did not change, participants' intentions for immediate dietary\nchange increased significantly after interacting with CounselLLM. Participants\nalso rated it as easy to use, understandable, and supportive. These findings\nsuggest theory-driven LLMs can effectively engage ambivalent individuals and\nprovide a scalable approach to digital counseling.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02455", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02455", "abs": "https://arxiv.org/abs/2511.02455", "authors": ["Yuhan Liu", "Varun Nagaraj Rao", "Sohyeon Hwang", "Janet Vertesi", "Andr\u00e9s Monroy-Hern\u00e1ndez"], "title": "OpenCourier: an Open Protocol for Building a Decentralized Ecosystem of Community-owned Delivery Platforms", "comment": null, "summary": "Although the platform gig economy has reshaped the landscape of work, its\ncentralized operation by select actors has brought about challenges that\nimpedes workers' well-being. We present the architecture and design of\nOpenCourier, an open protocol that defines communication patterns within a\ndecentralized ecosystem of delivery platforms. Through this protocol, we aim to\naddress three key challenges in the current economy: power imbalances between\nthe platform and workers, information asymmetries caused by black-boxed\nalgorithms and value misalignments in the infrastructure design process. With\nthe OpenCourier protocol, we outline a blueprint for community-owned ecosystem\nof delivery platforms that centers worker agency, transparency, and bottom-up\ndesign.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02468", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02468", "abs": "https://arxiv.org/abs/2511.02468", "authors": ["Chuhan Jiao", "Zhiming Hu", "Andreas Bulling"], "title": "HAGI++: Head-Assisted Gaze Imputation and Generation", "comment": "Extended version of our UIST'25 paper \"HAGI: Head-Assisted Gaze\n  Imputation for Mobile Eye Trackers\"", "summary": "Mobile eye tracking plays a vital role in capturing human visual attention\nacross both real-world and extended reality (XR) environments, making it an\nessential tool for applications ranging from behavioural research to\nhuman-computer interaction. However, missing values due to blinks, pupil\ndetection errors, or illumination changes pose significant challenges for\nfurther gaze data analysis. To address this challenge, we introduce HAGI++ - a\nmulti-modal diffusion-based approach for gaze data imputation that, for the\nfirst time, uses the integrated head orientation sensors to exploit the\ninherent correlation between head and eye movements. HAGI++ employs a\ntransformer-based diffusion model to learn cross-modal dependencies between eye\nand head representations and can be readily extended to incorporate additional\nbody movements. Extensive evaluations on the large-scale Nymeria, Ego-Exo4D,\nand HOT3D datasets demonstrate that HAGI++ consistently outperforms\nconventional interpolation methods and deep learning-based time-series\nimputation baselines in gaze imputation. Furthermore, statistical analyses\nconfirm that HAGI++ produces gaze velocity distributions that closely match\nactual human gaze behaviour, ensuring more realistic gaze imputations.\nMoreover, by incorporating wrist motion captured from commercial wearable\ndevices, HAGI++ surpasses prior methods that rely on full-body motion capture\nin the extreme case of 100% missing gaze data (pure gaze generation). Our\nmethod paves the way for more complete and accurate eye gaze recordings in\nreal-world settings and has significant potential for enhancing gaze-based\nanalysis and interaction across various application domains.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02515", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02515", "abs": "https://arxiv.org/abs/2511.02515", "authors": ["Obada Kraishan"], "title": "Emotional Contagion in Code: How GitHub Emoji Reactions Shape Developer Collaboration", "comment": "12 pages, 3 figures. Analysis of 106,743 emoji reactions across 2,098\n  GitHub items", "summary": "Developer communities increasingly rely on emoji reactions to communicate,\nbut we know little about how these emotional signals spread and influence\ntechnical discussions. We analyzed 2,098 GitHub issues and pull requests across\n50 popular repositories, examining patterns in 106,743 emoji reactions to\nunderstand emotional contagion in software development. Our findings reveal a\nsurprisingly positive emotional landscape: 57.4% of discussions carry positive\nsentiment, with positive emotional cascades outnumbering negative ones 23:1. We\nidentified five distinct patterns, with \"instant enthusiasm\" affecting 45.6% of\nitems--nearly half receive immediate positive reinforcement. Statistical\nanalysis confirms strong emotional contagion (r=0.679, p<0.001) with a massive\neffect size (d=2.393), suggesting that initial reactions powerfully shape\ndiscussion trajectories. These findings challenge assumptions about technical\ndiscourse being purely rational, demonstrating that even minimal emotional\nsignals create measurable ripple effects. Our work provides empirical evidence\nthat emoji reactions are not mere decoration but active forces shaping\ncollaborative outcomes in software development.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02560", "categories": ["cs.HC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02560", "abs": "https://arxiv.org/abs/2511.02560", "authors": ["Dan Bohus", "Sean Andrist", "Ann Paradiso", "Nick Saw", "Tim Schoonbeek", "Maia Stiber"], "title": "SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration", "comment": null, "summary": "We introduce SigmaCollab, a dataset enabling research on physically situated\nhuman-AI collaboration. The dataset consists of a set of 85 sessions in which\nuntrained participants were guided by a mixed-reality assistive AI agent in\nperforming procedural tasks in the physical world. SigmaCollab includes a set\nof rich, multimodal data streams, such as the participant and system audio,\negocentric camera views from the head-mounted device, depth maps, head, hand\nand gaze tracking information, as well as additional annotations performed\npost-hoc. While the dataset is relatively small in size (~ 14 hours), its\napplication-driven and interactive nature brings to the fore novel research\nchallenges for human-AI collaboration, and provides more realistic testing\ngrounds for various AI models operating in this space. In future work, we plan\nto use the dataset to construct a set of benchmarks for physically situated\ncollaboration in mixed-reality task assistive scenarios. SigmaCollab is\navailable at https://github.com/microsoft/SigmaCollab.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02694", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02694", "abs": "https://arxiv.org/abs/2511.02694", "authors": ["Siqi Zhang", "Mayank Goel", "Justin Chan"], "title": "DropleX: Liquid sensing on tablet touchscreens", "comment": null, "summary": "We present DropleX, the first system that enables liquid sensing using the\ncapacitive touchscreen of commodity tablets. DropleX detects microliter-scale\nliquid samples, and performs non-invasive, through-container measurements to\ndetect whether a drink has been spiked or if a sealed liquid has been\ncontaminated. These capabilities are made possible by a physics-informed\nmechanism that disables the touchscreen's built-in adaptive filters, originally\ndesigned to reject the effects of liquid drops such as rain, without any\nhardware modifications. We model the touchscreen's sensing capabilities,\nlimits, and non-idealities to inform the design of a signal processing and\nlearning-based pipeline for liquid sensing. Our system achieves 96-99% accuracy\nin detecting microliter-scale adulteration in soda, wine, and milk, 93-96%\naccuracy in threshold detection of trace chemical concentrations, and 86-96%\naccuracy in through-container adulterant detection. Given the predominance of\ntouchscreens, these exploratory results can open new opportunities for liquid\nsensing on everyday devices.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02807", "categories": ["cs.HC", "H.5.1; I.2.6; I.2.11"], "pdf": "https://arxiv.org/pdf/2511.02807", "abs": "https://arxiv.org/abs/2511.02807", "authors": ["You-Jin Kim", "Misha Sra", "Tobias H\u00f6llerer"], "title": "Audience Amplified: Virtual Audiences in Asynchronously Performed AR Theater", "comment": "Conference Paper, 10 pages. Published at the 2024 IEEE International\n  Symposium on Mixed and Augmented Reality (ISMAR)", "summary": "Audience reactions can considerably enhance live experiences; conversely, in\nanytime-anywhere augmented reality (AR) experiences, large crowds of people\nmight not always be available to congregate. To get closer to simulating live\nevents with large audiences, we created a mobile AR experience where users can\nwander around naturally and engage in AR theater with virtual audiences trained\nfrom real audiences using imitation learning. This allows us to carefully\ncapture the essence of human imperfections and behavior in artificial\nintelligence (AI) audiences. The result is a novel mobile AR experience in\nwhich solitary AR users experience an augmented performance in a physical space\nwith a virtual audience. Virtual dancers emerge from the surroundings,\naccompanied by a digitally simulated audience, to provide a community\nexperience akin to immersive theater. In a pilot study, simulated human avatars\nwere vastly preferred over just audience audio commentary. We subsequently\nengaged 20 participants as attendees of an AR dance performance, comparing a\nno-audience condition with a simulated audience of six onlookers. Through\nquestionnaires and experience reports, we investigated user reactions and\nbehavior. Our results demonstrate that the presence of virtual audience members\ncaused attendees to perceive the performance as a social experience with\nincreased interest and involvement in the event. On the other hand, for some\nattendees, the dance performances without the virtual audience evoked a\nstronger positive sentiment.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
