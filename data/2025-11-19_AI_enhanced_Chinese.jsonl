{"id": "2511.13910", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13910", "abs": "https://arxiv.org/abs/2511.13910", "authors": ["Parul Khanna", "Sameer Prabhu", "Ramin Karim", "Phillip Tretten"], "title": "Enhancing Decision Support in Construction through Industrial AI", "comment": null, "summary": "The construction industry is presently going through a transformation led by adopting digital technologies that leverage Artificial Intelligence (AI). These industrial AI solutions assist in various phases of the construction process, including planning, design, production and management. In particular, the production phase offers unique potential for the integration of such AI-based solutions. These AI-based solutions assist site managers, project engineers, coordinators and other key roles in making final decisions. To facilitate the decision-making process in the production phase of construction through a human-centric AI-based solution, it is important to understand the needs and challenges faced by the end users who interact with these AI-based solutions to enhance the effectiveness and usability of these systems. Without this understanding, the potential usage of these AI-based solutions may be limited. Hence, the purpose of this research study is to explore, identify and describe the key factors crucial for developing AI solutions in the construction industry. This study further identifies the correlation between these key factors. This was done by developing a demonstrator and collecting quantifiable feedback through a questionnaire targeting the end users, such as site managers and construction professionals. This research study will offer insights into developing and improving these industrial AI solutions, focusing on Human-System Interaction aspects to enhance decision support, usability, and overall AI solution adoption.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.13918", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13918", "abs": "https://arxiv.org/abs/2511.13918", "authors": ["Parul Khanna", "Ravdeep Kour", "Ramin Karim"], "title": "Human-centric Maintenance Process Through Integration of AI, Speech, and AR", "comment": null, "summary": "The adoption of Augmented Reality (AR) is increasing to enhance Human-System Interaction (HSI) by creating immersive experiences that improve efficiency and safety in various industries. In industrial maintenance, traditional practices involve physical documentation and device interactions, which might disrupt the task, affect efficiency, and increase the cognitive load for the maintenance personnel. AR has the potential to support and enhance industrial maintenance processes in these aspects. Therefore, the purpose of this research is to study and explore how advanced technologies like Artificial Intelligence (AI), AR and speech processing can be integrated to support hands-free, real-time task logging and interaction in maintenance environments. This is done by developing a demonstrator for Microsoft HoloLens 2 using Unity, C#, Azure Cognitive Services, and Azure Functions, which enables speech-to-text transcription for hands-free maintenance support. Using Azures' speech recognition, the demonstrator can achieve high transcription accuracy in an AR environment, facilitating natural interactions between users and the augmented environment. The study aims to explore the potential of AR to reduce cognitive load, streamline workflows, and improve safety by enhancing HSI for maintenance personnel in high-stakes environments.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.13979", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13979", "abs": "https://arxiv.org/abs/2511.13979", "authors": ["Harang Ju", "Sinan Aral"], "title": "Personality Pairing Improves Human-AI Collaboration", "comment": "28 pages, 7 figures", "summary": "Here we ask how AI agent \"personalities\" interact with human personalities, and other traits, to shape human-AI collaboration, productivity and performance. To estimate these relationships, we conducted a large-scale preregistered randomized experiment that paired 1,258 participants with AI agents that were prompted to exhibit varying levels of the Big Five personality traits. These human-AI teams produced 7,266 display ads for a real think tank, and the quality of these ads was evaluated by 1,995 independent human raters as well as in a field experiment conducted on X, which generated nearly 5 million impressions. We found, first, that personality pairing impacted teamwork quality. For example, neurotic AI improved teamwork for agreeable humans but impaired it for conscientious humans. Second, we found productivity effects of personality pairing and a \"productivity-performance trade-off\" in which certain pairings (e.g., agreeable human with neurotic AI) produced fewer ads but of higher quality. Third, personality pairing influenced ad quality and performance. For example, quality improved when open humans were paired with conscientious AI and when conscientious humans were paired with disagreeable AI. Some of these pairing effects were \"jagged\" in that they varied across text and visual tasks. For example open humans produced higher quality images but lower quality text when paired with agreeable AI. Pairing effects were also present in other human traits, like country of origin. For example, extroverted AI improved quality for Latin American workers, but degraded quality for East Asian workers. These findings demonstrate that human-AI personality alignment significantly improves collaboration, productivity, and performance and lay a foundation for future research on improving human-AI collaboration through AI personalization.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14296", "categories": ["cs.ET", "cs.DM", "physics.app-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.14296", "abs": "https://arxiv.org/abs/2511.14296", "authors": ["Chinonso Onah", "Roman Firt", "Kristel Michielsen"], "title": "Empirical Quantum Advantage in Constrained Optimization from Encoded Unitary Designs", "comment": "32 Pages, 5 figures, 2 tables", "summary": "We introduce the Constraint-Enhanced Quantum Approximate Optimization Algorithm (CE-QAOA), a shallow, constraint-aware ansatz that operates inside the one-hot product space of size [n]^m, where m is the number of blocks and each block is initialized with an n-qubit W_n state. We give an ancilla-free, depth-optimal encoder that prepares a W_n state using n-1 two-qubit rotations per block, and a two-local XY mixer restricted to the same block of n qubits with a constant spectral gap. Algorithmically, we wrap constant-depth sampling with a deterministic classical checker to obtain a polynomial-time hybrid quantum-classical solver (PHQC) that returns the best observed feasible solution in O(S n^2) time, where S is the number of shots. We obtain two advantages. First, when CE-QAOA fixes r >= 1 locations different from the start city, we achieve a Theta(n^r) reduction in shot complexity even against a classical sampler that draws uniformly from the feasible set. Second, against a classical baseline restricted to raw bitstring sampling, we show an exp(Theta(n^2)) separation in the minimax sense. In noiseless circuit simulations of TSP instances ranging from 4 to 10 locations from the QOPTLib benchmark library, we recover the global optimum at depth p = 1 using polynomial shot budgets and coarse parameter grids defined by the problem sizes.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14009", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14009", "abs": "https://arxiv.org/abs/2511.14009", "authors": ["Halle C. Braun", "Kushin Mukherjee", "Seth R. Gorelik", "Karen B. Schloss"], "title": "Affective Color Scales for Colormap Data Visualizations", "comment": "To be published in IEEE Transactions on Visualization and Computer Graphics", "summary": "Research on affective visualization design has shown that color is an especially powerful feature for influencing the emotional connotation of visualizations. Associations between colors and emotions are largely driven by lightness (e.g., lighter colors are associated with positive emotions, whereas darker colors are associated with negative emotions). Designing visualizations to have all light or all dark colors to convey particular emotions may work well for visualizations in which colors represent categories and spatial channels encode data values. However, this approach poses a problem for visualizations that use color to represent spatial patterns in data (e.g., colormap data visualizations) because lightness contrast is needed to reveal fine details in spatial structure. In this study, we found it is possible to design colormaps that have strong lightness contrast to support spatial vision while communicating clear affective connotation. We also found that affective connotation depended not only on the color scales used to construct the colormaps, but also the frequency with which colors appeared in the map, as determined by the underlying dataset (data-dependence hypothesis). These results emphasize the importance of data-aware design, which accounts for not only the design features that encode data (e.g., colors, shapes, textures), but also how those design features are instantiated in a visualization, given the properties of the data.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14400", "categories": ["cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.14400", "abs": "https://arxiv.org/abs/2511.14400", "authors": ["I-Ting Lee", "Bao-Kai Wang", "Liang-Chi Chen", "Wen Sheng Lim", "Da-Wei Chang", "Yu-Ming Chang", "Chieng-Chung Ho"], "title": "PIM or CXL-PIM? Understanding Architectural Trade-offs Through Large-Scale Benchmarking", "comment": null, "summary": "Processing-in-memory (PIM) reduces data movement by executing near memory, but our large-scale characterization on real PIM hardware shows that end-to-end performance is often limited by disjoint host and device address spaces that force explicit staging transfers. In contrast, CXL-PIM provides a unified address space and cache-coherent access at the cost of higher access latency. These opposing interface models create workload-dependent tradeoffs that are not captured by small-scale studies. This work presents a side-by-side, large-scale comparison of PIM and CXL-PIM using measurements from real PIM hardware and trace-driven CXL modeling. We identify when unified-address access amortizes link latency enough to overcome transfer bottlenecks, and when tightly coupled PIM remains preferable. Our results reveal phase- and dataset-size regimes in which the relative ranking between the two architectures reverses, offering practical guidance for future near-memory system design.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.13772", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13772", "abs": "https://arxiv.org/abs/2511.13772", "authors": ["Lyra Hoeben-Kuil", "Gijs van Dijck", "Jaromir Savelka", "Johanna Gunawan", "Konrad Kollnig", "Marta Kolacz", "Mindy Duffourc", "Shashank Chakravarthy", "Hannes Westermann"], "title": "Can LLMs Create Legally Relevant Summaries and Analyses of Videos?", "comment": "Accepted for publication at JURIX 2025 Torino, Italy. This is the preprint version. Code and data available at: https://github.com/maastrichtlawtech/jurix2025_LLM_video_analysis", "summary": "Understanding the legally relevant factual basis of an event and conveying it through text is a key skill of legal professionals. This skill is important for preparing forms (e.g., insurance claims) or other legal documents (e.g., court claims), but often presents a challenge for laypeople. Current AI approaches aim to bridge this gap, but mostly rely on the user to articulate what has happened in text, which may be challenging for many. Here, we investigate the capability of large language models (LLMs) to understand and summarize events occurring in videos. We ask an LLM to summarize and draft legal letters, based on 120 YouTube videos showing legal issues in various domains. Overall, 71.7\\% of the summaries were rated as of high or medium quality, which is a promising result, opening the door to a number of applications in e.g. access to justice.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14013", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14013", "abs": "https://arxiv.org/abs/2511.14013", "authors": ["Bifei Mao", "Lanqing Hong"], "title": "Developing a Grounded View of AI", "comment": null, "summary": "As a capability coming from computation, how does AI differ fundamentally from the capabilities delivered by rule-based software program? The paper examines the behavior of artificial intelligence (AI) from engineering points of view to clarify its nature and limits. The paper argues that the rationality underlying humanity's impulse to pursue, articulate, and adhere to rules deserves to be valued and preserved. Identifying where rule-based practical rationality ends is the beginning of making it aware until action. Although the rules of AI behaviors are still hidden or only weakly observable, the paper has proposed a methodology to make a sense of discrimination possible and practical to identify the distinctions of the behavior of AI models with three types of decisions. It is a prerequisite for human responsibilities with alternative possibilities, considering how and when to use AI. It would be a solid start for people to ensure AI system soundness for the well-being of humans, society, and the environment.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14119", "categories": ["cs.MM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14119", "abs": "https://arxiv.org/abs/2511.14119", "authors": ["Liuyi Jin", "Amran Haroon", "Radu Stoleru", "Pasan Gunawardena", "Michael Middleton", "Jeeeun Kim"], "title": "Real-Time Mobile Video Analytics for Pre-arrival Emergency Medical Services", "comment": null, "summary": "Timely and accurate pre-arrival video streaming and analytics are critical for emergency medical services (EMS) to deliver life-saving interventions. Yet, current-generation EMS infrastructure remains constrained by one-to-one video streaming and limited analytics capabilities, leaving dispatchers and EMTs to manually interpret overwhelming, often noisy or redundant information in high-stress environments. We present TeleEMS, a mobile live video analytics system that enables pre-arrival multimodal inference by fusing audio and video into a unified decision-making pipeline before EMTs arrive on scene.\n  TeleEMS comprises two key components: TeleEMS Client and TeleEMS Server. The TeleEMS Client runs across phones, smart glasses, and desktops to support bystanders, EMTs en route, and 911 dispatchers. The TeleEMS Server, deployed at the edge, integrates EMS-Stream, a communication backbone that enables smooth multi-party video streaming. On top of EMSStream, the server hosts three real-time analytics modules: (1) audio-to-symptom analytics via EMSLlama, a domain-specialized LLM for robust symptom extraction and normalization; (2) video-to-vital analytics using state-of-the-art rPPG methods for heart rate estimation; and (3) joint text-vital analytics via PreNet, a multimodal multitask model predicting EMS protocols, medication types, medication quantities, and procedures.\n  Evaluation shows that EMSLlama outperforms GPT-4o (exact-match 0.89 vs. 0.57) and that text-vital fusion improves inference robustness, enabling reliable pre-arrival intervention recommendations. TeleEMS demonstrates the potential of mobile live video analytics to transform EMS operations, bridging the gap between bystanders, dispatchers, and EMTs, and paving the way for next-generation intelligent EMS infrastructure.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14205", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2511.14205", "abs": "https://arxiv.org/abs/2511.14205", "authors": ["Minkwan Kim", "Yoonsang Lee"], "title": "FreeMusco: Motion-Free Learning of Latent Control for Morphology-Adaptive Locomotion in Musculoskeletal Characters", "comment": "SIGGRAPH Asia 2025", "summary": "We propose FreeMusco, a motion-free framework that jointly learns latent representations and control policies for musculoskeletal characters. By leveraging the musculoskeletal model as a strong prior, our method enables energy-aware and morphology-adaptive locomotion to emerge without motion data. The framework generalizes across human, non-human, and synthetic morphologies, where distinct energy-efficient strategies naturally appear--for example, quadrupedal gaits in Chimanoid versus bipedal gaits in Humanoid. The latent space and corresponding control policy are constructed from scratch, without demonstration, and enable downstream tasks such as goal navigation and path following--representing, to our knowledge, the first motion-free method to provide such capabilities. FreeMusco learns diverse and physically plausible locomotion behaviors through model-based reinforcement learning, guided by the locomotion objective that combines control, balancing, and biomechanical terms. To better capture the periodic structure of natural gait, we introduce the temporally averaged loss formulation, which compares simulated and target states over a time window rather than on a per-frame basis. We further encourage behavioral diversity by randomizing target poses and energy levels during training, enabling locomotion to be flexibly modulated in both form and intensity at runtime. Together, these results demonstrate that versatile and adaptive locomotion control can emerge without motion capture, offering a new direction for simulating movement in characters where data collection is impractical or impossible.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14118", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14118", "abs": "https://arxiv.org/abs/2511.14118", "authors": ["Md Mosharaf Hossan", "Rifat Ara Tasnim", "Farjana Z Eishita"], "title": "Gamified Virtual Reality Exposure Therapy for Mysophobia: Evaluating the Efficacy of a Simulated Sneeze Intervention", "comment": null, "summary": "Mysophobia, or the fear of germs, is a prevalent anxiety disorder that significantly impacts daily life. This study investigates the potential of a gamified virtual reality (VR) intervention to simulate contamination-related scenarios and assess their emotional and psychological effects. A VR game based sneeze simulation was developed to evaluate its influence on participants' emotional states. Seven participants completed two versions of the game: a baseline version and an experimental version featuring the sneeze simulation. Emotional responses were measured using the Positive and Negative Affect Schedule (PANAS) and State-Trait Anxiety Inventory - State (STAI-S) questionnaires. The results revealed slight increases in negative affect and anxiety levels during the sneeze simulation. Also, a reduction in positive affect was revealed. However, these differences were not statistically significant (p > 0.05). This is likely due to small sample sizes, a lack of grossness in the simulation, or participants not being clinically mysophobes. This exploratory study highlights the potential of VR-based interventions for understanding and addressing contamination-related anxieties. It provides a foundation for future research with larger and more diverse participant pools.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14196", "categories": ["cs.MM", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14196", "abs": "https://arxiv.org/abs/2511.14196", "authors": ["Xuan-Hao Liu", "Yan-Kai Liu", "Tianyi Zhou", "Bao-Liang Lu", "Wei-Long Zheng"], "title": "MindCross: Fast New Subject Adaptation with Limited Data for Cross-subject Video Reconstruction from Brain Signals", "comment": "AAAI 2026, 16 pages", "summary": "Reconstructing video from brain signals is an important brain decoding task. Existing brain decoding frameworks are primarily built on a subject-dependent paradigm, which requires large amounts of brain data for each subject. However, the expensive cost of collecting brain-video data causes severe data scarcity. Although some cross-subject methods being introduced, they often overfocus with subject-invariant information while neglecting subject-specific information, resulting in slow fine-tune-based adaptation strategy. To achieve fast and data-efficient new subject adaptation, we propose MindCross, a novel cross-subject framework. MindCross's N specific encoders and one shared encoder are designed to extract subject-specific and subject-invariant information, respectively. Additionally, a Top-K collaboration module is adopted to enhance new subject decoding with the knowledge learned from previous subjects' encoders. Extensive experiments on fMRI/EEG-to-video benchmarks demonstrate MindCross's efficacy and efficiency of cross-subject decoding and new subject adaptation using only one model.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14164", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14164", "abs": "https://arxiv.org/abs/2511.14164", "authors": ["Yibo Meng", "Xiaolan Ding", "Lyumanshan Ye", "Zhiming Liu", "Yan Guan"], "title": "Final Happiness: What Intelligent User Interfaces Can Do for the lonely Dying", "comment": null, "summary": "This study explores the design of Intelligent User Interfaces (IUIs) to address the profound existential loneliness of terminally ill individuals. While Human-Computer Interaction (HCI) has made inroads in \"Thanatechnology,\" current research often focuses on practical aspects like digital legacy management, overlooking the subjective, existential needs of those facing death in isolation. To address this gap, we conducted in-depth qualitative interviews with 14 lonely, terminally ill individuals. Our core contributions are: (1) An empirically-grounded model articulating the complex psychological, practical, social, and spiritual needs of this group; (2) The \"Three Pillars, Twelve Principles\" framework for designing IUIs as \"Existential Companions\"; and (3) A critical design directive derived from user evaluations: technology in this context should aim for transcendence over simulation. The findings suggest that IUIs should create experiences that augment or surpass human capabilities, rather than attempting to simulate basic human connections, which can paradoxically deepen loneliness. This research provides a clear, user-centered path for designing technology that serves not as a \"tool for dying,\" but as a \"partner for living fully until the end\".", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14231", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14231", "abs": "https://arxiv.org/abs/2511.14231", "authors": ["Huram Konjen"], "title": "Algorithmic Management and the Future of Human Work: Implications for Autonomy, Collaboration, and Innovation", "comment": "13 pages, 1 figure, 2 tables. Conceptual integration paper", "summary": "This study examines the evolving impact of algorithmic management on human resource management (HRM) practices, with a focus on employee autonomy, procedural transparency, and the sociotechnical dynamics of performance evaluation. Rather than adopting a qualitative or empirical approach, the paper develops a conceptual integration of insights from HRM, human-computer interaction (HCI), and Science and Technology Studies. The analysis highlights that although algorithmic systems can enhance operational efficiency, they risk reinforcing biases and narrowing the relational and contextual dimensions of work. These systems often overlook intangible contributions such as creativity, empathy, and collaborative problem solving, revealing gaps in data-driven performance measurement. In response, the study proposes a sociotechnical perspective on algorithmic accountability that emphasizes procedural transparency, organizational justice, and employee agency. By revisiting foundational questions within the rapidly evolving landscape of algorithmic management, the paper contributes to ongoing debates about the future of work and the design of managerial technologies that support, rather than constrain, human autonomy and organizational life.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14233", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14233", "abs": "https://arxiv.org/abs/2511.14233", "authors": ["Wei Xiang", "Ziyue Lei", "Jie Wang", "Yingying Huang", "Qi Zheng", "Tianyi Zhang", "An Zhao", "Lingyun Sun"], "title": "Visionary Co-Driver: Enhancing Driver Perception of Potential Risks with LLM and HUD", "comment": "Accepted for publication in IEEE Transactions on Intelligent Transportation Systems (T-ITS)", "summary": "Drivers' perception of risky situations has always been a challenge in driving. Existing risk-detection methods excel at identifying collisions but face challenges in assessing the behavior of road users in non-collision situations. This paper introduces Visionary Co-Driver, a system that leverages large language models to identify non-collision roadside risks and alert drivers based on their eye movements. Specifically, the system combines video processing algorithms and LLMs to identify potentially risky road users. These risks are dynamically indicated on an adaptive heads-up display interface to enhance drivers' attention. A user study with 41 drivers confirms that Visionary Co-Driver improves drivers' risk perception and supports their recognition of roadside risks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14242", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14242", "abs": "https://arxiv.org/abs/2511.14242", "authors": ["Yuan Li", "Xinyue Gui", "Ding Xia", "Mark Colley", "Takeo Igarashi"], "title": "TailCue: Exploring Animal-inspired Robotic Tail for Automated Vehicles Interaction", "comment": null, "summary": "Automated vehicles (AVs) are gradually becoming part of our daily lives. However, effective communication between road users and AVs remains a significant challenge. Although various external human-machine interfaces (eHMIs) have been developed to facilitate interactions, psychological factors, such as a lack of trust and inadequate emotional signaling, may still deter users from confidently engaging with AVs in certain contexts. To address this gap, we propose TailCue, an exploration of how tail-based eHMIs affect user interaction with AVs. We first investigated mappings between tail movements and emotional expressions from robotics and zoology, and accordingly developed a motion-emotion mapping scheme. A physical robotic tail was implemented, and specific tail motions were designed based on our scheme. An online, video-based user study with 21 participants was conducted. Our findings suggest that, although the intended emotions conveyed by the tail were not consistently recognized, open-ended feedback indicated that the tail motion needs to align with the scenarios and cues. Our result highlights the necessity of scenario-specific optimization to enhance tail-based eHMIs. Future work will refine tail movement strategies to maximize their effectiveness across diverse interaction contexts.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14359", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14359", "abs": "https://arxiv.org/abs/2511.14359", "authors": ["Sebastian Lubos", "Alexander Felfernig", "Damian Garber", "Viet-Man Le", "Thi Ngoc Trang Tran"], "title": "Towards LLM-Based Usability Analysis for Recommender User Interfaces", "comment": "The paper was presented at IntRS'25: Joint Workshop on Interfaces and Human Decision Making for Recommender Systems, September 22, 2025, Prague, Czech Republic and is published in the workshop proceedings: https://ceur-ws.org/Vol-4027/", "summary": "Usability is a key factor in the effectiveness of recommender systems. However, the analysis of user interfaces is a time-consuming process that requires expertise. Recent advances in multimodal large language models (LLMs) offer promising opportunities to automate such evaluations. In this work, we explore the potential of multimodal LLMs to assess the usability of recommender system interfaces by considering a variety of publicly available systems as examples. We take user interface screenshots from multiple of these recommender platforms to cover both preference elicitation and recommendation presentation scenarios. An LLM is instructed to analyze these interfaces with regard to different usability criteria and provide explanatory feedback. Our evaluation demonstrates how LLMs can support heuristic-style usability assessments at scale to support the improvement of user experience.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14414", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14414", "abs": "https://arxiv.org/abs/2511.14414", "authors": ["Yu Mei", "Xutong Wang", "Ziyao Zhang", "Yiming Fu", "Shiyi Wang", "Qingyang Wan", "Qinghuan Lan", "Chang Liu", "Jie Cai", "Chun Yu", "Yuanchun Shi"], "title": "PACEE: Supporting Children's Personal Emotion Education through Parent-AI Collaboration", "comment": null, "summary": "Emotion education is a crucial lesson for children aged 3 to 6. However, existing technologies primarily focus on promoting emotion education from the child's perspective, often neglecting the central role of parents in guiding early childhood emotion development. In this work, we conducted co-design sessions with five experienced kindergarten teachers and five parents to identify parental challenges and the roles that AI can play in family emotion education. Guided by these insights, we developed PACEE, an assistant for supporting parent-AI collaborative emotion education. PACEE enables parents to engage in emotional dialogues about common scenarios, with multiple forms of support provided by generative AI. It combines insights from parents and AI to model children's emotional states and collaboratively delivers personalized, parent-mediated guidance. In a user study involving 16 families, we found that PACEE significantly enhances parent-child engagement, encourages more in-depth emotional communication, and improves the parental experience. Our findings advance emotion coaching theory in both family settings and LLM-assisted contexts, offering valuable insights for designing AI-supported, parent-centered family education systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14437", "categories": ["cs.HC", "cs.FL"], "pdf": "https://arxiv.org/pdf/2511.14437", "abs": "https://arxiv.org/abs/2511.14437", "authors": ["Mehrnoush Hajnorouzi", "Astrid Rakow", "Martin Fr\u00e4nzle"], "title": "Model Learning for Adjusting the Level of Automation in HCPS", "comment": "In Proceedings FMAS 2025, arXiv:2511.13245", "summary": "The steadily increasing level of automation in human-centred systems demands rigorous design methods for analysing and controlling interactions between humans and automated components, especially in safety-critical applications. The variability of human behaviour poses particular challenges for formal verification and synthesis. We present a model-based framework that enables design-time exploration of safe shared-control strategies in human-automation systems. The approach combines active automata learning -- to derive coarse, finite-state abstractions of human behaviour from simulations -- with game-theoretic reactive synthesis to determine whether a controller can guarantee safety when interacting with these models. If no such strategy exists, the framework supports iterative refinement of the human model or adjustment of the automation's controllable actions. A driving case study, integrating automata learning with reactive synthesis in UPPAAL, illustrates the applicability of the framework on a simplified driving scenario and its potential for analysing shared-control strategies in human-centred cyber-physical systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14567", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14567", "abs": "https://arxiv.org/abs/2511.14567", "authors": ["Chen Chen", "Cuong Nguyen", "Alexa Siu", "Dingzeyu Li", "Nadir Weibel"], "title": "SweeperBot: Making 3D Browsing Accessible through View Analysis and Visual Question Answering", "comment": "28 pages, 16 figures, this article has been accepted for publication in the International Journal of Human-Computer Interaction (IJHCI), published by Taylor and Francis", "summary": "Accessing 3D models remains challenging for Screen Reader (SR) users. While some existing 3D viewers allow creators to provide alternative text, they often lack sufficient detail about the 3D models. Grounded on a formative study, this paper introduces SweeperBot, a system that enables SR users to leverage visual question answering to explore and compare 3D models. SweeperBot answers SR users' visual questions by combining an optimal view selection technique with the strength of generative- and recognition-based foundation models. An expert review with 10 Blind and Low-Vision (BLV) users with SR experience demonstrated the feasibility of using SweeperBot to assist BLV users in exploring and comparing 3D models. The quality of the descriptions generated by SweeperBot was validated by a second survey study with 30 sighted participants.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14591", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14591", "abs": "https://arxiv.org/abs/2511.14591", "authors": ["Nick von Felten", "Johannes Sch\u00f6ning", "Klaus Opwis", "Nicolas Scharowksi"], "title": "Biased Minds Meet Biased AI: How Class Imbalance Shapes Appropriate Reliance and Interacts with Human Base Rate Neglect", "comment": null, "summary": "Humans increasingly interact with artificial intelligence (AI) in decision-making. However, both AI and humans are prone to biases. While AI and human biases have been studied extensively in isolation, this paper examines their complex interaction. Specifically, we examined how class imbalance as an AI bias affects people's ability to appropriately rely on an AI-based decision-support system, and how it interacts with base rate neglect as a human bias. In a within-subject online study (N= 46), participants classified three diseases using an AI-based decision-support system trained on either a balanced or unbalanced dataset. We found that class imbalance disrupted participants' calibration of AI reliance. Moreover, we observed mutually reinforcing effects between class imbalance and base rate neglect, offering evidence of a compound human-AI bias. Based on these findings, we advocate for an interactionist perspective and further research into the mutually reinforcing effects of biases in human-AI interaction.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.14636", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14636", "abs": "https://arxiv.org/abs/2511.14636", "authors": ["Nyah Speicher", "Prashant Chandrasekar"], "title": "Theoretical basis for code presentation: A case for cognitive load", "comment": "10 pages, 1 figure", "summary": "Evidence supports that reducing cognitive load (CL) improves task performance for people of all abilities. This effect is specifically important for blind-and-low-vision (BLV) individuals because they cannot rely on many common methods of managing CL, which are frequently vision-based techniques. Current accessible \"solutions\" for BLV developers only sporadically consider CL in their design. There isn't a way to know whether CL is being alleviated by them. Neither do we know if alleviating CL is part of the mechanism behind why these solutions help BLV people. Using a strong foundation in psychological sciences, we identify aspects of CL that impact performance and learning in programming. These aspects are then examined when evaluating existing solutions for programming sub-tasks for BLV users. We propose an initial design \"recommendations\" for presentation of code which, when followed, will reduce cognitive load for BLV developers.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
