{"id": "2601.21001", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21001", "abs": "https://arxiv.org/abs/2601.21001", "authors": ["Ron Fulbright"], "title": "Designing the Interactive Memory Archive (IMA): A Socio-Technical Framework for AI-Mediated Reminiscence and Cultural Memory Preservation", "comment": "10 pages, 2 figures, 45 references cited", "summary": "This paper introduces the Interactive Memory Archive (IMA), a conceptual framework for AI-mediated reminiscence designed to support cognitive en-gagement among older adults experiencing memory loss. IMA integrates multimodal sensing, natural language conversational scaffolding, and cloud-based archiving within the familiar form of a large format historical picture book. The model theorizes reminiscence as a guided, context-aware interaction eliciting autobiographical memories and preserving them as cul-tural artifacts. The paper positions IMA as a theoretical contribution, articu-lates testable propositions, and outlines a research agenda for future empiri-cal, technical, and ethical inquiry."}
{"id": "2601.21043", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21043", "abs": "https://arxiv.org/abs/2601.21043", "authors": ["Michał Patryk Miazga", "Hannah Bussmann", "Antti Oulasvirta", "Patrick Ebel"], "title": "Log2Motion: Biomechanical Motion Synthesis from Touch Logs", "comment": null, "summary": "Touch data from mobile devices are collected at scale but reveal little about the interactions that produce them. While biomechanical simulations can illuminate motor control processes, they have not yet been developed for touch interactions. To close this gap, we propose a novel computational problem: synthesizing plausible motion directly from logs. Our key insight is a reinforcement learning-driven musculoskeletal forward simulation that generates biomechanically plausible motion sequences consistent with events recorded in touch logs. We achieve this by integrating a software emulator into a physics simulator, allowing biomechanical models to manipulate real applications in real-time. Log2Motion produces rich syntheses of user movements from touch logs, including estimates of motion, speed, accuracy, and effort. We assess the plausibility of generated movements by comparing against human data from a motion capture study and prior findings, and demonstrate Log2Motion in a large-scale dataset. Biomechanical motion synthesis provides a new way to understand log data, illuminating the ergonomics and motor control underlying touch interactions."}
{"id": "2601.21045", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21045", "abs": "https://arxiv.org/abs/2601.21045", "authors": ["Kamrul Hasan", "Oleg V. Komogortsev"], "title": "Eye Feel You: A DenseNet-driven User State Prediction Approach", "comment": "10 pages, 1 figure, 4 tables", "summary": "Subjective self-reports, collected with eye-tracking data, reveal perceived states like fatigue, effort, and task difficulty. However, these reports are costly to collect and challenging to interpret consistently in longitudinal studies. In this work, we focus on determining whether objective gaze dynamics can reliably predict subjective reports across repeated recording rounds in the eye-tracking dataset. We formulate subjective-report prediction as a supervised regression problem and propose a DenseNet-based deep learning regressor that learns predictive representations from gaze velocity signals. We conduct two complementary experiments to clarify our aims. First, the cross-round generalization experiment tests whether models trained on earlier rounds transfer to later rounds, evaluating the models' ability to capture longitudinal changes. Second, cross-subject generalization tests models' robustness by predicting subjective outcomes for new individuals. These experiments aim to reduce reliance on hand-crafted feature designs and clarify which states of subjective experience systematically appear in oculomotor behavior over time."}
{"id": "2601.21057", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21057", "abs": "https://arxiv.org/abs/2601.21057", "authors": ["Kamrul Hasan", "Oleg V. Komogortsev"], "title": "Privatization of Synthetic Gaze: Attenuating State Signatures in Diffusion-Generated Eye Movements", "comment": "9 pages, 4 figures", "summary": "The recent success of deep learning (DL) has enabled the generation of high-quality synthetic gaze data. However, such data also raises privacy concerns because gaze sequences can encode subjects' internal states, like fatigue, emotional load, or stress. Ideally, synthetic gaze should preserve the signal quality of real recordings and remove or attenuate state-related, privacy-sensitive attributes. Many recent DL-based generative models focus on replicating real gaze trajectories and do not explicitly consider subjective reports or the privatization of internal states. However, in this work, we consider a recent diffusion-based gaze synthesis approach and examine correlations between synthetic gaze features and subjective reports (e.g., fatigue and related self-reported states). Our result shows that these correlations are trivial, which suggests the generative approach suppresses state-related features. Moreover, synthetic gaze preserves necessary signal characteristics similar to those of real data, which supports its use for privacy-preserving gaze-based applications."}
{"id": "2601.21400", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2601.21400", "abs": "https://arxiv.org/abs/2601.21400", "authors": ["Ruiqi Zhang", "Jiacheng Wu", "Jie Chen"], "title": "Mesh Splatting for End-to-end Multiview Surface Reconstruction", "comment": null, "summary": "Surfaces are typically represented as meshes, which can be extracted from volumetric fields via meshing or optimized directly as surface parameterizations. Volumetric representations occupy 3D space and have a large effective receptive field along rays, enabling stable and efficient optimization via volumetric rendering; however, subsequent meshing often produces overly dense meshes and introduces accumulated errors. In contrast, pure surface methods avoid meshing but capture only boundary geometry with a single-layer receptive field, making it difficult to learn intricate geometric details and increasing reliance on priors (e.g., shading or normals). We bridge this gap by differentiably turning a surface representation into a volumetric one, enabling end-to-end surface reconstruction via volumetric rendering to model complex geometries. Specifically, we soften a mesh into multiple semi-transparent layers that remain differentiable with respect to the base mesh, endowing it with a controllable 3D receptive field. Combined with a splatting-based renderer and a topology-control strategy, our method can be optimized in about 20 minutes to achieve accurate surface reconstruction while substantially improving mesh quality."}
{"id": "2601.21126", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21126", "abs": "https://arxiv.org/abs/2601.21126", "authors": ["Kooktae Lee", "Julian Martinez"], "title": "AI-Augmented Density-Driven Optimal Control (D2OC) for Decentralized Environmental Mapping", "comment": null, "summary": "This paper presents an AI-augmented decentralized framework for multi-agent (multi-robot) environmental mapping under limited sensing and communication. While conventional coverage formulations achieve effective spatial allocation when an accurate reference map is available, their performance deteriorates under uncertain or biased priors. The proposed method introduces an adaptive and self-correcting mechanism that enables agents to iteratively refine local density estimates within an optimal transport-based framework, ensuring theoretical consistency and scalability. A dual multilayer perceptron (MLP) module enhances adaptivity by inferring local mean-variance statistics and regulating virtual uncertainty for long-unvisited regions, mitigating stagnation around local minima. Theoretical analysis rigorously proves convergence under the Wasserstein metric, while simulation results demonstrate that the proposed AI-augmented Density-Driven Optimal Control consistently achieves robust and precise alignment with the ground-truth density, yielding substantially higher-fidelity reconstruction of complex multi-modal spatial distributions compared with conventional decentralized baselines."}
{"id": "2601.21632", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2601.21632", "abs": "https://arxiv.org/abs/2601.21632", "authors": ["Shaina Raza", "Iuliia Eyriay", "Ahmed Y. Radwan", "Nate Lesperance", "Deval Pandya", "Sedef Akinli Kocak", "Graham W. Taylor"], "title": "Sustainable Open-Source AI Requires Tracking the Cumulative Footprint of Derivatives", "comment": null, "summary": "Open-source AI is scaling rapidly, and model hubs now host millions of artifacts. Each foundation model can spawn large numbers of fine-tunes, adapters, quantizations, merges, and forks. We take the position that compute efficiency alone is insufficient for sustainability in open-source AI: lower per-run costs can accelerate experimentation and deployment, increasing aggregate environmental footprint unless impacts are measurable and comparable across derivative lineages. However, the energy use, water consumption, and emissions of these derivative lineages are rarely measured or disclosed in a consistent, comparable manner, leaving ecosystem-level impact largely invisible. We argue that sustainable open-source AI requires coordination infrastructure that tracks impacts across model lineages, not only base models. We propose Data and Impact Accounting (DIA), a lightweight, non-restrictive transparency layer that (i) standardizes carbon and water reporting metadata, (ii) integrates low-friction measurement into common training and inference pipelines, and (iii) aggregates reports through public dashboards to summarize cumulative impacts across releases and derivatives. DIA makes derivative costs visible and supports ecosystem-level accountability while preserving openness. https://vectorinstitute.github.io/ai-impact-accounting/"}
{"id": "2601.21488", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2601.21488", "abs": "https://arxiv.org/abs/2601.21488", "authors": ["Jiahao Tang", "Youjun Li", "Yangxuan Zheng", "Xiangting Fan", "Siyuan Lu", "Nuo Zhang", "Zi-Gang Huang"], "title": "HADUA: Hierarchical Attention and Dynamic Uniform Alignment for Robust Cross-Subject Emotion Recognition", "comment": null, "summary": "Robust cross-subject emotion recognition from multimodal physiological signals remains a challenging problem, primarily due to modality heterogeneity and inter-subject distribution shift. To tackle these challenges, we propose a novel adaptive learning framework named Hierarchical Attention and Dynamic Uniform Alignment (HADUA). Our approach unifies the learning of multimodal representations with domain adaptation. First, we design a hierarchical attention module that explicitly models intra-modal temporal dynamics and inter-modal semantic interactions (e.g., between electroencephalogram(EEG) and eye movement(EM)), yielding discriminative and semantically coherent fused features. Second, to overcome the noise inherent in pseudo-labels during adaptation, we introduce a confidence-aware Gaussian weighting scheme that smooths the supervision from target-domain samples by down-weighting uncertain instances. Third, a uniform alignment loss is employed to regularize the distribution of pseudo-labels across classes, thereby mitigating imbalance and stabilizing conditional distribution matching. Extensive experiments on multiple cross-subject emotion recognition benchmarks show that HADUA consistently surpasses existing state-of-the-art methods in both accuracy and robustness, validating its effectiveness in handling modality gaps, noisy pseudo-labels, and class imbalance. Taken together, these contributions offer a practical and generalizable solution for building robust cross-subject affective computing systems."}
{"id": "2601.21141", "categories": ["cs.HC", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.21141", "abs": "https://arxiv.org/abs/2601.21141", "authors": ["Po-Hsun Chen", "Ivan C. H. Liu"], "title": "Optimization and Mobile Deployment for Anthropocene Neural Style Transfer", "comment": "7 pages, 11 figures, submitted to SIGGRAPH 2026", "summary": "This paper presents AnthropoCam, a mobile-based neural style transfer (NST) system optimized for the visual synthesis of Anthropocene environments. Unlike conventional artistic NST, which prioritizes painterly abstraction, stylizing human-altered landscapes demands a careful balance between amplifying material textures and preserving semantic legibility. Industrial infrastructures, waste accumulations, and modified ecosystems contain dense, repetitive patterns that are visually expressive yet highly susceptible to semantic erosion under aggressive style transfer.\n  To address this challenge, we systematically investigate the impact of NST parameter configurations on the visual translation of Anthropocene textures, including feature layer selection, style and content loss weighting, training stability, and output resolution. Through controlled experiments, we identify an optimal parameter manifold that maximizes stylistic expression while preventing semantic erasure. Our results demonstrate that appropriate combinations of convolutional depth, loss ratios, and resolution scaling enable the faithful transformation of anthropogenic material properties into a coherent visual language.\n  Building on these findings, we implement a low-latency, feed-forward NST pipeline deployed on mobile devices. The system integrates a React Native frontend with a Flask-based GPU backend, achieving high-resolution inference within 3-5 seconds on general mobile hardware. This enables real-time, in-situ visual intervention at the site of image capture, supporting participatory engagement with Anthropocene landscapes.\n  By coupling domain-specific NST optimization with mobile deployment, AnthropoCam reframes neural style transfer as a practical and expressive tool for real-time environmental visualization in the Anthropocene."}
{"id": "2601.22026", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.22026", "abs": "https://arxiv.org/abs/2601.22026", "authors": ["Constantin Kleinbeck", "Luisa Theelke", "Hannah Schieber", "Ulrich Eck", "Rüdiger von Eisenhart-Rothe", "Daniel Roth"], "title": "Hybrid Foveated Path Tracing with Peripheral Gaussians for Immersive Anatomy", "comment": "Scheduled for publication in the Proceedings of IEEE VR 2026", "summary": "Volumetric medical imaging offers great potential for understanding complex pathologies. Yet, traditional 2D slices provide little support for interpreting spatial relationships, forcing users to mentally reconstruct anatomy into three dimensions. Direct volumetric path tracing and VR rendering can improve perception but are computationally expensive, while precomputed representations, like Gaussian Splatting, require planning ahead. Both approaches limit interactive use.\n  We propose a hybrid rendering approach for high-quality, interactive, and immersive anatomical visualization. Our method combines streamed foveated path tracing with a lightweight Gaussian Splatting approximation of the periphery. The peripheral model generation is optimized with volume data and continuously refined using foveal renderings, enabling interactive updates. Depth-guided reprojection further improves robustness to latency and allows users to balance fidelity with refresh rate.\n  We compare our method against direct path tracing and Gaussian Splatting. Our results highlight how their combination can preserve strengths in visual quality while re-generating the peripheral model in under a second, eliminating extensive preprocessing and approximations. This opens new options for interactive medical visualization."}
{"id": "2601.21477", "categories": ["cs.MA", "cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.21477", "abs": "https://arxiv.org/abs/2601.21477", "authors": ["Tobias Schmidt", "Kai Cui"], "title": "Mean-Field Control on Sparse Graphs: From Local Limits to GNNs via Neighborhood Distributions", "comment": "19 pages", "summary": "Mean-field control (MFC) offers a scalable solution to the curse of dimensionality in multi-agent systems but traditionally hinges on the restrictive assumption of exchangeability via dense, all-to-all interactions. In this work, we bridge the gap to real-world network structures by proposing a rigorous framework for MFC on large sparse graphs. We redefine the system state as a probability measure over decorated rooted neighborhoods, effectively capturing local heterogeneity. Our central contribution is a theoretical foundation for scalable reinforcement learning in this setting. We prove horizon-dependent locality: for finite-horizon problems, an agent's optimal policy at time t depends strictly on its (T-t)-hop neighborhood. This result renders the infinite-dimensional control problem tractable and underpins a novel Dynamic Programming Principle (DPP) on the lifted space of neighborhood distributions. Furthermore, we formally and experimentally justify the use of Graph Neural Networks (GNNs) for actor-critic algorithms in this context. Our framework naturally recovers classical MFC as a degenerate case while enabling efficient, theoretically grounded control on complex sparse topologies."}
{"id": "2601.21846", "categories": ["cs.ET", "cs.GT"], "pdf": "https://arxiv.org/pdf/2601.21846", "abs": "https://arxiv.org/abs/2601.21846", "authors": ["Konstantinos Varsos", "Adamantia Stamou", "George D. Stamoulis", "Vasillios A. Siris"], "title": "Optimal Energy-Aware Service Management in Future Networks with a Gamified Incentives Mechanism", "comment": null, "summary": "As energy demands surge across ICT infrastructures, service providers must engage users in sustainable practices while maintaining the Quality of Experience (QoE) at acceptable levels. In this paper, we introduce such an approach, leveraging gamified incentives and a model for user's acceptance on incentives, thus encouraging energy-efficient behaviors such as adaptive bitrate streaming. Each user is characterized by an environmental sensitivity factor and a private incentive threshold, shaping probabilistic responses to energy-saving offers. A serious-game mechanism based on positive behavioral reinforcement and rewards of the users, due to their inclusion in top-K and bottom-M rankings, fosters peer comparison and competition, thus transforming passive acceptance into active engagement. Moreover, within a Stackelberg game formulation, the video streaming service provider--acting as the strategic leader--optimizes both incentive levels and game parameters to achieve network-wide energy and traffic reductions, while adhering to budgetary constraints. This structured approach empowers providers with proactive, application-level control over energy consumption, offering them measurable benefits such as reduced high-bitrate traffic and increased participation in energy-saving behaviors, while also considering user satisfaction. The results of our simulations show that indeed gamification boosts significantly user participation and energy savings provided that the incentive and game parameters are chosen optimally."}
{"id": "2601.21675", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2601.21675", "abs": "https://arxiv.org/abs/2601.21675", "authors": ["Zhiyu Xie", "Fuqiang Niu", "Genan Dai", "Qianlong Wang", "Li Dong", "Bowen Zhang", "Hu Huang"], "title": "Rethinking Fusion: Disentangled Learning of Shared and Modality-Specific Information for Stance Detection", "comment": "ICASSP 2026", "summary": "Multi-modal stance detection (MSD) aims to determine an author's stance toward a given target using both textual and visual content. While recent methods leverage multi-modal fusion and prompt-based learning, most fail to distinguish between modality-specific signals and cross-modal evidence, leading to suboptimal performance. We propose DiME (Disentangled Multi-modal Experts), a novel architecture that explicitly separates stance information into textual-dominant, visual-dominant, and cross-modal shared components. DiME first uses a target-aware Chain-of-Thought prompt to generate reasoning-guided textual input. Then, dual encoders extract modality features, which are processed by three expert modules with specialized loss functions: contrastive learning for modality-specific experts and cosine alignment for shared representation learning. A gating network adaptively fuses expert outputs for final prediction. Experiments on four benchmark datasets show that DiME consistently outperforms strong unimodal and multi-modal baselines under both in-target and zero-shot settings."}
{"id": "2601.21264", "categories": ["cs.HC", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.21264", "abs": "https://arxiv.org/abs/2601.21264", "authors": ["Yoonsang Kim", "Swapnil Dey", "Arie Kaufman"], "title": "Evaluating Spatialized Auditory Cues for Rapid Attention Capture in XR", "comment": "8 pages, 4 figures. This is the author's version of the article that will appear at the IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (IEEE VRW) 2026", "summary": "In time-critical eXtended reality (XR) scenarios where users must rapidly reorient their attention to hazards, alerts, or instructions while engaged in a primary task, spatial audio can provide an immediate directional cue without occupying visual bandwidth. However, such scenarios can afford only a brief auditory exposure, requiring users to interpret sound direction quickly and without extended listening or head-driven refinement. This paper reports a controlled exploratory study of rapid spatial-audio localization in XR. Using HRTF-rendered broadband stimuli presented from a semi-dense set of directions around the listener, we quantify how accurately users can infer coarse direction from brief audio alone. We further examine the effects of short-term visuo-auditory feedback training as a lightweight calibration mechanism. Our findings show that brief spatial cues can convey coarse directional information, and that even short calibration can improve users' perception of aural signals. While these results highlight the potential of spatial audio for rapid attention guidance, they also show that auditory cues alone may not provide sufficient precision for complex or high-stakes tasks, and that spatial audio may be most effective when complemented by other sensory modalities or visual cues, without relying on head-driven refinement. We leverage this study on spatial audio as a preliminary investigation into a first-stage attention-guidance channel for wearable XR (e.g., VR head-mounted displays and AR smart glasses), and provide design insights on stimulus selection and calibration for time-critical use."}
{"id": "2601.22143", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.22143", "abs": "https://arxiv.org/abs/2601.22143", "authors": ["Anthony Chen", "Naomi Ken Korem", "Tavi Halperin", "Matan Ben Yosef", "Urska Jelercic", "Ofir Bibi", "Or Patashnik", "Daniel Cohen-Or"], "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion", "comment": "Project webpage available at https://justdubit.github.io", "summary": "Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines."}
{"id": "2601.22041", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22041", "abs": "https://arxiv.org/abs/2601.22041", "authors": ["Naomi Pitzer", "Daniela Mihai"], "title": "Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems", "comment": "To be published in EvoLang XVI proceedings. 15 pages, 17 figures", "summary": "Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification entropy, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit's contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited fine-tuning enables successful cross-system communication. This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation."}
{"id": "2601.21903", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2601.21903", "abs": "https://arxiv.org/abs/2601.21903", "authors": ["Konstantinos Varsos", "Adamantia Stamou", "George D. Stamoulis", "Vasillios A. Siris"], "title": "User Acceptance Model for Smart Incentives in Sustainable Video Streaming towards 6G", "comment": null, "summary": "The rapid growth of 5G video streaming is intensifying energy consumption across access, core, and data-center networks, underscoring the critical need for energy and carbon-efficient solutions. While reducing streaming bitrates improves energy efficiency, its success hinges on user acceptance--particularly when lower bitrates may be perceived as reduced quality of experience (QoE). Therefore, there is a need to develop transparent, user-centric incentive models that balance sustainability with perceived value. We propose a user-acceptance model that combines diverse environmental awareness, personalized responsiveness to incentives, and varying levels of altruism into a unified probabilistic framework. The model incorporates dynamic, individualized incentives that adapt over time. We further enhance the framework by incorporating (i) social well-being as a motivator for altruistic choices, (ii) provider-driven education strategies that gradually adjust user acceptance thresholds, and (iii) data-driven learning of user traits from historical offer--response interactions. Extensive synthetic-data experiments reveal the trade-offs between provider cost and network flexibility, showing that personalized incentives and gradual behavioral adaptation can advance sustainability targets without compromising stakeholder requirements."}
{"id": "2601.21740", "categories": ["cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21740", "abs": "https://arxiv.org/abs/2601.21740", "authors": ["Meng Yang", "Jon McCormack", "Maria Teresa Llano", "Wanchao Su", "Chao Lei"], "title": "MIDI-LLaMA: An Instruction-Following Multimodal LLM for Symbolic Music Understanding", "comment": "Accepted for publication at International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "Recent advances in multimodal large language models (MLLM) for audio music have demonstrated strong capabilities in music understanding, yet symbolic music, a fundamental representation of musical structure, remains unexplored. In this work, we introduce MIDI-LLaMA, the first instruction-following MLLM for symbolic music understanding. Our approach aligns the MIDI encoder MusicBERT and Llama-3-8B via a two-stage pipeline comprising feature alignment and instruction tuning. To support training, we design a scalable annotation pipeline that annotates GiantMIDI-Piano with fine-grained metadata, resulting in a MIDI-text dataset. Compared with the baseline trained on converting MIDI into ABC notation under the same instruction-tuning procedure, MIDI-LLaMA substantially outperforms in captioning and semantic alignment in question answering. Human evaluation further confirms the advantages of MIDI-LLaMA in music understanding, emotion recognition, creativity, and overall preference. These findings demonstrate that incorporating symbolic music into large language models enhances their capacity for musical understanding."}
{"id": "2601.21271", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21271", "abs": "https://arxiv.org/abs/2601.21271", "authors": ["Tram Thi Minh Tran", "Soojeong Yoo", "Oliver Weidlich", "Yidan Cao", "Xinyan Yu", "Xin Cheng", "Yin Ye", "Natalia Gulbransen-Diaz", "Callum Parker"], "title": "Envisioning Audio Augmented Reality in Everyday Life", "comment": null, "summary": "While visual augmentation dominates the augmented reality landscape, devices like Meta Ray-Ban audio smart glasses signal growing industry movement toward audio augmented reality (AAR). Hearing is a primary channel for sensing context, anticipating change, and navigating social space, yet AAR's everyday potential remains underexplored. We address this gap through a collaborative autoethnography (N=5, authoring) and an online survey (N=74). We identify ten roles for AAR, grouped into three categories: task- and utility-oriented, emotional and social, and perceptual collaborator. These roles are further layered with a rhythmic and embodied collaborator framing, mapping them onto micro-, meso-, and macro-rhythms of everyday life. Our analysis surfaces nuanced tensions, such as blocking distractions without erasing social presence, highlighting the need for context-aware design. This paper contributes a foundational and forward-looking framework for AAR in everyday life, providing design groundwork for systems attuned to daily routines, sensory engagement, and social expectations."}
{"id": "2601.21141", "categories": ["cs.HC", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.21141", "abs": "https://arxiv.org/abs/2601.21141", "authors": ["Po-Hsun Chen", "Ivan C. H. Liu"], "title": "Optimization and Mobile Deployment for Anthropocene Neural Style Transfer", "comment": "7 pages, 11 figures, submitted to SIGGRAPH 2026", "summary": "This paper presents AnthropoCam, a mobile-based neural style transfer (NST) system optimized for the visual synthesis of Anthropocene environments. Unlike conventional artistic NST, which prioritizes painterly abstraction, stylizing human-altered landscapes demands a careful balance between amplifying material textures and preserving semantic legibility. Industrial infrastructures, waste accumulations, and modified ecosystems contain dense, repetitive patterns that are visually expressive yet highly susceptible to semantic erosion under aggressive style transfer.\n  To address this challenge, we systematically investigate the impact of NST parameter configurations on the visual translation of Anthropocene textures, including feature layer selection, style and content loss weighting, training stability, and output resolution. Through controlled experiments, we identify an optimal parameter manifold that maximizes stylistic expression while preventing semantic erasure. Our results demonstrate that appropriate combinations of convolutional depth, loss ratios, and resolution scaling enable the faithful transformation of anthropogenic material properties into a coherent visual language.\n  Building on these findings, we implement a low-latency, feed-forward NST pipeline deployed on mobile devices. The system integrates a React Native frontend with a Flask-based GPU backend, achieving high-resolution inference within 3-5 seconds on general mobile hardware. This enables real-time, in-situ visual intervention at the site of image capture, supporting participatory engagement with Anthropocene landscapes.\n  By coupling domain-specific NST optimization with mobile deployment, AnthropoCam reframes neural style transfer as a practical and expressive tool for real-time environmental visualization in the Anthropocene."}
{"id": "2601.22013", "categories": ["cs.HC", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.22013", "abs": "https://arxiv.org/abs/2601.22013", "authors": ["Catherine Yeh", "Anh Truong", "Mira Dontcheva", "Bryan Wang"], "title": "Vidmento: Creating Video Stories Through Context-Aware Expansion With Generative Video", "comment": "25 pages, 18 figures", "summary": "Video storytelling is often constrained by available material, limiting creative expression and leaving undesired narrative gaps. Generative video offers a new way to address these limitations by augmenting captured media with tailored visuals. To explore this potential, we interviewed eight video creators to identify opportunities and challenges in integrating generative video into their workflows. Building on these insights and established filmmaking principles, we developed Vidmento, a tool for authoring hybrid video stories that combine captured and generated media through context-aware expansion. Vidmento surfaces opportunities for story development, generates clips that blend stylistically and narratively with surrounding media, and provides controls for refinement. In a study with 12 creators, Vidmento supported narrative development and exploration by systematically expanding initial materials with generative media, enabling expressive video storytelling aligned with creative intent. We highlight how creators bridge story gaps with generative content and where they find this blending capability most valuable."}
{"id": "2601.21460", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21460", "abs": "https://arxiv.org/abs/2601.21460", "authors": ["Suifang Zhou", "Qi Gong", "Ximing Shen", "RAY LC"], "title": "Tell Me What I Missed: Tell Me What I Missed: Interacting with GPT during Recalling of One-Time Witnessed Events", "comment": "19 pages, 9 figures, CHI 2026", "summary": "LLM-assisted technologies are increasingly used to support cognitive processing and information interpretation, yet their role in aiding memory recall, and how people choose to engage with them, remains underexplored. We studied participants who watched a short robbery video (approximating a one-time eyewitness scenario) and composed recall statements using either a default GPT or a guided GPT prompted with a standardized eyewitness protocol. Results show that, in the default condition, participants who believed they had a clearer understanding of the event were more likely to trust GPT's output, whereas in the guided condition, participants showed stronger alignment between subjective clarity and actual recall. Additionally, participants evaluated the legitimacy of the individuals in the incident differently across conditions. Interaction analysis further revealed that default-GPT users spontaneously developed diverse strategies, including building on existing recollections, requesting potentially missing details, and treating GPT as a recall coach. This work shows how GPT-user interplay can subconsciously shape beliefs and perceptions of remembered events."}
{"id": "2601.21490", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21490", "abs": "https://arxiv.org/abs/2601.21490", "authors": ["Fabian Albers", "Sebastian Strauß", "Nikol Rummel", "Nils Köbis"], "title": "Are they just delegating? Cross-Sample Predictions on University Students' & Teachers' Use of AI", "comment": "27 pages, 5 figures", "summary": "Mutual trust between teachers and students is a prerequisite for effective teaching, learning, and assessment in higher education. Accurate predictions about the other group's use of generative artificial intelligence (AI) are fundamental for such trust. However, the disruptive rise of AI has transformed academic work practices, raising important questions about how teachers and students use these tools and how well they can estimate each other's usage. While the frequency of use is well studied, little is known about how AI is used, and comparisons with similar practices are rare. This study surveyed German university teachers (N = 113) and students (N = 123) on the frequency of AI use and the degree of delegation across six identical academic tasks. Participants also provided incentivized cross-sample predictions of the other group's AI use to assess the accuracy of their predictions. We find that students reported higher use of AI and greater delegation than teachers. Both groups significantly overestimated the other group's use, with teachers predicting very frequent use and high delegation by students, and students assuming teachers use AI similarly to themselves. These findings reveal a perception gap between teachers' and students' expectations and actual AI use. Such gaps may hinder trust and effective collaboration, underscoring the need for open dialogue about AI practices in academia and for policies that support the equitable and transparent integration of AI tools in higher education."}
{"id": "2601.21492", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21492", "abs": "https://arxiv.org/abs/2601.21492", "authors": ["Thomas Herrmann"], "title": "Organizational Practices and Socio-Technical Design of Human-Centered AI", "comment": "29 pages, 3 figures, 1 table, Published in: Wei Xu, (Ed.) 2026, Handbook of Human-Centered Artificial Intelligence, pp 1-45, Springe Nature Singapore", "summary": "This contribution explores how the integration of Artificial Intelligence (AI) into organizational practices can be effectively framed through a socio-technical perspective to comply with the requirements of Human-centered AI (HCAI). Instead of viewing AI merely as a technical tool, the analysis emphasizes the importance of embedding AI into communication, collaboration, and decision-making processes within organizations from a human-centered perspective. Ten case-based patterns illustrate how AI support of predictive maintenance can be organized to address quality assurance and continuous improvement and to provide different types of sup-port for HCAI. The analysis shows that AI adoption often requires and enables new forms of organizational learning, where specialists jointly interpret AI output, adapt workflows, and refine rules for system improve-ment. Different dimensions and levels of socio-technical integration of AI are considered to reflect the effort and benefits of keeping the organization in the loop."}
{"id": "2601.21518", "categories": ["cs.HC", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.21518", "abs": "https://arxiv.org/abs/2601.21518", "authors": ["Xinyi Zhang", "Mamtaj Akter", "Heajun An", "Minqian Liu", "Qi Zhang", "Lifu Huang", "Jin-Hee Cho", "Pamela J. Wisniewski", "Sang Won Lee"], "title": "From Vulnerable to Resilient: Examining Parent and Teen Perceptions on How to Respond to Unwanted Cybergrooming Advances", "comment": null, "summary": "Cybergrooming is a form of online abuse that threatens teens' mental health and physical safety. Yet, most prior work has focused on detecting perpetrators' behaviors, leaving a limited understanding of how teens might respond to such unwanted advances. To address this gap, we conducted an online survey with 74 participants -- 51 parents and 23 teens -- who responded to simulated cybergrooming scenarios in two ways: responses that they think would make teens more vulnerable or resilient to unwanted sexual advances. Through a mixed-methods analysis, we identified four types of vulnerable responses (encouraging escalation, accepting an advance, displaying vulnerability, and negating risk concern) and four types of protective strategies (setting boundaries, directly declining, signaling risk awareness, and leveraging avoidance techniques). As the cybergrooming risk escalated, both vulnerable responses and protective strategies showed a corresponding progression. This study contributes a teen-centered understanding of cybergrooming, a labeled dataset, and a stage-based taxonomy of perceived protective strategies, while offering implications for educational programs and sociotechnical interventions."}
{"id": "2601.21650", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21650", "abs": "https://arxiv.org/abs/2601.21650", "authors": ["Alexander Erlei", "Federico Cau", "Radoslav Georgiev", "Sagar Kumar", "Kilian Bizer", "Ujwal Gadiraju"], "title": "When Life Gives You AI, Will You Turn It Into A Market for Lemons? Understanding How Information Asymmetries About AI System Capabilities Affect Market Outcomes and Adoption", "comment": null, "summary": "AI consumer markets are characterized by severe buyer-supplier market asymmetries. Complex AI systems can appear highly accurate while making costly errors or embedding hidden defects. While there have been regulatory efforts surrounding different forms of disclosure, large information gaps remain. This paper provides the first experimental evidence on the important role of information asymmetries and disclosure designs in shaping user adoption of AI systems. We systematically vary the density of low-quality AI systems and the depth of disclosure requirements in a simulated AI product market to gauge how people react to the risk of accidentally relying on a low-quality AI system. Then, we compare participants' choices to a rational Bayesian model, analyzing the degree to which partial information disclosure can improve AI adoption. Our results underscore the deleterious effects of information asymmetries on AI adoption, but also highlight the potential of partial disclosure designs to improve the overall efficiency of human decision-making."}
{"id": "2601.21791", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21791", "abs": "https://arxiv.org/abs/2601.21791", "authors": ["Valerie Tan", "Luisa Jost", "Jens Gerken", "Max Pascher"], "title": "Preliminary Results of a Scoping Review on Assistive Technologies for Adults with ADHD", "comment": "9 pages, 1 fvigure", "summary": "Attention Deficit Hyperactivity Disorder (ADHD), characterized by inattention, hyperactivity, and impulsivity, is prevalent in the adult population. Long perceived and treated as a childhood condition, ADHD and its characteristics nonetheless impact a significant portion of adults today. In contrast to children with ADHD, adults with ADHD face unique challenges in the workplace and in higher education. In this work-in-progress paper, we present a scoping review as a foundation to understand and explore existing technology-based approaches to support adults with ADHD. In total, our search returned 3,538 papers upon which we selected, based on PRISMA-ScR, a total of 46 papers for in-depth analysis. Our initial findings highlight that most papers take on a therapeutic or intervention perspective instead of a more positive support perspective. Our analysis also found a tremendous increase in recent papers on the topic, which highlights that more and more researchers are becoming aware of the need to address ADHD with adults. For the future, we aim to further analyze the corpus and identify research gaps and potentials for further development of ADHD assistive technologies."}
{"id": "2601.21920", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.21920", "abs": "https://arxiv.org/abs/2601.21920", "authors": ["Upol Ehsan", "Samir Passi", "Koustuv Saha", "Todd McNutt", "Mark O. Riedl", "Sara Alcorn"], "title": "From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction", "comment": null, "summary": "In the future of work discourse, AI is touted as the ultimate productivity amplifier. Yet, beneath the efficiency gains lie subtle erosions of human expertise and agency. This paper shifts focus from the future of work to the future of workers by navigating the AI-as-Amplifier Paradox: AI's dual role as enhancer and eroder, simultaneously strengthening performance while eroding underlying expertise. We present a year-long study on the longitudinal use of AI in a high-stakes workplace among cancer specialists. Initial operational gains hid ``intuition rust'': the gradual dulling of expert judgment. These asymptomatic effects evolved into chronic harms, such as skill atrophy and identity commoditization. Building on these findings, we offer a framework for dignified Human-AI interaction co-constructed with professional knowledge workers facing AI-induced skill erosion without traditional labor protections. The framework operationalizes sociotechnical immunity through dual-purpose mechanisms that serve institutional quality goals while building worker power to detect, contain, and recover from skill erosion, and preserve human identity. Evaluated across healthcare and software engineering, our work takes a foundational step toward dignified human-AI interaction futures by balancing productivity with the preservation of human expertise."}
{"id": "2601.21965", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21965", "abs": "https://arxiv.org/abs/2601.21965", "authors": ["Deeksha M. Shama", "Dimitra Emmanouilidou", "Ivan J. Tashev"], "title": "Cognitive Load Estimation Using Brain Foundation Models and Interpretability for BCIs", "comment": null, "summary": "Accurately monitoring cognitive load in real time is critical for Brain-Computer Interfaces (BCIs) that adapt to user engagement and support personalized learning. Electroencephalography (EEG) offers a non-invasive, cost-effective modality for capturing neural activity, though traditional methods often struggle with cross-subject variability and task-specific preprocessing. We propose leveraging Brain Foundation Models (BFMs), large pre-trained neural networks, to extract generalizable EEG features for cognitive load estimation. We adapt BFMs for long-term EEG monitoring and show that fine-tuning a small subset of layers yields improved accuracy over the state-of-the-art. Despite their scale, BFMs allow for real-time inference with a longer context window. To address often-overlooked interpretability challenges, we apply Partition SHAP (SHapley Additive exPlanations) to quantify feature importance. Our findings reveal consistent emphasis on prefrontal regions linked to cognitive control, while longitudinal trends suggest learning progression. These results position BFMs as efficient and interpretable tools for continuous cognitive load monitoring in real-world BCIs."}
{"id": "2601.21977", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.21977", "abs": "https://arxiv.org/abs/2601.21977", "authors": ["Javier Argota Sánchez-Vaquerizo", "Luis Borunda Monsivais"], "title": "From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation", "comment": "Paper selected for the workshop Human Cognition, AI, and the Future of HCI: Navigating the Disruptive and Wild Landscape of Large Language Models and Agentic AI as part of the Human-Computer Interaction (HCI) conference of the Alpine region (AlpCHI 2026) hosted at the Congressi Stefano Franscini, March 1st to March 5th, 2026 on Monte Verità in Ascona, Switzerland", "summary": "Traditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based \"particles\" rather than cognitive \"agents\". To bridge this, we introduce \\textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \\textbf{Cognitive Friction} ($C_f$) it is possible to reveal \"Phantom Affordances\", i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity."}
{"id": "2601.22013", "categories": ["cs.HC", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.22013", "abs": "https://arxiv.org/abs/2601.22013", "authors": ["Catherine Yeh", "Anh Truong", "Mira Dontcheva", "Bryan Wang"], "title": "Vidmento: Creating Video Stories Through Context-Aware Expansion With Generative Video", "comment": "25 pages, 18 figures", "summary": "Video storytelling is often constrained by available material, limiting creative expression and leaving undesired narrative gaps. Generative video offers a new way to address these limitations by augmenting captured media with tailored visuals. To explore this potential, we interviewed eight video creators to identify opportunities and challenges in integrating generative video into their workflows. Building on these insights and established filmmaking principles, we developed Vidmento, a tool for authoring hybrid video stories that combine captured and generated media through context-aware expansion. Vidmento surfaces opportunities for story development, generates clips that blend stylistically and narratively with surrounding media, and provides controls for refinement. In a study with 12 creators, Vidmento supported narrative development and exploration by systematically expanding initial materials with generative media, enabling expressive video storytelling aligned with creative intent. We highlight how creators bridge story gaps with generative content and where they find this blending capability most valuable."}
{"id": "2601.22081", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.22081", "abs": "https://arxiv.org/abs/2601.22081", "authors": ["Yichun Zhao", "Miguel A. Nacenta", "Mahadeo A. Sukhai", "Sowmya Somanath"], "title": "Accessibility-Driven Information Transformations in Mixed-Visual Ability Work Teams", "comment": "To appear in ACM CHI 2026. DOI: https://doi.org/10.1145/3772318.3790872", "summary": "Blind and low-vision (BLV) employees in mixed-visual ability teams often encounter information (e.g., PDFs, diagrams) in inaccessible formats. To enable teamwork, teams must transform these representations by modifying or re-creating them into accessible forms. However, these transformations are frequently overlooked, lack infrastructural support, and cause additional labour. To design systems that move beyond one-off accommodations to effective mixed-ability collaboration, we need a deeper understanding of the representations, their transformations and how they occur. We conducted a week-long diary study with follow-up interviews with 23 BLV and sighted professionals from five legal, non-profit, and consulting teams, documenting 36 transformation cases. Our analysis characterizes how teams perform representational transformations for accessibility: how they are triggered proactively or reactively, how they simplify or enhance, and four common patterns in which workers coordinate with each other to address representational incompatibility. Our findings uncover opportunities for designing systems that can better support mixed-visual ability work."}
{"id": "2601.22082", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.22082", "abs": "https://arxiv.org/abs/2601.22082", "authors": ["Yi Fei Cheng", "Jarod Bloch", "Alexander Wang", "Andrea Bianchi", "Anusha Withana", "Anhong Guo", "Laurie M. Heller", "David Lindlbauer"], "title": "Auditorily Embodied Conversational Agents: Effects of Spatialization and Situated Audio Cues on Presence and Social Perception", "comment": null, "summary": "Embodiment can enhance conversational agents, such as increasing their perceived presence. This is typically achieved through visual representations of a virtual body; however, visual modalities are not always available, such as when users interact with agents using headphones or display-less glasses. In this work, we explore auditory embodiment. By introducing auditory cues of bodily presence - through spatially localized voice and situated Foley audio from environmental interactions - we investigate how audio alone can convey embodiment and influence perceptions of a conversational agent. We conducted a 2 (spatialization: monaural vs. spatialized) x 2 (Foley: none vs. Foley) within-subjects study, where participants (n=24) engaged in conversations with agents. Our results show that spatialization and Foley increase co-presence, but reduce users' perceptions of the agent's attention and other social attributes."}
